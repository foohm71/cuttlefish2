"id","created","description","key","priority","project","project_name","repositoryname","resolution","resolved","status","title","type","updated","votes","watchers","assignee_id","reporter_id"
16037,"2011-06-16 22:07:44.375","traverseNode in DataTree will never actually traverse the limit nodes properly.","ZOOKEEPER-1097","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-06-27 07:30:59.505","Closed","Quota is not correctly rehydrated on snapshot reload","Bug","2011-11-24 03:22:06.217",0,0,7832,7832
16615,"2009-08-07 08:57:50.287","when calling the asynchronous version of create, the completion routine is called more than once.","ZOOKEEPER-502","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-27 07:19:07.334","Closed","bookkeeper create calls completion too many times","Bug","2010-03-27 01:24:56.283",0,0,7750,7829
16189,"2010-11-20 06:12:26.124","test -e FILENAME is not support on /bin/sh in solaris. This is used in bin/zkEnv.sh. We can substitute test -f FILENAME. Attaching a patch.","ZOOKEEPER-937","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-12-08 03:02:17.059","Closed","test -e not available on solaris /bin/sh","Bug","2011-11-24 03:21:59.751",0,0,8219,8219
16616,"2009-08-07 06:11:42.25","It timed out according to the console output:http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/406/testReport/org.apache.zookeeper.test/CnxManagerTest/testCnxManager/","ZOOKEEPER-501","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-08 05:03:30.561","Closed","CnxManagerTest failed on hudson","Bug","2009-09-06 06:36:20.729",0,0,7750,7750
16934,"2008-10-07 04:50:15.001","fast leader election test failing .","ZOOKEEPER-178","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-11 07:16:55.237","Closed","FLE test.","Bug","2008-10-26 09:10:43.46",0,0,7750,7827
16989,"2008-08-25 03:29:13.949","Copy and paste strikes again.  In two logger instantiations, the wrong class is passed to the constructor.  In ClientTest.java and ClientCnxn.java","ZOOKEEPER-123","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-08-26 02:17:05.262","Closed","In two places, the wrong class is specified for the logger","Bug","2008-10-26 09:10:42.131",0,0,8066,8066
17003,"2008-08-02 06:01:07.69","NPE/ResourceLeak cleanup for issues found during static analysis.","ZOOKEEPER-109","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-08-09 03:14:00.251","Closed","cleanup of NPE and Resource issue nits found by static analysis","Bug","2008-10-26 09:10:41.752",1,0,7818,7818
16811,"2009-02-06 22:06:17.352","Macs don't support the -f option in readlink{noformat}$ bin/zkServer.sh startJMX enabled by defaultreadlink: illegal option -- fusage: readlink [-n] [file ...]{noformat}","ZOOKEEPER-303","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-02-11 02:41:20.272","Closed","Bin scripts don't work on Mac","Bug","2011-01-06 11:45:21.761",0,1,8367,8367
16755,"2009-04-03 00:32:07.732","We need a strong reference to prevent a key in masterKeys on Bookie.java to be garbage collected.","ZOOKEEPER-360","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-04 05:43:56.226","Closed","WeakHashMap in Bookie.java causes NPE","Bug","2009-07-09 04:24:02.452",0,0,7750,7750
16701,"2009-05-22 04:29:44.15","the c tests hang sometimes. ","ZOOKEEPER-415","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-22 15:25:56.259","Closed","zookeeper c tests hang.","Bug","2009-07-09 04:24:04.856",0,1,7827,7827
15801,"2011-12-20 09:24:33.917","I think a NPE was created in the fix for https://issues.apache.org/jira/browse/ZOOKEEPER-1269Looking in DataTree.processTxn(TxnHeader header, Record txn) it seems likely that if rc.err != Code.OK then rc.path will be null. I'm currently working on a minimal test case for the bug, I'll attach it to this issue when it's ready.java.lang.NullPointerException	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.processTransaction(FileTxnSnapLog.java:203)	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:150)	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:418)	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:410)	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:151)	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)","ZOOKEEPER-1333","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-12-22 04:40:04.133","Closed","NPE in FileTxnSnapLog when restarting a cluster","Bug","2011-12-30 07:46:54.22",0,6,7818,8099
15811,"2011-12-09 09:28:16.547","EAI_NODATA and EAI_ADDRFAMILY have been deprecated in FreeBSD. I'm getting this error:src/zookeeper.c: In function `getaddrinfo_errno':src/zookeeper.c:446: error: `EAI_NODATA' undeclared (first use in this function)src/zookeeper.c:446: error: (Each undeclared identifier is reported only oncesrc/zookeeper.c:446: error: for each function it appears in.)src/zookeeper.c: In function `getaddrs':src/zookeeper.c:581: error: `EAI_ADDRFAMILY' undeclared (first use in this function)I'll submit a patch.--Michi","ZOOKEEPER-1323","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-12-15 07:19:38.105","Closed","c client doesn't compile on freebsd","Bug","2011-12-30 07:46:53.97",0,0,7800,7800
15815,"2011-12-06 11:06:10.836","I've been trying to update to ZK 3.4.0 and have had some issues where some data become inaccessible after adding a node to a cluster.  My use case is a bit strange (as explained before on this list) in that I try to grow the cluster dynamically by having an external program automatically restart Zookeeper servers in a controlled way whenever the list of participating ZK servers needs to change.  This used to work just fine in 3.3.3 (and before), so this represents a regression.The scenario I see is this:1) Start up a 1-server ZK cluster (the server has ZK ID 0).2) A client connects to the server, and makes a bunch of znodes, in particular a znode called ""/membership"".3) Shut down the cluster.4) Bring up a 2-server ZK cluster, including the original server 0 with its existing data, and a new server with ZK ID 1.5) Node 0 has the highest zxid and is elected leader.6) A client connecting to server 1 tries to ""get /membership"" and gets back a -101 error code (no such znode).7) The same client then tries to ""create /membership"" and gets back a -110 error code (znode already exists).8) Clients connecting to server 0 can successfully ""get /membership"".I will attach a tarball with debug logs for both servers, annotating where steps #1 and #4 happen.  You can see that the election involves a proposal for zxid 110 from server 0, but immediately following the election server 1 has these lines:2011-12-05 17:18:48,308 9299 [QuorumPeer[myid=1]/127.0.0.1:2901] WARN org.apache.zookeeper.server.quorum.Learner  - Got zxid 0x100000001 expected 0x12011-12-05 17:18:48,313 9304 [SyncThread:1] INFO org.apache.zookeeper.server.persistence.FileTxnLog  - Creating new log file: log.100000001Perhaps that's not relevant, but it struck me as odd.  At the end of server 1's log you can see a repeated cycle of getData->create->getData as the client tries to make sense of the inconsistent responses.The other piece of information is that if I try to use the on-disk directories for either of the servers to start a new one-node ZK cluster, all the data are accessible.I haven't tried writing a program outside of my application to reproduce this, but I can do it very easily with some of my app's tests if anyone needs more information.","ZOOKEEPER-1319","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-12-10 03:09:12.572","Closed","Missing data after restarting+expanding a cluster","Bug","2011-12-17 09:33:57.89",0,3,7818,8053
15817,"2011-12-04 21:48:10.434","zookeeper_init does not check the return value of strdup(index_chroot).When it returns NULL, it causes segfault when it try to strlen(zh->chroot).","ZOOKEEPER-1317","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-12-10 02:37:50.029","Closed","Possible segfault in zookeeper_init","Bug","2011-12-17 09:33:59.004",0,0,8106,8106
15818,"2011-12-04 21:34:33.877","zookeeper_init does not free strdup'ed memory when chroot is just '/'.","ZOOKEEPER-1316","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-12-09 06:30:51.589","Closed","zookeeper_init leaks memory if chroot is just '/'","Bug","2011-12-17 09:33:58.459",0,0,8106,8106
15819,"2011-12-04 18:31:17.996","zookeeper_init always reports sessionPasswd=<hidden> even when it's empty.","ZOOKEEPER-1315","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-12-09 08:21:55.303","Closed","zookeeper_init always reports sessionPasswd=<hidden>","Bug","2011-12-17 09:33:58.109",0,0,8106,8106
15823,"2011-12-01 02:04:06.345","In http://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.0/ the test jar cannot be accessed by maven. There are two possible solutions to this. a) rename zookeeper-3.4.0-test.jar to zookeeper-3.4.0-tests.jar and remove zookeeper-3.4.0-test.pom*With this, the maven can access the test jar with{code}     <dependency>       <groupId>org.apache.zookeeper</groupId>       <artifactId>zookeeper</artifactId>       <version>3.4.0</version>       <type>test-jar</type>       <scope>test</scope>     </dependency>{code}b) Alternatively, zookeeper test could be it's own submodule. To do this, it must be deployed in the following layout{code}./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.jar./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.jar.md5./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.jar.sha1./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.pom./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.pom.md5./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.pom.sha1{code}This can then be accessed by maven with{code}     <dependency>       <groupId>org.apache.zookeeper</groupId>       <artifactId>zookeeper-test</artifactId>       <version>3.4.0</version>       <scope>test</scope>     </dependency>{code}I think a) is the better solution.","ZOOKEEPER-1311","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-12-01 15:14:23.476","Closed","ZooKeeper test jar is broken","Bug","2011-12-17 09:33:58.63",0,0,7956,7956
15829,"2011-11-19 02:23:33.597","All the callers of the function prepend_string make a call to prepend_string before checking that zhandle_t *zh is not null. At the top of prepend_string, zh is dereferenced without checking for a null ptr:static char* prepend_string(zhandle_t *zh, const char* client_path) {    char *ret_str;    if (zh->chroot == NULL)        return (char *) client_path;I propose fixing this by adding the check here in prepend_string:static char* prepend_string(zhandle_t *zh, const char* client_path) {    char *ret_str;    if (zh==NULL || zh->chroot == NULL)        return (char *) client_path;","ZOOKEEPER-1305","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-12-09 06:03:58.205","Closed","zookeeper.c:prepend_string func can dereference null ptr","Bug","2012-05-03 10:06:44.289",0,1,7995,7995
15835,"2011-11-16 14:53:42.741","We need to add the winconfig.h to ignores in release audits.","ZOOKEEPER-1299","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-11-16 15:19:47.257","Closed","Add winconfig.h file to ignore in release audit.","Bug","2011-11-24 03:22:04.992",0,0,7827,7827
15851,"2011-11-03 13:35:25.483","I tried to compile 3.3.3 or the current 3.3 branch head, in both cases using ant 1.8.2 fails, however 1.7.0 is successfulhere's the error:{noformat}Testsuite: org.apache.zookeeper.VerGenTestTests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.009 secTestcase: warning took 0.001 sec	FAILEDClass org.apache.zookeeper.VerGenTest has no public constructor TestCase(String name) or TestCase()junit.framework.AssertionFailedError: Class org.apache.zookeeper.VerGenTest has no public constructor TestCase(String name) or TestCase(){noformat}","ZOOKEEPER-1283","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-11-16 02:10:25.594","Closed","building 3.3 branch fails with Ant 1.8.2 (success with 1.7.1 though)","Bug","2011-11-30 01:54:43.763",0,0,7818,7818
15863,"2011-10-29 06:33:08.449","See:https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper_branch34_solaris/1/testReport/junit/org.apache.zookeeper.server.quorum/QuorumPeerMainTest/testEarlyLeaderAbandonment/Notice that the clients attempt to connect before the servers have bound, then 30 seconds later, after seemingly no further client activity we see:2011-10-28 21:40:56,828 [myid:] - INFO  [main-SendThread(localhost:11227):ClientCnxn$SendThread@1057] - Client session timed out, have not heard from server in 30032ms for sessionid 0x0, closing socket connection and attempting reconnectI believe this is different from ZOOKEEPER-1270 because in the 1270 case it seems like the clients are attempting to connect but the servers are not accepting (notice the stat commands are being dropped due to no server running)","ZOOKEEPER-1271","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-11-03 05:59:00.251","Closed","testEarlyLeaderAbandonment failing on solaris - clients not retrying connection","Bug","2011-11-24 03:21:58.433",0,1,7827,7818
15864,"2011-10-29 06:25:36.207","Looks pretty serious - quorum is formed but no clients can attach. Will attach logs momentarily.This test was introduced in the following commit (all three jira commit at once):ZOOKEEPER-335. zookeeper servers should commit the new leader txn to their logs.ZOOKEEPER-1081. modify leader/follower code to correctly deal with new leaderZOOKEEPER-1082. modify leader election to correctly take into account current","ZOOKEEPER-1270","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-11-05 19:46:06.14","Closed","testEarlyLeaderAbandonment failing intermittently, quorum formed, no serving.","Bug","2011-11-24 03:22:04.609",0,3,7750,7818
15865,"2011-10-29 05:34:37.027","From the mailing list:FileTxnSnapLog.restore contains a code block handling a NODEEXISTS failure during deserialization. The problem is explained there in a code comment. The code block however is only executed for a CREATE txn, not for a multiTxn containing a CREATE.Even if the mentioned code block would also be executed for multi transactions, it needs adaption for multi transactions. What, if after the first failed transaction in a multi txn during deserialization, there would be subsequent transactions in the same multi that would also have failed?We don't know, since the first failed transaction hides the information about the remaining transactions.","ZOOKEEPER-1269","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-12-10 06:25:16.548","Closed","Multi deserialization issues","Bug","2011-12-17 09:33:58.321",0,1,7832,7832
15866,"2011-10-29 02:01:05.605","I'm having a lot problems testing the 3.4.0 release candidate (0). I'm seeing frequent failures in RO unit tests, also the solaris tests are broken on jenkins, some of which is due to RO mode:https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper_trunk_solaris/30/#showFailuresLinkI'm also seeing ERROR level messages in the logs during test runs that are a result of attempting to start RO mode.Given this is a new feature, one that could be very disruptive, I think we need to control whether the feature is enabled or not through a config option (system prop is fine), disabled by default.I'll look at the RO mode tests to see if I can find the cause of the failures on solaris, but I may also turn off these tests for the time being. (I need to look at this further).I'm marking this as a blocker for 3.4.0, Mahadev LMK if you feel similarly or whether I should be shooting for 3.4.1 with this. (or perhaps I'm just way off in general).","ZOOKEEPER-1268","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-11-01 15:15:50.147","Closed","problems with read only mode, intermittent test failures and ERRORs in the log","Bug","2011-11-24 03:22:06.59",0,0,7818,7818
15870,"2011-10-28 12:23:28.192","The FollowerResyncConcurrencyTest test is failing intermittently. saw the following on 3.4:{noformat}junit.framework.AssertionFailedError: Should have same number ofephemerals in both followers expected:<11741> but was:<14001>       at org.apache.zookeeper.test.FollowerResyncConcurrencyTest.verifyState(FollowerResyncConcurrencyTest.java:400)       at org.apache.zookeeper.test.FollowerResyncConcurrencyTest.testResyncBySnapThenDiffAfterFollowerCrashes(FollowerResyncConcurrencyTest.java:196)       at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52){noformat}","ZOOKEEPER-1264","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-11-06 04:58:22.326","Closed","FollowerResyncConcurrencyTest failing intermittently","Bug","2011-11-24 03:22:41.542",0,4,7832,7818
15926,"2011-09-28 12:35:47.557","Copying from email thread.We found our ZK server in a state where an ephemeral node still exists aftera client session is long gone. I used the cons command on each ZK host tolist all connections and couldn't find the ephemeralOwner id. We are usingZK 3.3.3. Has anyone seen this problem?I got the following information from the logs.The node that still exists is /kafka-tracking/consumers/UserPerformanceEvent-<host>/owners/UserPerformanceEvent/529-7I saw that the ephemeral owner is 86167322861045079 which is session id 0x13220b93e610550.After searching in the transaction log of one of the ZK servers found that session expired9/22/11 12:17:57 PM PDT session 0x13220b93e610550 cxid 0x74 zxid 0x601bd36f7 closeSession nullOn digging further into the logs I found that there were multiple sessions created in quick succession and every session tried to create the same node. But i verified that the sessions were closed and opened in order9/22/11 12:17:56 PM PDT session 0x13220b93e610550 cxid 0x0 zxid 0x601bd36b5 createSession 60009/22/11 12:17:57 PM PDT session 0x13220b93e610550 cxid 0x74 zxid 0x601bd36f7 closeSession null9/22/11 12:17:58 PM PDT session 0x13220b93e610551 cxid 0x0 zxid 0x601bd36f8 createSession 60009/22/11 12:17:59 PM PDT session 0x13220b93e610551 cxid 0x74 zxid 0x601bd373a closeSession null9/22/11 12:18:00 PM PDT session 0x13220b93e610552 cxid 0x0 zxid 0x601bd373e createSession 60009/22/11 12:18:01 PM PDT session 0x13220b93e610552 cxid 0x6c zxid 0x601bd37a0 closeSession null9/22/11 12:18:02 PM PDT session 0x13220b93e610553 cxid 0x0 zxid 0x601bd37e9 createSession 60009/22/11 12:18:03 PM PDT session 0x13220b93e610553 cxid 0x74 zxid 0x601bd382b closeSession null9/22/11 12:18:04 PM PDT session 0x13220b93e610554 cxid 0x0 zxid 0x601bd383c createSession 60009/22/11 12:18:05 PM PDT session 0x13220b93e610554 cxid 0x6a zxid 0x601bd388f closeSession null9/22/11 12:18:06 PM PDT session 0x13220b93e610555 cxid 0x0 zxid 0x601bd3895 createSession 60009/22/11 12:18:07 PM PDT session 0x13220b93e610555 cxid 0x6a zxid 0x601bd38cd closeSession null9/22/11 12:18:10 PM PDT session 0x13220b93e610556 cxid 0x0 zxid 0x601bd38d1 createSession 60009/22/11 12:18:11 PM PDT session 0x13220b93e610557 cxid 0x0 zxid 0x601bd38f2 createSession 60009/22/11 12:18:11 PM PDT session 0x13220b93e610557 cxid 0x51 zxid 0x601bd396a closeSession nullHere is the log output for the sessions that tried creating the same node9/22/11 12:17:54 PM PDT session 0x13220b93e61054f cxid 0x42 zxid 0x601bd366b create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-79/22/11 12:17:56 PM PDT session 0x13220b93e610550 cxid 0x42 zxid 0x601bd36ce create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-79/22/11 12:17:58 PM PDT session 0x13220b93e610551 cxid 0x42 zxid 0x601bd3711 create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-79/22/11 12:18:00 PM PDT session 0x13220b93e610552 cxid 0x42 zxid 0x601bd3777 create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-79/22/11 12:18:02 PM PDT session 0x13220b93e610553 cxid 0x42 zxid 0x601bd3802 create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-79/22/11 12:18:05 PM PDT session 0x13220b93e610554 cxid 0x44 zxid 0x601bd385d create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-79/22/11 12:18:07 PM PDT session 0x13220b93e610555 cxid 0x44 zxid 0x601bd38b0 create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-79/22/11 12:18:11 PM PDT session 0x13220b93e610557 cxid 0x52 zxid 0x601bd396b create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7Let me know if you need additional information.","ZOOKEEPER-1208","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-11-15 03:34:12.621","Closed","Ephemeral node not removed after the client session is long gone","Bug","2011-11-24 03:22:11.531",3,11,7818,7933
15922,"2011-10-04 04:24:24.143","According to LSB Core para 20.2:==================================================================================Otherwise,  the exit  status shall  be non­zero,  as de­fined below. In addition to straightforward success, the following situations arealso to be considered successful: • restarting a service (instead of reloading it) with the force­reload argument• running start on a service already running• running stop on a service already stopped or not running• running restart on a service already stopped or not running• running try­restart on a service already stopped or not running==================================================================================Yet, zkServer.sh fails on stop if it can't find a PID file:{noformat}stop)    echo -n ""Stopping zookeeper ... ""    if [ ! -f ""$ZOOPIDFILE"" ]    then      echo ""error: could not find file $ZOOPIDFILE""      exit 1    else      $KILL -9 $(cat ""$ZOOPIDFILE"")      rm ""$ZOOPIDFILE""      echo STOPPED      exit 0    fi{noformat}","ZOOKEEPER-1212","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-10-19 14:42:03.422","Closed","zkServer.sh stop action is not conformat with LSB para 20.2 Init Script Actions","Bug","2011-11-24 03:22:15.679",0,0,7953,7953
15928,"2011-09-28 00:26:16.812","While I always expect to be able to parse a sequential node by looking for digits, under some locals you end up with non digits - for example: n_००००००००००It looks like the problem is around line 236 in PrepRequestProcessor:{code}                if (createMode.isSequential()) {                    path = path + String.format(""%010d"", parentCVersion);                }{code}Instead we should pass Locale.ENGLISH to the format call.{code}                if (createMode.isSequential()) {                    path = path + String.format(Locale.ENGLISH, ""%010d"", parentCVersion);                }{code}Lucene/Solr tests with random Locales, and some of my tests that try and inspect the node name and order things expect to find digits - currently my leader election recipe randomly fails when the wrong locale pops up.","ZOOKEEPER-1206","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-30 05:36:43.596","Closed","Sequential node creation does not use always use digits in node name given certain Locales.","Bug","2011-11-24 03:22:12.533",0,0,8093,8093
15931,"2011-09-24 06:44:59.022","For running these tests, I am following instructions on https://github.com/apache/zookeeper/blob/trunk/src/java/systest/README.txt In Step 4, when I try to run java -jar build/contrib/fatjar/zookeeper-<version>-fatjar.jar systest org.apache.zookeeper.test.system.SimpleSysTest , it throws the following error,Exception in thread ""main"" java.lang.NoClassDefFoundError: junit/framework/TestCaseThe problem is that zookeeper-dev-fatjar.jar does not contain the TestCase class.Patrick Hunt suggested that adding <zipgroupfileset dir=""${zk.root}/build/test/lib"" includes=""*.jar"" /> to fatjar/build.xml should solve the problem and it does.","ZOOKEEPER-1203","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-30 02:27:46.178","Closed","Zookeeper systest is missing Junit Classes ","Bug","2011-11-24 03:22:26.62",0,0,8127,8127
15939,"2011-09-20 23:15:41.063","Tom Klonikowski writes:    Hello developers,    the SaslServerCallbackHandler in trunk changes the principal name    service/host@REALM to service/service@REALM (i guess unintentionally).    lines 131-133:    if (!removeHost() && (kerberosName.getHostName() != null)) {      userName += ""/"" + kerberosName.getServiceName();    }    Server Log:    SaslServerCallbackHandler@115] - Successfully authenticated client:    authenticationID=fetcher/ubook@QUINZOO;    authorizationID=fetcher/ubook@QUINZOO.    SaslServerCallbackHandler@137] - Setting authorizedID:    fetcher/fetcher@QUINZOO","ZOOKEEPER-1195","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-29 15:42:51.173","Closed","SASL authorizedID being incorrectly set: should use getHostName() rather than getServiceName()","Bug","2013-05-02 10:29:44.982",0,1,7842,7842
15940,"2011-09-20 08:10:08.471","Leader.getEpochToPropose() and Leader.waitForNewEpoch() act as barriers - they make sure that a leader/follower can return from calling the method only once connectingFollowers (or electingFollowers) contain a quorum. But these methods don't make sure that the leader itself is in connectingFollowers/electingFollowers. So the leader didn't necessarily reach the barrier when followers pass it. This can cause the following problems:1. If the leader is not in connectingFollowers when a LearnerHandler returns from getEpochToPropose(), then the epoch sent by the leader to the follower might be smaller than the leader's own last accepted epoch.2. If the leader is not in electingFollowers when LearnerHandler returns from waitForNewEpoch() then the leader will send a NEWLEADER message to followers, and the followers will respond, but it is possible that the NEWLEADER message is not in outstandingProposals when these NEWLEADER  acks arrive, which will cause the NEWLEADER acks to be dropped.To fix this I propose to explicitly check that the leader is in connectingFollowers/electingFollowers before anyone can pass these barriers.","ZOOKEEPER-1194","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-11-05 14:38:24.298","Closed","Two possible race conditions during leader establishment","Bug","2011-11-24 03:22:46.083",0,0,7820,7820
15942,"2011-09-19 10:00:24.303","A follower/leader should block in Leader.waitForEpochAck() until either electingFollowers contains a quorum and electionFinished=true or until a timeout occurs. A timeout means that a quorum of followers didn't ack the epoch on time, which is an error. But the check in Leader.waitForEpochAck() is ""if (waitingForNewEpoch) throw..."" and this will never be triggered, even if the wait statement just timed out,  because Leader.getEpochToPropose() completes and sets waitingForNewEpoch to false before Leader.waitForEpochAck() is invoked.Instead of ""if (waitingForNewEpoch) throw"" the condition in Leader.waitForEpochAck() should be ""if (!electionFinished) throw"".The guarded block introduced in ZK-1191 should be checking !electionFinished.","ZOOKEEPER-1192","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-11-05 14:15:12.105","Closed","Leader.waitForEpochAck() checks waitingForNewEpoch instead of checking electionFinished","Bug","2011-11-24 03:22:06.417",0,1,7820,7820
15944,"2011-09-17 08:30:19.902","run ""ant package"" and look in the build/zookeeper-<version>/bin directory. many of the bin scripts are missing.","ZOOKEEPER-1190","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-10-08 04:48:26.76","Closed","ant package is not including many of the bin scripts in the package (zkServer.sh for example)","Bug","2011-11-24 03:22:49.937",0,1,8112,7818
15945,"2011-09-16 22:29:05.822","When loading the snapshot, ZooKeeper will consider only the 'snapshots with atleast 10 bytes size'. Otherwsie it will ignore and just return without closing the RandomAccessFile.{noformat}Util.isValidSnapshot() having the following logic.        // Check for a valid snapshot        RandomAccessFile raf = new RandomAccessFile(f, ""r"");        // including the header and the last / bytes        // the snapshot should be atleast 10 bytes        if (raf.length() < 10) {            return false;        }{noformat}Since the snapshot file validation logic is outside try block, it won't go to the finally block and will be leaked.Suggestion: Move the validation logic to the try/catch block.","ZOOKEEPER-1189","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-27 09:11:20.687","Closed","For an invalid snapshot file(less than 10bytes size) RandomAccessFile stream is leaking.","Bug","2011-11-24 03:22:22.355",0,0,7806,7806
15949,"2011-09-15 09:11:09.719","There are 3 places where ClientCnxn should queue a AuthFailed event if client fails to authenticate. Without sending this event, clients may be stuck watching for a SaslAuthenticated event that will never come (since the client failed to authenticate).","ZOOKEEPER-1185","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-27 10:09:47.017","Closed","Send AuthFailed event to client if SASL authentication fails","Bug","2013-05-02 10:29:44.761",0,1,7842,7842
15960,"2011-09-09 02:47:42.742","In the socket connection logic there are several errors that result in bad behavior.  The basic problem is that a socket is registered with a selector unconditionally when there are nuances that should be dealt with.  First, the socket may connect immediately.  Secondly, the connect may throw an exception.  In either of these two cases, I don't think that the socket should be registered.I will attach a test case that demonstrates the problem.  I have been unable to create a unit test that exhibits the problem because I would have to mock the low level socket libraries to do so.  It would still be good to do so if somebody can figure out a good way.","ZOOKEEPER-1174","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-10-01 06:02:40.562","Closed","FD leak when network unreachable","Bug","2011-11-24 03:22:48.85",0,0,7925,7925
15953,"2011-09-14 07:38:55.229","Currently, in Zookeeper trunk, there are two problems with Kerberos TGT renewal:1. TGTs obtained from a keytab are not refreshed periodically. They should be, just as those from ticket cache are refreshed.2. Ticket renewal should be retried if it fails. Ticket renewal might fail if two or more separate processes (different JVMs) running as the same user try to renew Kerberos credentials at the same time. ","ZOOKEEPER-1181","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-10-24 14:47:23.157","Closed","Fix problems with Kerberos TGT renewal","Bug","2013-05-02 10:29:44.696",0,1,7842,7842
15963,"2011-09-03 03:35:51.709","I tried testing out zk on java 7 (not yet officially supported) but I ran into a road block due to the build failing. Patch coming next.","ZOOKEEPER-1171","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-14 05:50:58.413","Closed","fix build for java 7","Bug","2011-11-24 03:22:48.638",0,0,7818,7818
15966,"2011-09-01 03:52:13.698","OS: Windows 64-bitJRE: IKVM 7.0.4258IKVM 7.0.4258 does not support ManagementFactory.getPlatformMBeanServer(); It will throw a java.lang.Error.","ZOOKEEPER-1168","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-02 01:15:50.149","Closed","ZooKeeper fails to run with IKVM","Bug","2011-11-24 03:22:07.601",0,0,8132,8132
15969,"2011-08-30 06:08:07.12","The Eclipse test runner tries to run tests from all classes that inherit from TestCase. However, this class is inherited by at least one class (org.apache.zookeeper.test.system.BaseSysTest) that has no test cases as it is used as infrastructure for other real test cases. This patch annotates that class with @Ignore, which causes the class to be Ignored. Also, due to the way annotations are not inherited by default, this patch will not affect classes that inherit from this class.","ZOOKEEPER-1165","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-03 01:02:49.717","Closed","better eclipse support in tests","Bug","2011-11-24 03:22:00.944",0,0,7993,7993
15978,"2011-08-19 01:48:07.943","The log truncation relies on position calculation for a particular zxid to figure out the new size of the log file. There is a bug in PositionInputStream implementation which skips counting the bytes in the log which have value 0. This can lead to underestimating the actual log size. The log records which should be there can get truncated, leading to data loss on the participant which is executing the trunc.Clients can see different values depending on whether they connect to the node on which trunc was executed. ","ZOOKEEPER-1156","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-06 04:04:35.765","Closed","Log truncation truncating log too much - can cause data loss","Bug","2011-11-24 03:22:43.938",0,1,8071,8071
15980,"2011-08-16 01:36:07.1","If a participant with the highest zxid (lets call it A) isn't present during leader election, a participant with a lower zxid (say B) might be chosen as a leader. When A comes up, it will replay the log with that higher zxid. The change that was in that higher zxid will only be visible to the clients connecting to the participant A, but not to other participants.I was able to reproduce this problem by1. connect debugger to B and C and suspend them, so they don't write anything2. Issue an update to the leader A.3. After a few seconds, crash all servers (A,B,C)4. Start B and C, let the leader election take place5. Start A.6. You will find that the update done in step 2 is visible on A but not on B,C, hence the inconsistency.Below is a more detailed analysis of what is happening in the code.Initial Condition1.	Lets say there are three nodes in the ensemble A,B,C with A being the leader2.	The current epoch is 7. 3.	For simplicity of the example, lets say zxid is a two digit number, with epoch being the first digit.4.	The zxid is 735.	All the nodes have seen the change 73 and have persistently logged it.Step 1Request with zxid 74 is issued. The leader A writes it to the log but there is a crash of the entire ensemble and B,C never write the change 74 to their log.Step 3B,C restart, A is still downB,C form the quorumB is the new leader. Lets say  B minCommitLog is 71 and maxCommitLog is 73epoch is now 8, zxid is 80Request with zxid 81 is successful. On B, minCommitLog is now 71, maxCommitLog is 81Step 4A starts up. It applies the change in request with zxid 74 to its in-memory data treeA contacts B to registerAsFollower and provides 74 as its ZxIdSince 71<=74<=81, B decides to send A the diff. B will send to A the proposal 81.Problem:The problem with the above sequence is that A's data tree has the update from request 74, which is not correct. Before getting the proposals 81, A should have received a trunc to 73. I don't see that in the code. If the maxCommitLog on B hadn't bumped to 81 but had stayed at 73, that case seems to be fine.","ZOOKEEPER-1154","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-06 04:04:21.751","Closed","Data inconsistency when the node(s) with the highest zxid is not present at the time of leader election","Bug","2011-11-24 03:22:45.186",0,1,8071,8071
15982,"2011-08-13 04:27:37.41","Exceptions thrown by an AuthenticationProvider's handleAuthentication method will not be caught, and can cause the buffers in the NIOServer to not read requests fully or properly. Any exceptions thrown here should be caught and treated as auth failure. ","ZOOKEEPER-1152","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-08-21 09:05:50.011","Closed","Exceptions thrown from handleAuthentication can cause buffer corruption issues in NIOServer","Bug","2011-11-24 03:22:16.398",0,0,7832,7832
15988,"2011-08-05 01:21:13.579","I tried running my latency tester against trunk, in so doing I noticed that the C/Python (not sure which yet) client performance has seriously degraded since 3.3.3.The first run (below) is with released 3.3.3 client against a 3 server ensemble running released 3.3.3 server code. The second run is the exact same environment (same ensemble), however using trunk c/zkpython client.Notice:1) in the first run operations are approx 10ms/write, 0.25ms/read - which is pretty much what's expected.2) however in the second run we are seeing 50ms/operation regardless of read or write.{noformat}[phunt@c0309 zk-smoketest-3.3.3]$ PYTHONPATH=lib.linux-x86_64-2.6/ LD_LIBRARY_PATH=lib.linux-x86_64-2.6/ python26 ./zk-latencies.py --servers ""c0309:2181,c0310:2181,c0311:2181"" --znode_size=100 --znode_count=100 --timeout=5000 --synchronousConnecting to c0309:2181Connected in 16 ms, handle is 0Connecting to c0310:2181Connected in 16 ms, handle is 1Connecting to c0311:2181Connected in 15 ms, handle is 2Testing latencies on server c0309:2181 using syncronous callscreated     100 permanent znodes  in    959 ms (9.599378 ms/op 104.173415/sec)set         100           znodes  in    933 ms (9.332101 ms/op 107.157002/sec)get         100           znodes  in     27 ms (0.270889 ms/op 3691.551589/sec)deleted     100 permanent znodes  in    881 ms (8.812950 ms/op 113.469388/sec)created     100 ephemeral znodes  in    956 ms (9.564152 ms/op 104.557103/sec)watched     100           znodes  in     26 ms (0.264361 ms/op 3782.707587/sec)deleted     100 ephemeral znodes  in    881 ms (8.819292 ms/op 113.387792/sec)notif       100           watches in    999 ms (9.994299 ms/op 100.057038/sec)Testing latencies on server c0310:2181 using syncronous callscreated     100 permanent znodes  in    964 ms (9.640460 ms/op 103.729490/sec)set         100           znodes  in    933 ms (9.332800 ms/op 107.148981/sec)get         100           znodes  in     29 ms (0.299308 ms/op 3341.036650/sec)deleted     100 permanent znodes  in    886 ms (8.864651 ms/op 112.807603/sec)created     100 ephemeral znodes  in    958 ms (9.585140 ms/op 104.328161/sec)watched     100           znodes  in     30 ms (0.300801 ms/op 3324.459240/sec)deleted     100 ephemeral znodes  in    886 ms (8.865030 ms/op 112.802779/sec)notif       100           watches in   1000 ms (10.000212 ms/op 99.997878/sec)Testing latencies on server c0311:2181 using syncronous callscreated     100 permanent znodes  in    958 ms (9.582071 ms/op 104.361569/sec)set         100           znodes  in    935 ms (9.359350 ms/op 106.845024/sec)get         100           znodes  in     25 ms (0.252700 ms/op 3957.263893/sec)deleted     100 permanent znodes  in    891 ms (8.913291 ms/op 112.192013/sec)created     100 ephemeral znodes  in    958 ms (9.584489 ms/op 104.335246/sec)watched     100           znodes  in     25 ms (0.251091 ms/op 3982.627356/sec)deleted     100 ephemeral znodes  in    891 ms (8.915379 ms/op 112.165730/sec)notif       100           watches in   1000 ms (10.000508 ms/op 99.994922/sec)Latency test complete[phunt@c0309 zk-smoketest-3.3.3]$ cd ../zk-smoketest-trunk/[phunt@c0309 zk-smoketest-trunk]$ PYTHONPATH=lib.linux-x86_64-2.6/ LD_LIBRARY_PATH=lib.linux-x86_64-2.6/ python26 ./zk-latencies.py --servers ""c0309:2181,c0310:2181,c0311:2181"" --znode_size=100 --znode_count=100 --timeout=5000 --synchronousConnecting to c0309:2181Connected in 31 ms, handle is 0Connecting to c0310:2181Connected in 16 ms, handle is 1Connecting to c0311:2181Connected in 16 ms, handle is 2Testing latencies on server c0309:2181 using syncronous callscreated     100 permanent znodes  in   5099 ms (50.999281 ms/op 19.608119/sec)set         100           znodes  in   5066 ms (50.665429 ms/op 19.737324/sec)get         100           znodes  in   4009 ms (40.093150 ms/op 24.941916/sec)deleted     100 permanent znodes  in   5040 ms (50.404449 ms/op 19.839519/sec)created     100 ephemeral znodes  in   5124 ms (51.249170 ms/op 19.512511/sec)watched     100           znodes  in   4051 ms (40.514441 ms/op 24.682557/sec)deleted     100 ephemeral znodes  in   5048 ms (50.484939 ms/op 19.807888/sec)notif       100           watches in   1000 ms (10.004182 ms/op 99.958199/sec)Testing latencies on server c0310:2181 using syncronous callscreated     100 permanent znodes  in   5115 ms (51.157510 ms/op 19.547472/sec)set         100           znodes  in   5056 ms (50.568910 ms/op 19.774996/sec)get         100           znodes  in   4099 ms (40.999382 ms/op 24.390612/sec)deleted     100 permanent znodes  in   5041 ms (50.418010 ms/op 19.834182/sec)created     100 ephemeral znodes  in   5083 ms (50.835850 ms/op 19.671157/sec)watched     100           znodes  in   4100 ms (41.003261 ms/op 24.388304/sec)deleted     100 ephemeral znodes  in   5058 ms (50.581930 ms/op 19.769906/sec)notif       100           watches in   1000 ms (10.005081 ms/op 99.949219/sec)Testing latencies on server c0311:2181 using syncronous callscreated     100 permanent znodes  in   5099 ms (50.992720 ms/op 19.610642/sec)set         100           znodes  in   5091 ms (50.916569 ms/op 19.639972/sec)get         100           znodes  in   4099 ms (40.996401 ms/op 24.392385/sec)deleted     100 permanent znodes  in   5066 ms (50.669601 ms/op 19.735699/sec)created     100 ephemeral znodes  in   5124 ms (51.249208 ms/op 19.512496/sec)watched     100           znodes  in   4099 ms (40.999141 ms/op 24.390755/sec)deleted     100 ephemeral znodes  in   5049 ms (50.498819 ms/op 19.802443/sec)notif       100           watches in    999 ms (9.997852 ms/op 100.021486/sec)Latency test complete{noformat}","ZOOKEEPER-1146","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-08-15 01:30:58.715","Closed","significant regression in client (c/python) performance","Bug","2011-11-24 03:22:40.681",0,1,7818,7818
15989,"2011-08-04 06:36:07.07","Use the attached repeat.sh to run ObserverTest repeatedly by doing: src/repeat.sh ObserverTestThe test will will fail eventually after a few iterations; should be only a few minutes.The line that fails in the test is: zk = new ZooKeeper(""127.0.0.1:"" + CLIENT_PORT_OBS,                ClientBase.CONNECTION_TIMEOUT, this);Attached as out.txt is the output showing a successful run, for comparison, followed by a failed run.Note that in the seconds before the test fails, in the following lines, that there is a 24 second gap in time (between 22:13:02 and 22:13:26):bq.[junit] 2011-08-03 22:13:02,167 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11229:ZooKeeperServer@833] - Client attempting to establish new session at /127.0.0.1:46929[junit] 2011-08-03 22:13:26,003 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11228:Leader@419] - Shutting down[junit] 2011-08-03 22:13:26,003 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:11228:Leader@425] - Shutdown called[junit] java.lang.Exception: shutdown Leader! reason: Only 0 followers, need 1","ZOOKEEPER-1145","Blocker","ZOOKEEPER","ZooKeeper","ASF","Duplicate","2011-08-15 09:02:59.501","Closed","ObserverTest.testObserver fails at particular point after several runs of ant junt.run -Dtestcase=ObserverTest","Bug","2011-11-24 03:22:43.408",0,0,8126,7842
15990,"2011-08-04 06:35:49.802","I have found one problem that is causing QuorumPeerMainTest:testQuorum to fail. This test uses 2 ZK servers. The test is failing because leader is not starting ZooKeeperServer after leader election. so everything halts.With the new changes, the server is now started in Leader.processAck() which is called from LeaderHandler. processAck() starts ZooKeeperServer if majority have acked NEWLEADER. The leader puts its ack in the the ackSet in Leader.lead(). Since processAck() is called from LearnerHandler it can happen that the learner's ack is processed before the leader is able to put its ack in the ackSet. When LearnerHandler invokes processAck(), the ackSet for newLeaderProposal will not have quorum (in this case 2). As a result, the ZooKeeperServer is never started on the Leader.The leader needs to ensure that its ack is put in ackSet before starting LearnerCnxAcceptor or invoke processAck() itself after adding to ackSet. I haven't had time to go through the ZAB2 changes so I am not too familiar with the code. Can Ben/Flavio fix this?","ZOOKEEPER-1144","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-08-12 02:10:08.034","Closed","ZooKeeperServer not starting on leader due to a race condition","Bug","2011-11-24 03:22:46.251",0,0,8126,8126
15992,"2011-08-03 08:29:06.612","stat output seems to be missing some end of line:{noformat}echo stat |nc c0309 2181Zookeeper version: 3.4.0--1, built on 08/02/2011 22:25 GMTClients: /172.29.81.91:33378[0](queued=0,recved=1,sent=0Latency min/avg/max: 0/28/252Received: 246844Sent: 266737Outstanding: 0Zxid: 0x4000508c2Mode: followerNode count: 4{noformat}Multiple clients end up on the same line (missing newline)","ZOOKEEPER-1142","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-08-11 14:00:40.77","Closed","incorrect stat output","Bug","2011-11-24 03:22:21.553",0,0,7818,7818
15993,"2011-08-03 07:31:40.819","""ant test"" under python 2.4 is failing due to a small issue in the test code - using a new feature introduced in 2.5.I have a small patch which addresses this, after which I was able to compile and run the tests successfully under python 2.4.","ZOOKEEPER-1141","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-08-15 08:41:47.535","Closed","zkpython fails tests under python 2.4","Bug","2011-11-24 03:22:39.538",0,0,7818,7818
15994,"2011-07-30 00:49:37.972","Near the end of QuorumZxidSyncTest there are tons of threads running - 115 ""ProcessThread"" threads, similar numbers of SessionTracker.Also I see ~100 ReadOnlyRequestProcessor - why is this running as a separate thread? (henry/flavio?)","ZOOKEEPER-1140","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-08-30 14:37:27.216","Closed","server shutdown is not stopping threads","Bug","2011-11-24 03:22:38.015",0,0,8043,7818
15995,"2011-07-28 05:42:05.018","cleanup jenkins report, currently 2 compiler warnings being reported.","ZOOKEEPER-1139","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-29 07:23:57.619","Closed","jenkins is reporting two warnings, fix these","Bug","2011-11-24 03:22:41.772",0,0,7818,7818
15996,"2011-07-28 04:04:23.541","I'm seeing a number of problems in the release audit output for 3.4.0, these must be fixed before 3.4.0 release:{noformat}[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/contrib/ZooInspector/config/defaultConnectionSettings.cfg[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/contrib/ZooInspector/config/defaultNodeVeiwers.cfg[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/contrib/ZooInspector/licences/epl-v10.html[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/Cli.vcproj[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/include/winconfig.h[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/include/winstdint.h[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/zookeeper.sln[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/zookeeper.vcproj[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/huebrowser/zkui/src/zkui/static/help/index.html[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/huebrowser/zkui/src/zkui/static/js/package.yml[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/log4j.properties[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/date.format.js[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.bar.js[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.dot.js[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.line.js[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.pie.js[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.raphael.js[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/raphael.js[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/yui-min.js[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/monitoring/JMX-RESOURCES[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/zooinspector/config/defaultConnectionSettings.cfg[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/zooinspector/config/defaultNodeVeiwers.cfg[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/zooinspector/lib/log4j.properties[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/zooinspector/licences/epl-v10.html[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/java/test/org/apache/zookeeper/MultiTransactionRecordTest.java[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/java/test/org/apache/zookeeper/server/quorum/LearnerTest.javaLines that start with ????? in the release audit report indicate files that do not have an Apache license header.{noformat}","ZOOKEEPER-1138","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-29 06:06:09.344","Closed","release audit failing for a number of new files","Bug","2011-11-24 03:22:28.473",0,0,7818,7818
15998,"2011-07-27 00:58:51.44","the NEW_LEADER message was sent at the beginning of the sync phase in Zab pre1.0, but it must be at the end in Zab 1.0. if the protocol is 1.0 or greater we need to queue rather than send the packet.","ZOOKEEPER-1136","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-14 14:59:35.248","Closed","NEW_LEADER should be queued not sent to match the Zab 1.0 protocol on the twiki","Bug","2011-11-24 03:22:08.627",0,1,7829,7829
16000,"2011-07-23 06:18:55.05","Noticed string comparison using == rather than equals.","ZOOKEEPER-1134","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-26 05:32:33.368","Closed","ClientCnxnSocket string comparison using == rather than equals","Bug","2011-11-24 03:22:08.182",0,0,7818,7818
16010,"2011-07-14 00:17:58.307","The new Multiop support added under zookeeper-965 fails every single time if the multiop is submitted to a non-leader in quorum mode. In standalone mode it always works properly and this bug only presents itself in quorum mode (with 2 or more nodes). After 12 hours of debugging (*sigh*) it turns out to be a really simple fix. There are a couple of missing case statements inside FollowerRequestProcessor.java and ObserverRequestProcessor.java to ensure that multiop is forwarded to the leader for commit. I've attached a patch that fixes this problem.It's probably worth nothing that zookeeper-965 has already been committed to trunk. But this is a fatal flaw that will prevent multiop support from working properly and as such needs to get committed to 3.4.0 as well. Is there a way to tie these two cases together in some way?","ZOOKEEPER-1124","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-15 12:51:26.697","Closed","Multiop submitted to non-leader always fails due to timeout","Bug","2011-11-24 03:22:26.03",0,0,7824,7824
16015,"2011-07-07 18:33:08.798","Hello, adding the following commented-out dataDir to the zoo.cfg file (keeping the default one provided active):{noformat}# the directory where the snapshot is stored.# dataDir=test123/datadataDir=/export/crawlspace/mahadev/zookeeper/server1/data{noformat}and then running sh zkServer.sh stop is showing that the program is incorrectly reading the commented-out dataDir:{noformat}gmazza@gmazza-work:~/dataExt3/apps/zookeeper-3.3.3/bin$ sh zkServer.sh stopJMX enabled by defaultUsing config: /media/NewDriveExt3_/apps/zookeeper-3.3.3/bin/../conf/zoo.cfgStopping zookeeper ... error: could not find file test123/data/export/crawlspace/mahadev/zookeeper/server1/data/zookeeper_server.pidgmazza@gmazza-work:~/dataExt3/apps/zookeeper-3.3.3/bin$ {noformat}If I change the commented-out line in zoo.cfg to ""test123456/data"" and run the stop command again I get:error: could not find file test123456/datashowing that it's incorrectly doing a run-time read of the commented-out lines.  (Difficult to completely confirm, but this problem  doesn't appear to occur with the start command, only the stop one.)","ZOOKEEPER-1119","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-26 06:22:04.225","Closed","zkServer stop command incorrectly reading comment lines in zoo.cfg","Bug","2011-11-24 03:22:44.388",0,1,7818,8149
16017,"2011-07-05 22:50:48.125","zookeeper 3.3.3 (and 3.3.1) fails to build on Debian and Ubuntu systems with gcc >= 4.6.1:/bin/bash ./libtool  --tag=CC   --mode=compile gcc -DHAVE_CONFIG_H -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O2 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c -o zookeeper.lo `test -f 'src/zookeeper.c' || echo './'`src/zookeeper.clibtool: compile:  gcc -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -Wall -Werror -g -O2 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c src/zookeeper.c  -fPIC -DPIC -o .libs/zookeeper.osrc/zookeeper.c: In function 'getaddrs':src/zookeeper.c:455:13: error: variable 'port' set but not used [-Werror=unused-but-set-variable]cc1: all warnings being treated as errorsSee http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=625441 for more information.","ZOOKEEPER-1117","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-08-26 15:52:15.05","Closed","zookeeper 3.3.3 fails to build with gcc >= 4.6.1 on Debian/Ubuntu","Bug","2011-11-24 03:22:32.985",0,1,7922,7922
16023,"2011-06-30 16:51:00.653","As stated in the title, org.apache.zookeeper.test.JMXEnv uses System.err.println to output traces. This makes for a lot of noise on the console when you run the tests. It has a logging object already, so it should use that instead.","ZOOKEEPER-1111","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-20 05:39:23.949","Closed","JMXEnv uses System.err instead of logging","Bug","2011-11-24 03:22:07.359",0,0,7956,7956
16033,"2011-06-22 01:15:16.007","These are generated by ant package since ZOOKEEPER-1042, they just need to be pushed to a maven repo. Bookkeeper requires this package to build.","ZOOKEEPER-1101","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-08-02 02:31:57.744","Closed","Upload zookeeper-test maven artifacts to maven repository.","Bug","2011-11-24 03:22:28.63",0,0,7818,7956
16025,"2011-06-24 12:48:18.96","*Problem* Zookeeper is not shut down completely when dataDir disk space is full and ZK Cluster went into unserviceable state. *Scenario*If the leader zookeeper disk is made full, the zookeeper is trying to shutdown. But it is waiting indefinitely while shutting down the SyncRequestProcessor thread.*Root Cause* this.join() is invoked in the same thread where System.exit(11) has been triggered.When disk space full happens, It got the exception as follows 'No space left on device' and invoked System.exit(11) from the SyncRequestProcessor thread(The following logs shows the same). Before exiting JVM, ZK will execute the ShutdownHook of QuorumPeerMain and the flow comes to SyncRequestProcessor.shutdown(). Here this.join() is invoked in the same thread where System.exit(11) has been invoked.","ZOOKEEPER-1109","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-26 05:01:11.184","Closed","Zookeeper service is down when SyncRequestProcessor meets any exception.","Bug","2011-11-24 03:22:24.926",0,1,8043,8043
16026,"2011-06-24 05:02:34.026","3 issues:In zoo_add_auth: there is a race condition:   2940     // [ZOOKEEPER-800] zoo_add_auth should return ZINVALIDSTATE if   2941     // the connection is closed.   2942     if (zoo_state(zh) == 0) {   2943         return ZINVALIDSTATE;   2944     }when we do zookeeper_init, the state is initialized to 0 and above we check if state = 0 then throw exception.There is a race condition where the doIo thread is slow and has not changed the state to CONNECTING, then you end up returning back ZKINVALIDSTATE.The problem is we use 0 for CLOSED state and UNINITIALIZED state. in case of uninitialized case it should let it go through.2nd issue:Another Bug: in send_auth_info, the check is not correctwhile (auth->next != NULL) { //--BUG: in cases where there is only one auth in the list, this will never send that auth, as its next will be NULL    rc = send_info_packet(zh, auth);    auth = auth->next; }FIX IS:do {   rc = send_info_packet(zh, auth);   auth = auth->next;  } while (auth != NULL); //this will make sure that even if there is one auth ,that will get sent.3rd issue:   2965     add_last_auth(&zh->auth_h, authinfo);   2966     zoo_unlock_auth(zh);   2967   2968     if(zh->state == ZOO_CONNECTED_STATE || zh->state == ZOO_ASSOCIATING_STATE)   2969         return send_last_auth_info(zh);if it is connected, we only send the last_auth_info, which may be different than the one we added, as we unlocked it before sending it.","ZOOKEEPER-1108","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-09 10:27:39.664","Closed","Various bugs in zoo_add_auth in C","Bug","2011-11-24 03:22:45.338",0,5,8139,8139
16043,"2011-06-10 11:21:23.876","if the chrootPath of ClientCnxn is not null and the Watches of zooKeeper is not null; and then for some reason(like zookeeper server stop and start), the zookeeper client will primeConnection to server again and tell server the watcher path,but the path is wrong,it show be serverpath but not clientpath;if the wrong watcher clientPath is sended to server,the exception will occurr, the exceptions:2011-06-10 04:33:16,935 [pool-2-thread-30-SendThread(DB1-6:2181)] WARN  org.apache.zookeeper.ClientCnxn - Session 0x5302c4403a30232 for server DB1-6/192.168.1.6:2181, unexpected error, closing socket connection and attempting reconnectjava.lang.StringIndexOutOfBoundsException: String index out of range: -6	at java.lang.String.substring(String.java:1937)	at java.lang.String.substring(String.java:1904)	at org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:794)	at org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:881)	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1130) ","ZOOKEEPER-1091","Critical","ZOOKEEPER","ZooKeeper","ASF","Duplicate","2011-10-17 02:21:22.624","Closed","when the chrootPath of ClientCnxn is not null and the Watches of zooKeeper is not null and the method primeConnection(SelectionKey k) of ClientCnxn Occurred again for some reason ,then the wrong watcher clientPath is sended to server","Bug","2011-11-24 03:22:30.401",0,3,NULL,8167
16044,"2011-06-09 22:24:06.527","I think I have found a bug in the snapshot mechanism.The problem occurs because dt.lastProcessedZxid is not synchronized (or rather set before the data tree is modified):FileTxnSnapLog:{code}    public void save(DataTree dataTree,            ConcurrentHashMap<Long, Integer> sessionsWithTimeouts)        throws IOException {        long lastZxid = dataTree.lastProcessedZxid;        LOG.info(""Snapshotting: "" + Long.toHexString(lastZxid));        File snapshot=new File(                snapDir, Util.makeSnapshotName(lastZxid));        snapLog.serialize(dataTree, sessionsWithTimeouts, snapshot);   <=== the Datatree may not have the modification for lastProcessedZxid    }{code}DataTree:{code}    public ProcessTxnResult processTxn(TxnHeader header, Record txn) {        ProcessTxnResult rc = new ProcessTxnResult();        String debug = """";        try {            rc.clientId = header.getClientId();            rc.cxid = header.getCxid();            rc.zxid = header.getZxid();            rc.type = header.getType();            rc.err = 0;            if (rc.zxid > lastProcessedZxid) {                lastProcessedZxid = rc.zxid;            }            [...modify data tree...]            }{code}The lastProcessedZxid must be set after the modification is done.As a result, if server crashes after taking the snapshot (and the snapshot does not contain change corresponding to lastProcessedZxid) restore will not restore the data tree correctly:{code}public long restore(DataTree dt, Map<Long, Integer> sessions,            PlayBackListener listener) throws IOException {        snapLog.deserialize(dt, sessions);        FileTxnLog txnLog = new FileTxnLog(dataDir);        TxnIterator itr = txnLog.read(dt.lastProcessedZxid+1); <=== Assumes lastProcessedZxid is deserialized }{code}I have had offline discussion with Ben and Camille on this. I will be posting the discussion shortly.","ZOOKEEPER-1090","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-28 13:50:38.974","Closed","Race condition while taking snapshot can lead to not restoring data tree correctly","Bug","2011-11-24 03:22:34.132",0,3,8126,8126
16047,"2011-06-07 04:09:54.569","Cannot use forceSync=no to asynchronously write transaction logs. This is a critical bug, please address it ASAP. More details:The class org.apache.zookeeper.server.persistence.FileTxnLog initializes forceSync property in a static block. However, the static variable is defined after the static block with a default value of true. Therefore, the value of the variable can never be false. Please move the declaration of the variable before the static block.","ZOOKEEPER-1087","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-06-21 13:34:27.77","Closed","ForceSync VM arguement not working when set to ""no""","Bug","2011-11-24 03:22:19.256",0,1,8169,8170
16048,"2011-06-01 18:52:19.998","The zookeeper test jar, (zookeeper-<version>-test.jar) depends on accessive.jar which is not available in maven. This is problematic for projects using the test jar (i.e. hedwig). ","ZOOKEEPER-1086","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-10-19 14:56:15.511","Closed","zookeeper test jar has non mavenised dependency.","Bug","2011-11-24 03:22:07.944",0,1,7956,7956
16051,"2011-06-01 00:23:44.759","See title.","ZOOKEEPER-1083","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-06-14 01:25:19","Closed","Javadoc for WatchedEvent not being generated","Bug","2011-11-24 03:22:01.482",0,0,7956,7956
16058,"2011-05-26 06:19:48.13","Some tests are unnecessarily extending QuorumBase. Typically this is not a big issue, but it may cause more servers than necessary to be started (harder to debug a failing test in particular).","ZOOKEEPER-1076","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-29 16:14:46.019","Closed","some quorum tests are unnecessarily extending QuorumBase","Bug","2011-11-24 03:22:29.154",0,0,7818,7818
16060,"2011-05-26 02:27:39.401","zkServer.sh is missing nohup and ""sleep 1"" when starting the background daemon.This is fine normally, however when running the server remotely via ssh this causes the process to not run successfully (it starts but immediately exits).I'll be submitting a patch for this shortly.","ZOOKEEPER-1074","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-06-27 13:04:09.41","Closed","zkServer.sh is missing nohup/sleep, which are necessary for remote invocation","Bug","2011-11-24 03:22:12.898",0,0,7818,7818
16061,"2011-05-26 01:52:08.061","ZOOKEEPER-1030 updated the generated docs, not the source docs. I'll submit a patch to address in the src.","ZOOKEEPER-1073","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-07 15:32:33.586","Closed","address a documentation issue in ZOOKEEPER-1030","Bug","2011-11-24 03:22:24.195",0,0,7818,7818
16065,"2011-05-24 07:53:31.183","I've only seen this happen once.  In order to restart Zookeeper with a new set of servers, we have a wrapper class that calls shutdown() on an existing QuorumPeer, and then starts a new one with a new set of servers.  Specifically, our shutdown code looks like this:{code}  synchronized(_quorum_peer) {    _quorum_peer.shutdown();    FastLeaderElection fle = (FastLeaderElection) _quorum_peer.getElectionAlg();    fle.shutdown();  // I think this is unnecessary    try {      _quorum_peer.getTxnFactory().commit();    } catch (java.nio.channels.ClosedChannelException e) {      // ignore    }  }{code}One time, our wrapper class started one QuorumPeer, and then had to shut it down and start a new one very soon after the QuorumPeer transitioned into a FOLLOWING state.  When the new QuorumPeer tried to read in the latest log from disk, it encountered a bogus magic number of all zeroes:{noformat}2011-05-18 22:42:29,823 10467 [pool-1-thread-2] FATAL org.apache.zookeeper.server.quorum.QuorumPeer  - Unable to load database on diskjava.io.IOException: Transaction log: /var/cloudnet/data/zookeeper/version-2/log.700000001 has invalid magic number 0 != 1514884167        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:510)        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:527)        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:493)        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:576)        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:479)        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:454)        at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:325)        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:126)        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222)        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:398)...2011-05-18 22:42:29,823 10467 [pool-1-thread-2] ERROR com.nicira.onix.zookeeper.Zookeeper  - Unexpected exceptionjava.lang.RuntimeException: Unable to run quorum server         at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:401)        at com.nicira.onix.zookeeper.Zookeeper.StartZookeeper(Zookeeper.java:198)        at com.nicira.onix.zookeeper.Zookeeper.RestartZookeeper(Zookeeper.java:277)        at com.nicira.onix.zookeeper.ZKRPCService.setServers(ZKRPC.java:83)        at com.nicira.onix.zookeeper.Zkrpc$ZKRPCService.callMethod(Zkrpc.java:8198)        at com.nicira.onix.rpc.RPC$10.run(RPC.java:534)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)        at java.lang.Thread.run(Thread.java:662)Caused by: java.io.IOException: Transaction log: /var/cloudnet/data/zookeeper/version-2/log.700000001 has invalid magic number 0 != 1514884167        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:510)        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:527)        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:493)        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:576)        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:479)        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:454)        at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:325)        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:126)        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222)        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:398)        ... 8 more{noformat}I looked into the code a bit, and I believe the problem comes from the fact that QuorumPeer.shutdown() does not join() on this before returning.  Here's the scenario I think can happen:# QuorumPeer.run() notices it is in the FOLLOWING state, makes a new Follower, and calls Follower.followLeader(), which starts connecting to the leader.# In the main program thread, QuorumPeer.shutdown() is called.# Through a complicated series of calls, this eventually leads to FollowerZooKeeperServer.shutdown() being called.# This method calls SyncRequestProcess.shutdown(), which joins on this and returns.  However, it's possible that the SyncRequestProcessor thread hasn't yet been started because followLeader() hasn't yet called Learner.syncWithLeader(), which hasn't yet called ZooKeeperServer.startup(), which actually starts the thread.  Thus, the join would have no request, though a requestOfDeath is added to the queued requests list (possibly behind other requests).# Back in the main thread, FileTxnSnapLog.commit() is called, which doesn't do much because the processor hasn't processed anything yet.# Finally, ZooKeeperServer.startup is called in the QuorumPeer.run() thread, starting up the SyncRequestProcessor thread.# That thread appends some request to the log.  The log doesn't exist yet, so it creates a new one, padding it with zeroes.# Now either the SyncRequestProcessor hits the requestOfDeath or the whole QuorumPeer object is deleted.  It exits that thread without ever committing the log to disk (or the new QuorumPeer tries to read the log before the old thread gets to commit anything), and the log ends up with all zeroes instead of a proper magic number.I haven't yet looked into whether there's an easy way to join() on the QuorumPeer thread from shutdown(), so that it won't go on to start the processor threads after it's been shutdown.  I wanted to check with the group first and see if anyone else agrees this could be a problem.I marked this as minor since I think almost no one else uses Zookeeper this way, but it's pretty important to me personally.I will upload a log file showing this behavior shortly.","ZOOKEEPER-1069","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-17 22:36:48.114","Closed","Calling shutdown() on a QuorumPeer too quickly can lead to a corrupt log","Bug","2011-11-24 03:22:21.908",0,0,8126,8053
16066,"2011-05-24 07:08:12.766","Documentation and default config suggest /var/zookeeper as a value for dataDir. This practice is, strictly speaking, incompatible with UNIX/Linux filesystem layout standards (e.g. http://www.s-gms.ms.edus.si/cgi-bin/man-cgi?filesystem+5 , http://tldp.org/LDP/Linux-Filesystem-Hierarchy/html/index.html  ). Even though Zookeeper use is not limited to UNIX-like OSes I'd recommend that we change references to /var/zookeeper to /var/lib/zookeeper","ZOOKEEPER-1068","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-06-22 01:24:41.779","Closed","Documentation and default config suggest incorrect location for Zookeeper state","Bug","2011-11-24 03:22:45.732",0,0,7953,7953
16071,"2011-05-18 05:34:09.995","Synchronization around dataWatches, existWatches and childWatches in Zookeeper is incorrect.Synchronization around outgoingQueue and pendingQueue in ClientCnxnSocketNIO is incorrect.Synchronization around selector and key sets in ClientCnxnSocketNIO seems odd.","ZOOKEEPER-1063","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-15 12:11:22.129","Closed","Dubious synchronization in Zookeeper and ClientCnxnSocketNIO classes","Bug","2011-11-24 03:22:02.689",0,1,8179,8179
16073,"2011-05-11 04:38:36.387","The zkServer.sh script doesn't check properly to see if a previously startedserver is still running.  If you call start twice, the second invocationwill over-write the PID file with a process that then fails due to portoccupancy.This means that stop will subsequently fail.Here is a reference that describes how init scripts should normally work:http://refspecs.freestandards.org/LSB_3.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html","ZOOKEEPER-1061","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-05-17 01:12:48.39","Closed","Zookeeper stop fails if start called twice","Bug","2011-11-24 03:22:43.75",0,2,7925,7925
16074,"2011-05-11 03:32:58.807","This problem is seen only if you have ZooKeeper embedded in your application. QuorumPeerMain.initializeAndRun() does a quorumPeer.join() before exiting.QuorumPeer.shutdown() tries to cleanup everything, but it does not interrupt itself. As a result, a if the peer is running FLE, it might be waiting to receive notifications (recvqueue.poll()) in FastLeaderElection. Therefore, quorumPeer.join() will wait until the peer wakes up from poll().The fix is simple - call this.interrupt() in QuorumPeer.shutdown().","ZOOKEEPER-1060","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-06-14 20:14:48.27","Closed","QuorumPeer takes a long time to shutdown","Bug","2011-11-24 03:22:37.09",0,1,8126,8126
16075,"2011-05-04 18:50:42.099","*stat* command issues on non existing zookeeper node,causes NPE to the client.{noformat}[zk: localhost:2181(CONNECTED) 2] stat /invalidPathException in thread ""main"" java.lang.NullPointerException        at org.apache.zookeeper.ZooKeeperMain.printStat(ZooKeeperMain.java:131)        at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:723)        at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:582)        at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:354)        at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:312)        at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:271){noformat}","ZOOKEEPER-1059","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-05-17 01:39:15.599","Closed","stat command isses on non-existing node causes NPE ","Bug","2011-11-24 03:22:24.354",0,0,8180,8180
16076,"2011-05-04 08:34:45.179","fix Request getData to print that instead of getDate","ZOOKEEPER-1058","Trivial","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-05-21 05:42:53.013","Closed","fix typo in opToString for getData","Bug","2011-11-24 03:22:07.755",0,0,7832,7832
16079,"2011-04-27 05:08:47.328","actual result:[zk: (CONNECTED) 0] create /test2 'test2' digest:test:test:cdrwa,digest:test:test:cdrwaCreated /test2[zk: (CONNECTED) 1] getAcl /test2'digest,'test:test: cdrwa'digest,'test:test: cdrwa[zk: (CONNECTED) 2]but getAcl should only have a single entry.","ZOOKEEPER-1055","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-08-15 08:35:32.375","Closed","check for duplicate ACLs in addACL() and create()","Bug","2011-11-24 03:22:26.208",0,0,7842,7842
16082,"2011-04-24 21:45:34.376","{noformat}REC 	Exception is caught when Exception is not thrown in org.apache.zookeeper.server.quorum.QuorumPeer$ResponderThread.run(){noformat}","ZOOKEEPER-1052","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-05-04 01:58:31.782","Closed","Findbugs warning in QuorumPeer.ResponderThread.run()","Bug","2011-11-24 03:22:37.557",1,0,7750,7750
16083,"2011-04-21 21:56:15.309","In libzookeeper_mt, if your process is going rather slowly (such as when running it in Valgrind's Memcheck) or you are using gdb with breakpoints, you can occasionally get SIGPIPE when trying to send a message to the cluster. For example:==12788====12788== Process terminating with default action of signal 13 (SIGPIPE)==12788==    at 0x3F5180DE91: send (in /lib64/libpthread-2.5.so)==12788==    by 0x7F060AA: ??? (in /usr/lib64/libzookeeper_mt.so.2.0.0)==12788==    by 0x7F06E5B: zookeeper_process (in /usr/lib64/libzookeeper_mt.so.2.0.0)==12788==    by 0x7F0D38E: ??? (in /usr/lib64/libzookeeper_mt.so.2.0.0)==12788==    by 0x3F5180673C: start_thread (in /lib64/libpthread-2.5.so)==12788==    by 0x3F50CD3F6C: clone (in /lib64/libc-2.5.so)==12788==This is probably not the behavior we would like, since we handle server disconnections after a failed call to send. To fix this, there are a few options we could use. For BSD environments, we can tell a socket to never send SIGPIPE with send using setsockopt:setsockopt(sd, SOL_SOCKET, SO_NOSIGPIPE, (void *)&set, sizeof(int));For Linux environments, we can add a MSG_NOSIGNAL flag to every send call that says to not send SIGPIPE on a bad file descriptor.For more information, see: http://stackoverflow.com/questions/108183/how-to-prevent-sigpipes-or-handle-them-properly","ZOOKEEPER-1051","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-08-30 15:02:07.344","Closed","SIGPIPE in Zookeeper 0.3.* when send'ing after cluster disconnection","Bug","2011-11-24 03:21:59.156",1,2,7881,7881
16085,"2011-04-16 11:42:55.931","Let's say we have 100 clients (group A) already connected to three-node ZK ensemble with session timeout of 15 second.  And we have 1000 clients (group B) already connected to the same ZK ensemble, all watching several nodes (with 15 second session timeout)Consider a case in which All clients in group B suddenly hung or deadlocked (JVM OOME) all at the same time. 15 seconds later, all sessions in group B gets expired, creating session closing stampede. Depending on the number of this clients in group B, all request/response ZK ensemble should process get delayed up to 8 seconds (1000 clients we have tested).This delay causes some clients in group A their sessions expired due to delay in getting heartbeat response. This causes normal servers to drop out of clusters. This is a serious problem in our installation, since some of our services running batch servers or CI servers creating the same scenario as above almost everyday.I am attaching a graph showing ping response time delay.I think ordering of creating/closing sessions and ping exchange isn't important (quorum state machine). at least ping request / response should be handle independently (different queue and different thread) to keep realtime-ness of ping.As a workaround, we are raising session timeout to 50 seconds.But this causes max. failover of cluster to significantly increased, thus initial QoS we promised cannot be met.","ZOOKEEPER-1049","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-05-04 05:30:42.894","Closed","Session expire/close flooding renders heartbeats to delay significantly","Bug","2011-11-24 03:22:20.716",0,4,8184,8184
16088,"2011-04-13 06:24:19.329","On several occasions, I've seen a create() with the sequential flag set fail with a ZNODEEXISTS error, and I don't think that should ever be possible.  In past runs, I've been able to closely inspect the state of the system with the command line client, and saw that the parent znode's cversion is smaller than the sequential number of existing children znode under that parent.  In one example:{noformat}[zk:<ip:port>(CONNECTED) 3] stat /zkrsmcZxid = 0x5ctime = Mon Jan 17 18:28:19 PST 2011mZxid = 0x5mtime = Mon Jan 17 18:28:19 PST 2011pZxid = 0x1d819cversion = 120710dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 2955{noformat}However, the znode /zkrsm/000000000000002d_record0000120804 existed on disk.In a recent run, I was able to capture the Zookeeper logs, and I will attach them to this JIRA.  The logs are named as nodeX.<zxid_prefixes>.log, and each new log represents an application process restart.Here's the scenario:# There's a cluster with nodes 1,2,3 using zxid 0x3.# All three nodes restart, forming a cluster of zxid 0x4.# Node 3 restarts, leading to a cluster of 0x5.At this point, it seems like node 1 is the leader of the 0x5 epoch.  In its log (node1.0x4-0x5.log) you can see the first (of many) instances of the following message:{noformat}2011-04-11 21:16:12,607 16649 [ProcessThread:-1] INFO org.apache.zookeeper.server.PrepRequestProcessor  - Got user-level KeeperException when processing sessionid:0x512f466bd44e0002 type:create cxid:0x4da376ab zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a Error Path:/zkrsm/00000000000000b2_record0001761440 Error:KeeperErrorCode = NodeExists for /zkrsm/00000000000000b2_record0001761440{noformat}This then repeats forever as my application isn't expecting to ever get this error message on a sequential node create, and just continually retries.  The message even transfers over to node3.0x5-0x6.log once the 0x6 epoch comes into play.I don't see anything terribly fishy in the transition between the epochs; the correct snapshots seem to be getting transferred, etc.  Unfortunately I don't have a ZK snapshot/log that exhibits the problem when starting with a fresh system.Some oddities you might notice in these logs:* Between epochs 0x3 and 0x4, the zookeeper IDs of the nodes changed due to a bug in our application code.  (They are assigned randomly, but are supposed to be consistent across restarts.)* We manage node membership dynamically, and our application restarts the ZooKeeperServer classes whenever a new node wants to join (without restarting the entire application process).  This is why you'll see messages like the following in node1.0x4-0x5.log before a new election begins:{noformat}2011-04-11 21:16:00,762 4804 [QuorumPeer:/0.0.0.0:2888] INFO org.apache.zookeeper.server.quorum.Learner  - shutdown called{noformat}* There is in fact one of these dynamic membership changes in node1.0x4-0x5.log, just before the 0x4 epoch is formed.  I'm not sure how this would be related though, as no transactions are done during this period.","ZOOKEEPER-1046","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-14 22:24:41.319","Closed","Creating a new sequential node results in a ZNODEEXISTS error","Bug","2011-11-24 03:22:03.075",2,2,8126,8053
16100,"2011-03-28 04:45:50.69","Installing Net::ZooKeeper from cpan or the zookeeper distribution tarballs will always fail due to not finding c-client header files.  In conjunction with ZOOKEEPER-1033 update perl bindings to look for c-client header files in INCDIR/zookeeper/a.k.a. make installs of Net::ZooKeeper via cpan/cpanm/whatever *just work*, assuming you've already got the zookeeper c client installed.","ZOOKEEPER-1034","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-08-15 09:41:13.077","Closed","perl bindings should automatically find the zookeeper c-client headers","Bug","2011-11-24 03:22:05.956",0,1,8133,8133
16101,"2011-03-28 04:40:09.658","header files are installed into foo/include/c-client-src/, which doesn't indicate a relationship with zookeeper and doesn't correspond to foo/lib/libzookeeper*header files should be installed into foo/include/zookeeper/ as this is the common practice.","ZOOKEEPER-1033","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-05-04 14:03:00.75","Closed","c client should install includes into INCDIR/zookeeper, not INCDIR/c-client-src","Bug","2011-11-24 03:22:14.117",0,1,8133,8133
16106,"2011-03-25 04:26:04.374","There is a small bug in the python bindings, specifically with the zookeeper.set2() call. This method should return a stat dictionary, but actually returns None. The fix is a one-character change to zookeeper.c such that the return value is '&stat' rather than 'stat'.","ZOOKEEPER-1028","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-04-07 04:22:12.491","Closed","In python bindings, zookeeper.set2() should return a stat dict but instead returns None","Bug","2011-11-24 03:22:23.304",0,1,8191,8191
16107,"2011-03-24 13:06:47.054","I've recently started to use the chroot functionality (introduced in3.2.0) as part of my connect string.It mostly works as expected, butthere is one case that is unexpected: when I create a path withzoo_create() I can retrieve the created path. This is very useful whenyou set the ZOO_SEQUENCE flag. Unfortunately the returned pathincludes the chroot as part of the path. This was unexpected to me: Iexpected that the chroot would be totally transparent. Thedocumentation for zoo_create() says:""path_buffer : Buffer which will be filled with the path of the newnode (this might be different than the supplied path because of theZOO_SEQUENCE flag).""This gave me the impression that this flag is the only reason thereturned path is different from the created path, but apparently it'snot. Is this a bug or intended behavior? I workaround this issue now by remembering the chroot inmy wrapper code and after a call to zoo_create() i check if the returnedpath starts with the chroot. If it does, I remove it.My use case is to create a path with a sequence number and then deletethis path later. Unfortunately I cannot delete the path because it hasthe chroot prepended to it, and thus it will result in two chroots.I believe this only affects the create functions.","ZOOKEEPER-1027","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-26 01:45:31.181","Closed","chroot not transparent in zoo_create()","Bug","2011-11-24 03:22:13.762",0,4,8192,8192
16119,"2011-03-12 04:28:02.671","currently the ""Usage"" message for zkServer shows:  echo ""Usage: $0 {start|stop|restart|status}"" But it seems to me that it should show the other startup options as well, which are currently: start-foreground, upgrade, print-cmd.","ZOOKEEPER-1013","Trivial","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-03-16 02:39:07.8","Closed","zkServer.sh usage message should mention all startup options","Bug","2011-11-24 03:22:16.834",0,0,7842,7842
16124,"2011-03-05 05:42:58.77","On line 1957, zookeeper_process() returns without cleaning up the ""ia"" buffer that was previously allocated.  I don't know how often this code path is taken, but I thought it was worth reporting.  I will attach a simple patch shortly.","ZOOKEEPER-1007","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-03-16 04:42:44.226","Closed","iarchive leak in C client","Bug","2011-11-24 03:22:47.054",0,0,8053,8053
16125,"2011-03-04 01:38:18.493","CnxManagerTest.testWorkerThreads See attachment, this is the first time I've seen this test fail, and it's failed 2 out of the last three test runs.Notice (attachment) once this happens the port never becomes available.{noformat}2011-03-02 15:53:12,425 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11245:NIOServerCnxn$Factory@251] - Accepted socket connection from /172.29.6.162:514412011-03-02 15:53:12,430 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11245:NIOServerCnxn@639] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running2011-03-02 15:53:12,430 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11245:NIOServerCnxn@1435] - Closed socket connection for client /172.29.6.162:51441 (no session established for client)2011-03-02 15:53:12,430 - WARN  [QuorumPeer:/0:0:0:0:0:0:0:0:11241:Follower@82] - Exception when following the leaderjava.io.EOFException	at java.io.DataInputStream.readInt(DataInputStream.java:375)	at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)	at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:84)	at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)	at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:148)	at org.apache.zookeeper.server.quorum.Learner.registerWithLeader(Learner.java:267)	at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:66)	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:645)2011-03-02 15:53:12,431 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11241:Follower@165] - shutdown calledjava.lang.Exception: shutdown Follower	at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:165)	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:649)2011-03-02 15:53:12,432 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11241:QuorumPeer@621] - LOOKING2011-03-02 15:53:12,432 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11241:FastLeaderElection@663] - New election. My id =  0, Proposed zxid = 02011-03-02 15:53:12,433 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 0 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 0 (n.sid), LOOKING (my state)2011-03-02 15:53:12,433 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 0 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 0 (n.sid), LOOKING (my state)2011-03-02 15:53:12,433 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 0 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 0 (n.sid), LOOKING (my state)2011-03-02 15:53:12,633 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 0 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 0 (n.sid), LOOKING (my state)2011-03-02 15:53:12,633 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11245:QuorumPeer@655] - LEADING2011-03-02 15:53:12,636 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11245:Leader@54] - TCP NoDelay set to: true2011-03-02 15:53:12,638 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11245:ZooKeeperServer@151] - Created server with tickTime 1000 minSessionTimeout 2000 maxSessionTimeout 20000 datadir /var/lib/hudson/workspace/CDH3-ZooKeeper-3.3.3_sles/build/test/tmp/test9001250572426375869.junit.dir/version-2 snapdir /var/lib/hudson/workspace/CDH3-ZooKeeper-3.3.3_sles/build/test/tmp/test9001250572426375869.junit.dir/version-22011-03-02 15:53:12,639 - ERROR [QuorumPeer:/0:0:0:0:0:0:0:0:11245:Leader@133] - Couldn't bind to port 11245java.net.BindException: Address already in use	at java.net.PlainSocketImpl.socketBind(Native Method)	at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:365)	at java.net.ServerSocket.bind(ServerSocket.java:319)	at java.net.ServerSocket.<init>(ServerSocket.java:185)	at java.net.ServerSocket.<init>(ServerSocket.java:97)	at org.apache.zookeeper.server.quorum.Leader.<init>(Leader.java:131)	at org.apache.zookeeper.server.quorum.QuorumPeer.makeLeader(QuorumPeer.java:512)	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:657){noformat}","ZOOKEEPER-1006","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-28 01:21:18.407","Closed","QuorumPeer ""Address already in use"" -- regression in 3.3.3","Bug","2011-11-24 03:22:20.927",0,0,7818,7818
16135,"2011-02-18 03:38:49.082","The ""eclipse"" target in the zoo-keeper build script doesn't include the accessive.jar present in the folder /src/java/libtest in the .classpath file. But the accessive.jar is being referenced from a couple of test classes.However, the build is successful :)","ZOOKEEPER-994","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-02-27 15:11:39.958","Closed","""eclipse"" target in the build script doesnot include libraray required for test classes in the classpath","Bug","2011-11-24 03:22:23.474",0,0,8194,8194
16144,"2011-02-10 03:24:51.381","The unit test fails on trunk on my mac. I think this might be the same on other platforms as well. Ill attach the error logs.","ZOOKEEPER-985","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-02-19 01:55:02.35","Closed","Test BookieRecoveryTest fails on trunk.","Bug","2011-11-24 03:22:26.843",0,0,7750,7827
16146,"2011-02-03 13:21:01.053","If zkServer.sh is run remotely using ssh as follows ssh will ""hang"" - i.e. not complete/return once the server is started. This is even though zkServer.sh starts the java vm in the background.$ ssh <host> ""zkServer.sh start""this is due to the following issue:http://www.slac.stanford.edu/comp/unix/ssh_faq.html#logoff_hangs","ZOOKEEPER-983","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-02-27 14:57:03.05","Closed","running zkServer.sh start remotely using ssh hangs","Bug","2011-11-24 03:22:41.337",0,1,7818,7818
16148,"2011-02-02 04:23:01.578","I saw a hang once when my C++ application called the zookeeper_close() method of the multi-threaded Zookeeper client library.  The stack trace of the hung thread was the following:{quote}Thread 8 (Thread 5644):#0  0x00007f5d7bb5bbe4 in __lll_lock_wait () from /lib/libpthread.so.0#1  0x00007f5d7bb59ad0 in pthread_cond_broadcast@@GLIBC_2.3.2 () from /lib/libpthread.so.0#2  0x00007f5d793628f6 in unlock_completion_list (l=0x32b4d68) at .../zookeeper/src/c/src/mt_adaptor.c:66#3  0x00007f5d79354d4b in free_completions (zh=0x32b4c80, callCompletion=1, reason=-116) at .../zookeeper/src/c/src/zookeeper.c:1069#4  0x00007f5d79355008 in cleanup_bufs (zh=0x32b4c80, callCompletion=1, rc=-116) at .../thirdparty/zookeeper/src/c/src/zookeeper.c:1125#5  0x00007f5d79353200 in destroy (zh=0x32b4c80) at .../thirdparty/zookeeper/src/c/src/zookeeper.c:366#6  0x00007f5d79358e0e in zookeeper_close (zh=0x32b4c80) at .../zookeeper/src/c/src/zookeeper.c:2326#7  0x00007f5d79356d18 in api_epilog (zh=0x32b4c80, rc=0) at .../zookeeper/src/c/src/zookeeper.c:1661#8  0x00007f5d79362f2f in adaptor_finish (zh=0x32b4c80) at .../zookeeper/src/c/src/mt_adaptor.c:205#9  0x00007f5d79358c8c in zookeeper_close (zh=0x32b4c80) at .../zookeeper/src/c/src/zookeeper.c:2297 ...{quote}The omitted part of the stack trace is entirely within my application, and contains no other calls to/from the Zookeeper client.  In particular, I am not calling zookeeper_close() from within a completion handler or any of the library's threads.I haven't been able to reproduce this, and when I encountered this I wasn't capturing logging from the client library, so unfortunately I don't have any more information at this time.  But I will update this JIRA if I see it again.","ZOOKEEPER-981","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-14 12:10:37.237","Closed","Hang in zookeeper_close() in the multi-threaded C client","Bug","2011-11-24 03:22:00.382",1,6,8053,8053
16153,"2011-01-18 09:36:16.625","From bug filed on CDH: https://issues.cloudera.org/browse/DISTRO-47 - moving it to this jira to address:------------------------------------------------------Bug filed by ""grep.alex"" at http://getsatisfaction.com/cloudera/topics/cdh3b3_zookeeper_startup_script_doesnt_use_java_homeOn RedHat 5 (using the RPM installer) I was able to install and run all the Hadoop components. The Zookeeper install was fine, but it wouldn't start:{noformat}[root@aholmes-desktop init.d]# ./hadoop-zookeeper start JMX enabled by default Using config: /etc/zookeeper/zoo.cfg Starting zookeeper ... STARTED [root@aholmes-desktop init.d]# Exception in thread ""main"" java.lang.NoSuchMethodError: method java.lang.management.ManagementFactory.getPlatformMBeanServer with signature ()Ljavax.management.MBeanServer; was not found. at org.apache.zookeeper.jmx.ManagedUtil.registerLog4jMBeans(ManagedUtil.java:48 ...{noformat} After some digging around I found the cause - the Zookeeper startup script (/usr/lib/zookeeper/bin/zkServer.sh ) uses the java found in the path, whereas the other startup scripts use JAVA_HOME. In my case I had the default RHEL5 1.4 JDK in the path, and the 1.6 JDK RPM's installed under /usr/java, hence the above error, which I'm guessing is a fairly common setup.In my opinion all the startup scripts should all use the same mechanism to determine where to pick java.","ZOOKEEPER-976","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-02-27 15:02:15.99","Closed","ZooKeeper startup script doesn't use JAVA_HOME","Bug","2011-11-24 03:22:31.321",0,1,7818,7818
16166,"2010-12-23 16:34:39.437","It's possible to make Forrest work with JDK6 by disabling sitemap validationin the forrest.properties file. See FOR-984 and PIG-1508 for more details.","ZOOKEEPER-963","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-12-29 09:08:57.46","Closed","Make Forrest work with JDK6","Bug","2011-11-24 03:22:28.089",0,0,8130,8130
16154,"2011-01-14 20:29:33.071","Scenario:1. 2 of the 3 ZK nodes are online2. Third node is attempting to join3. Third node unnecessarily goes in ""LEADING"" state4. Then third goes back to LOOKING (no majority of followers) and finally goes to FOLLOWING state.While going through the logs I noticed that a peer C that is trying tojoin an already formed cluster goes in LEADING state. This is becauseQuorumCnxManager of A and B sends the entire history of notificationmessages to C. C receives the notification messages that wereexchanged between A and B when they were forming the cluster.In FastLeaderElection.lookForLeader(), due to the following piece ofcode, C quits lookForLeader assuming that it is supposed to lead.740                             //If have received from all nodes, then terminate741                             if ((self.getVotingView().size() == recvset.size()) &&742                                     (self.getQuorumVerifier().getWeight(proposedLeader) != 0)){743                                 self.setPeerState((proposedLeader == self.getId()) ?744                                         ServerState.LEADING: learningState());745                                 leaveInstance();746                                 return new Vote(proposedLeader, proposedZxid);747748                             } else if (termPredicate(recvset,This can cause:1.  C to unnecessarily go in LEADING state and wait for tickTime * initLimit and then restart the FLE.2. C waits for 200 ms (finalizeWait) and then considers whatevernotifications it has received to make a decision. C could potentiallydecide to follow an old leader, fail to connect to the leader, andthen restart FLE. See code below.752                             if (termPredicate(recvset,753                                     new Vote(proposedLeader, proposedZxid,754                                             logicalclock))) {755 756                                 // Verify if there is any change in the proposed leader757                                 while((n = recvqueue.poll(finalizeWait,758                                         TimeUnit.MILLISECONDS)) != null){759                                     if(totalOrderPredicate(n.leader, n.zxid,760                                             proposedLeader, proposedZxid)){761                                         recvqueue.put(n);762                                         break;763                                     }764                                 }In general, this does not affect correctness of FLE since C willeventually go back to FOLLOWING state (A and B won't vote forC). However, this delays C from joining the cluster. This can in turnaffect recovery time of an application.Proposal: A and B should send only the latest notification (mostrecent) instead of the entire history. Does this sound reasonable?","ZOOKEEPER-975","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-04-30 00:13:46.32","Closed","new peer goes in LEADING state even if ensemble is online","Bug","2011-11-24 03:22:03.282",0,3,8126,8126
16167,"2010-12-22 02:42:59.384","From mailing list:It seems like we rely on the LearnerHandler thread startup to capture all of the missing committedtransactions in the SNAP or DIFF, but I don't see anything (especially in the DIFF case) thatis preventing us for committing more transactions before we actually start forwarding updatesto the new follower.Let me explain using my example from ZOOKEEPER-919. Assume we have quorum already, so theleader can be processing transactions while my follower is starting up.I'm a follower at zxid N-5, the leader is at N. I send my FOLLOWERINFO packet to the leaderwith that information. The leader gets the proposals from its committed log (time T1), thensyncs on the proposal list (LearnerHandler line 267. Why? It's a copy of the underlying proposallist... this might be part of our problem). I check to see if the peerLastZxid is within mymax and min committed log and it is, so I'm going to send a diff. I set the zxidToSend tobe the maxCommittedLog at time T3 (we already know this is sketchy), and forward the proposalsfrom my copied proposal list starting at the peerLastZxid+1 up to the last proposal transaction(as seen at time T1).After I have queued up all those diffs to send, I tell the leader to startFowarding updatesto this follower (line 308). So, let's say that at time T2 I actually swap out the leader to the thread that is handlingthe various request processors, and see that I got enough votes to commit zxid N+1. I commitN+1 and so my maxCommittedLog at T3 is N+1, but this proposal is not in the list of proposalsthat I got back at time T1, so I don't forward this diff to the client. Additionally, I processedthe commit and removed it from my leader's toBeApplied list. So when I call startForwardingfor this new follower, I don't see this transaction as a transaction to be forwarded. There's one problem. Let's also imagine, however, that I commit N+1 at time T4. The maxCommittedLogvalue is consistent with the max of the diff packets I am going to send the follower. But,I still committed N+1 and removed it from the toBeApplied list before calling startFowardingwith this follower. How does the follower get this transaction? Does it?To put it another way, here is the thread interaction, hopefully formatted so you can readit...		LearnerHandlerThread					RequestProcessorThreadT1(LH):	get list of proposals (COPY)T2(RPT):								commit N+1, remove from toBeAppliedT3(LH):	get maxCommittedLogT4(LH):	send diffs from view at T1T5(LH):	startForwardingOrT1(LH):	get list of proposals (COPY)T2(LH):	get maxCommittedLogT3(RPT):								commit N+1, remove from toBeAppliedT4(LH):	send diffs from view at T1T5(LH):	startFowardingI'm trying to figure out what, if anything, keeps the requests from being committed, removed,and never seen by the follower before it fully starts up. ","ZOOKEEPER-962","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-01-23 13:31:02.347","Closed","leader/follower coherence issue when follower is receiving a DIFF","Bug","2011-11-24 03:22:24.763",0,2,8153,7832
16168,"2010-12-22 00:07:16.368","Let's say you're using connection string ""127.0.0.1:2182/foo"".1) put a childrenchanged watch on relative / (that is, on absolute path /foo)2) stop the zk server3) start the zk server4) at this point, the client recovers the connection, and should have put back a watch on relative path /, but instead the client puts a watch on the *absolute* path /- if some other client adds or removes a node under /foo, nothing will happen- if some other client adds or removes a node under /, then you will get an error from the zk client library (string operation error)","ZOOKEEPER-961","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-14 13:51:47.824","Closed","Watch recovery after disconnection when connection string contains a prefix","Bug","2011-11-24 03:22:02.12",0,2,8115,8215
16169,"2010-12-15 17:18:22.22","Currently the hedwig cpp client will automatically send a consume message to the server when the calling client indicated that it has received the message. If the client wants to queue the messages and not acknowledge them to the server immediately, they need to block, which means interfering with any other running callbacks. ","ZOOKEEPER-958","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-12-22 03:34:07.375","Closed","Flag to turn off autoconsume in hedwig c++ client","Bug","2011-11-24 03:22:42.19",0,0,7956,7956
16170,"2010-12-14 01:09:05.998","Somebody left some echo statements in the zkCleanup.sh which prevents the java commands from actually running.Patch coming forthwith.","ZOOKEEPER-957","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-12-15 11:17:40.281","Closed","zkCleanup.sh doesn't do anything","Bug","2011-11-24 03:21:58.141",0,0,7925,7925
16204,"2010-11-08 16:51:59.599","Calling {{zookeeper.create()}} seems, under certain circumstances, to be corrupting a subsequent call to Python's {{logging}} module.Specifically, if the node does not exist (but its parent does), I end up with a traceback like this when I try to make the logging call:{noformat}Traceback (most recent call last):  File ""zktest.py"", line 21, in <module>    logger.error(""Boom?"")  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/logging/__init__.py"", line 1046, in error    if self.isEnabledFor(ERROR):  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/logging/__init__.py"", line 1206, in isEnabledFor    return level >= self.getEffectiveLevel()  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/logging/__init__.py"", line 1194, in getEffectiveLevel    while logger:TypeError: an integer is required{noformat}But if the node already exists, or the parent does not exist, I get the appropriate NodeExists or NoNode exceptions.I'll be attaching a test script that can be used to reproduce this behavior.","ZOOKEEPER-921","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-12-29 08:46:01.711","Closed","zkPython incorrectly checks for existence of required ACL elements","Bug","2011-11-24 03:22:03.842",0,1,8223,8223
16206,"2010-11-04 21:43:21.307","I was testing stability of Zookeeper ensemble for production deployment. Three node ensemble cluster configuration.In a loop, I kill/restart three Zookeeper clients that created one ephemeral node each, and at the same time,I killed Java process on one of ensemble (dont' know if it was a leader or not). Then I restarted Zookeeper on the server,It turns out that on two zookeeper ensemble servers, all the ephemeral nodes are gone (it should), but on the newly startedZookeeper server, the two old ephemeral nodes stayed.  The zookeeper didn't restart in standalone mode since new ephemeralnodes gets created on all ensemble servers. I captured the log.2010-11-04 17:48:50,201 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17288:NIOServerCnxn$Factory@250] - Accepted socket connection from /10.25.131.21:111912010-11-04 17:48:50,202 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17288:NIOServerCnxn@776] - Client attempting to establish new session at /10.25.131.21:111912010-11-04 17:48:50,203 - INFO  [CommitProcessor:1:NIOServerCnxn@1579] - Established session 0x12c160c31fc000b with negotiated timeout 30000 for client /10.25.131.21:111912010-11-04 17:48:50,206 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17288:NIOServerCnxn@633] - EndOfStreamException: Unable to read additional data from client sessionid 0x12c160c31fc000b, likely client has closed socket2010-11-04 17:48:50,207 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17288:NIOServerCnxn@1434] - Closed socket connection for client /10.25.131.21:11191 which had sessionid 0x12c160c31fc000b2010-11-04 17:48:50,207 - ERROR [CommitProcessor:1:NIOServerCnxn@444] - Unexpected Exception:java.nio.channels.CancelledKeyException        at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)        at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:59)        at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:417)        at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1508)        at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:367)        at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:73)","ZOOKEEPER-919","Blocker","ZOOKEEPER","ZooKeeper","ASF","Duplicate","2011-11-19 09:11:55.277","Closed","Ephemeral nodes remains in one of ensemble after deliberate SIGKILL","Bug","2011-11-24 03:22:22.523",0,2,NULL,8184
16240,"2010-10-01 17:11:25.955","We no longer use LedgerSequence, so we need to remove references in documentation and comments sprinkled throughout the code.","ZOOKEEPER-884","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-11-05 13:18:45.514","Closed","Remove LedgerSequence references from BookKeeper documentation and comments in tests ","Bug","2011-11-24 03:22:02.506",0,0,7750,7750
16211,"2010-10-26 14:50:16.166","Cannot build 3.3.1 from release tarball do to VerGen parser inability to parse ""3.3.2-dev"".version-info:     [java] All version-related parameters must be valid integers!     [java] Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""2-dev""     [java] 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)     [java] 	at java.lang.Integer.parseInt(Integer.java:481)     [java] 	at java.lang.Integer.parseInt(Integer.java:514)     [java] 	at org.apache.zookeeper.version.util.VerGen.main(VerGen.java:131)     [java] Java Result: 1","ZOOKEEPER-913","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-01-27 15:45:24.891","Closed","Version parser fails to parse ""3.3.2-dev"" from build.xml.","Bug","2011-11-24 03:22:17.761",0,1,7818,8224
16217,"2010-10-21 02:27:19.11","The sync request does not set the session owner in Request.As a result, the leader keeps printing:2010-07-01 10:55:36,733 - INFO  [ProcessThread:-1:PrepRequestProcessor@405] - Got user-level KeeperException when processing sessionid:0x298d3b1fa90000 type:sync: cxid:0x6 zxid:0xfffffffffffffffe txntype:unknown reqpath:/ Error Path:null Error:KeeperErrorCode = Session moved","ZOOKEEPER-907","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-11-05 00:29:28.353","Closed","Spurious ""KeeperErrorCode = Session moved"" messages","Bug","2011-11-24 03:22:29.006",0,2,8126,8126
16220,"2010-10-20 04:44:01.011","The documentation states:New in 3.2:  Enables a ZooKeeper ensemble administrator to access the znode hierarchy as a ""super"" user. In particular no ACL checking occurs for a user authenticated as super.However, if a super user does something like:zk.setACL(""/"", Ids.READ_ACL_UNSAFE, -1);the super user is now bound by read-only ACL. This is not what I would expect to see given the documentation. It can be fixed by moving the chec for the ""super"" authId in PrepRequestProcessor.checkACL to before the for(ACL a : acl) loop.","ZOOKEEPER-904","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-10-27 06:31:11.447","Closed","super digest is not actually acting as a full superuser","Bug","2011-11-24 03:22:24.586",0,0,7832,7832
16222,"2010-10-19 01:41:49.757","https://hudson.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/970/artifact/trunk/findbugs/zookeeper-findbugs-report.html#Warnings_MALICIOUS_CODEMalicious code vulnerability WarningsCode	WarningMS	org.apache.zookeeper.server.quorum.LeaderElection.epochGen isn't final but should be","ZOOKEEPER-902","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-02-08 03:27:45.011","Closed","Fix findbug issue in trunk ""Malicious code vulnerability""","Bug","2011-11-24 03:21:58.863",0,1,7750,7818
16257,"2010-09-07 15:29:12.811","client cleanup test is failing on hudson. fd count is off.","ZOOKEEPER-867","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-09-15 07:22:17.819","Closed","ClientTest is failing on hudson - fd cleanup","Bug","2011-11-24 03:22:01.25",0,0,7818,7818
16226,"2010-10-15 03:35:57.62","I was looking through the c-client code and noticed a situation where a counter can be incorrectly incremented and a small memory leak can occur.In zookeeper.c : add_completion(), if close_requested is true, then the completion will not be queued.  But at the end, outstanding_sync is still incremented and free() never called on the newly allocated completion_list_t.  I will submit for review a diff that I believe corrects this issue.","ZOOKEEPER-898","Trivial","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-10-29 02:51:35.113","Closed","C Client might not cleanup correctly during close","Bug","2011-11-24 03:22:20.045",0,0,8164,8164
16227,"2010-10-15 03:26:06.675","We observed a crash while closing our c client.  It was in the do_io() thread that was processing as during the close() call.#0  queue_buffer (list=0x6bd4f8, b=0x0, add_to_front=0) at src/zookeeper.c:969#1  0x000000000046234e in check_events (zh=0x6bd480, events=<value optimized out>) at src/zookeeper.c:1687#2  0x0000000000462d74 in zookeeper_process (zh=0x6bd480, events=2) at src/zookeeper.c:1971#3  0x0000000000469c34 in do_io (v=0x6bd480) at src/mt_adaptor.c:311#4  0x00007ffff7bc59ca in start_thread () from /lib/libpthread.so.0#5  0x00007ffff6f706fd in clone () from /lib/libc.so.6#6  0x0000000000000000 in ?? ()We tracked down the sequence of events, and the cause is that input_buffer is being freed from a thread other than the do_io thread that relies on it:1. do_io() call check_events()2. if(events&ZOOKEEPER_READ) branch executes3. if (rc > 0) branch executes4. if (zh->input_buffer != &zh->primer_buffer) branch executes.....in the meantime......     5. zookeeper_close() called     6. if (inc_ref_counter(zh,0)!=0) branch executes     7. cleanup_bufs() is called     8. input_buffer is freed at the end..... back to check_events().........9. queue_events() is called on a NULL buffer.I believe the patch is to only call free_completions() in zookeeper_close() and not cleanup_bufs().  The original reason cleanup_bufs() was added was to call any outstanding synhcronous completions, so only free_completions (which is guarded) is needed.  I will submit a patch for review with this change.","ZOOKEEPER-897","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-10-29 00:25:53.348","Closed","C Client seg faults during close","Bug","2011-11-24 03:22:21.328",0,0,8164,8164
16231,"2010-10-11 15:15:55.382","When ZooKeeper receives certain illegally formed messages on the internal communication port (:4181 by default), it's possible for ZooKeeper to enter an infinite loop which causes 100% cpu usage. It's related to ZOOKEEPER-427, but that patch does not resolve all issues.from: src/java/main/org/apache/zookeeper/server/quorum/QuorumCnxManager.java the two affected parts:===========int length = msgLength.getInt();                                                        if(length <= 0) {                                                                           throw new IOException(""Invalid packet length:"" + length);                           } ======================while (message.hasRemaining()) {                                                        temp_numbytes = channel.read(message);                                              if(temp_numbytes < 0) {                                                                 throw new IOException(""Channel eof before end"");                                }                                                                                   numbytes += temp_numbytes;                                                      } ===========how to replicate this bug:perform an nmap portscan against your zookeeper server: ""nmap -sV -n your.ip.here -p4181""wait for a while untill you see some messages in the logfile and then you will see 100% cpu usage. It does not recover from this situation. With my patch, it does not occur anymore","ZOOKEEPER-893","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-10-20 06:38:09.92","Closed","ZooKeeper high cpu usage when invalid requests","Bug","2011-11-24 03:22:29.72",0,2,8192,8192
16236,"2010-10-06 21:26:52.88","the c-client / zkpython wrapper invokes already freed watcher callbacksteps to reproduce:  0. start a zookeper server on your machine  1. run the attached python script  2. suspend the zookeeper server process (e.g. using `pkill -STOP -f org.apache.zookeeper.server.quorum.QuorumPeerMain` )  3. wait until the connection and the node observer fired with a session event  4. resume the zookeeper server process  (e.g. using `pkill -CONT -f org.apache.zookeeper.server.quorum.QuorumPeerMain` )-> the client tries to dispatch the node observer function again, but it was already freed -> double free corruption","ZOOKEEPER-888","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-10-20 03:02:22.019","Closed","c-client / zkpython: Double free corruption on node watcher","Bug","2011-11-24 03:22:44.576",1,2,8193,8193
16242,"2010-09-29 07:46:36.08","On startup, the server first loads the latest snapshot, and then loads from the log starting at the last transaction in the snapshot.  It should begin from one past that last transaction in the log.  I will attach a possible patch.","ZOOKEEPER-882","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-12-23 20:43:17.004","Closed","Startup loads last transaction from snapshot","Bug","2011-11-24 03:22:29.552",0,0,8164,8164
16243,"2010-09-29 07:41:03.456","zkDb.loadDataBase() is called twice at the beginning of loadData().  It shouldn't have any negative affects, but is unnecessary.   A patch should be trivial.","ZOOKEEPER-881","Trivial","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-10-19 02:30:02.713","Closed","ZooKeeperServer.loadData loads database twice","Bug","2011-11-24 03:21:59.531",0,0,8164,8164
16244,"2010-09-28 07:40:46.374","We're seeing an issue where one server in the ensemble has a steady growing number of QuorumCnxManager$SendWorker threads up to a point where the OS runs out of native threads, and at the same time we see a lot of exceptions in the logs.  This is on 3.2.2 and our config looks like:{noformat}tickTime=3000dataDir=/somewhere_thats_not_tmpclientPort=2181initLimit=10syncLimit=5server.0=sv4borg9:2888:3888server.1=sv4borg10:2888:3888server.2=sv4borg11:2888:3888server.3=sv4borg12:2888:3888server.4=sv4borg13:2888:3888{noformat}The issue is on the first server. I'm going to attach threads dumps and logs in moment.","ZOOKEEPER-880","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-03-17 02:49:33.019","Closed","QuorumCnxManager$SendWorker grows without bounds","Bug","2011-11-24 03:22:01.703",0,3,8126,8232
16250,"2010-09-18 00:01:03.667","FileTxnSnapLog.restore() does not call listener passed as parameter. The result is that the commitLogs list is empty. When a follower connects to the leader, the leader is forced to send a snapshot to the follower instead of a couple of requests and commits.","ZOOKEEPER-874","Trivial","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-04-14 00:10:24.84","Closed","FileTxnSnapLog.restore does not call listener","Bug","2013-05-02 10:29:33.975",0,1,8236,8236
16254,"2010-09-15 06:16:29.707","the zookeeper current trunk build is broken mostly due to some netty changes. This is causing a huge backlog of PA's and other impediments to the review process. For now I plan to disable the test and fix them as part of 3.4 later.","ZOOKEEPER-870","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-09-15 13:57:29.288","Closed","Zookeeper trunk build broken.","Bug","2011-11-24 03:22:32.123",0,0,7827,7827
16263,"2010-09-03 01:40:49.657","The Hedwig code checked into Apache is missing a test SSL certificate file used for running the server junit tests.  We need this file otherwise the tests that use this (e.g. TestHedwigHub) will fail.","ZOOKEEPER-861","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-09-08 02:29:25.246","Closed","Missing the test SSL certificate used for running junit tests.","Bug","2011-11-24 03:22:37.764",0,1,8216,8216
16307,"2010-07-14 14:59:31.921","Andrei, I just realized that src/contrib/monitoring files are missing apache license headers.  Please add them (in particular any script files like python, see similar files in svn for examples - in some cases like README it's not strictly necessary.) You can run the RAT tool to verify (see build.xml or http://incubator.apache.org/rat/)","ZOOKEEPER-814","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-07-27 06:01:02.332","Closed","monitoring scripts are missing apache license headers","Bug","2011-11-24 03:22:13.555",0,0,8091,7818
16269,"2010-08-26 22:49:18.956","The server documentation states that the configuration parameter for binding to a specific ip address is clientPortBindAddress.  The code believes the parameter is clientPortAddress.  The documentation for 3.3.X versions needs changed to reflect the correct parameter .  This parameter was added in ZOOKEEPER-635.","ZOOKEEPER-855","Trivial","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-10-19 05:56:47.213","Closed","clientPortBindAddress should be clientPortAddress","Bug","2011-11-24 03:22:48.175",0,0,8164,8164
16270,"2010-08-20 04:04:45.535","BookKeeper does not compile due to changes in the NIOServerCnxn class of ZooKeeper.","ZOOKEEPER-854","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-08-29 00:11:43.676","Closed","BookKeeper does not compile due to changes in the ZooKeeper code","Bug","2011-11-24 03:22:03.664",0,0,7750,7750
16278,"2010-08-13 01:19:53.973","Using HBase 0.20.6 (with HBASE-2473) we encountered a situation where Regionserverprocess was shutting down and seemed to hang.Here is the bottom of region server log:http://pastebin.com/YYawJ4jAzookeeper-3.2.2 is used.Here is relevant portion from jstack - I attempted to attach jstack twice in my email to dev@hbase.apache.org but failed:""DestroyJavaVM"" prio=10 tid=0x00002aabb849c800 nid=0x6c60 waiting on condition [0x0000000000000000]   java.lang.Thread.State: RUNNABLE""regionserver/10.32.42.245:60020"" prio=10 tid=0x00002aabb84ce000 nid=0x6c81 in Object.wait() [0x0000000043755000]   java.lang.Thread.State: WAITING (on object monitor)        at java.lang.Object.wait(Native Method)        - waiting on <0x00002aaab76633c0> (a org.apache.zookeeper.ClientCnxn$Packet)        at java.lang.Object.wait(Object.java:485)        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1099)        - locked <0x00002aaab76633c0> (a org.apache.zookeeper.ClientCnxn$Packet)        at org.apache.zookeeper.ClientCnxn.close(ClientCnxn.java:1077)        at org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:505)        - locked <0x00002aaabf5e0c30> (a org.apache.zookeeper.ZooKeeper)        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.close(ZooKeeperWrapper.java:681)        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:654)        at java.lang.Thread.run(Thread.java:619)""main-EventThread"" daemon prio=10 tid=0x0000000043474000 nid=0x6c80 waiting on condition [0x00000000413f3000]   java.lang.Thread.State: WAITING (parking)        at sun.misc.Unsafe.park(Native Method)        - parking to wait for  <0x00002aaabf6e9150> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:414)","ZOOKEEPER-846","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-09-22 14:39:00.416","Closed","zookeeper client doesn't shut down cleanly on the close call","Bug","2011-11-24 03:22:15.054",0,2,7818,8254
16308,"2010-07-12 15:14:10.246","SBT doesn't like the pom file for zookeeper because while it's under the ""org.apache.hadoop"" directory, it's organisation is actually ""org.apache.zookeeper"". A simple fix for this is to just change ""org.apache.zookeeper"" to ""org.apache.hadoop"".","ZOOKEEPER-813","Critical","ZOOKEEPER","ZooKeeper","ASF","Duplicate","2010-07-13 06:49:14.922","Closed","maven install is broken due to incorrect organisation","Bug","2011-11-24 03:22:49.048",0,0,8262,8262
16280,"2010-08-13 00:05:02.313","ClientCnxn.java currently has the following code:  if (replyHdr.getXid() == -4) {                // -2 is the xid for AuthPacket                // TODO: process AuthPacket here                if (LOG.isDebugEnabled()) {                    LOG.debug(""Got auth sessionid:0x""                            + Long.toHexString(sessionId));                }                return;            }Auth failures appear to cause the server to disconnect but the client never gets a proper state change or notification that auth has failed, which makes handling this scenario very difficult as it causes the client to go into a loop of sending bad auth, getting disconnected, trying to reconnect, sending bad auth again, over and over. ","ZOOKEEPER-844","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-10-07 00:19:27.659","Closed","handle auth failure in java client","Bug","2011-11-24 03:22:13.91",0,0,7832,7832
16293,"2010-08-05 05:25:16.857","Reads and writes in BookKeeper are asymmetric: a write request writes one entry, whereas a read request may read multiple requests. The current implementation of throttling only counts the number of read requests instead of counting the number of entries being read. Consequently, a few read requests reading a large number of entries each will spawn a large number of read-entry requests. ","ZOOKEEPER-831","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-09-18 00:59:19.876","Closed","BookKeeper: Throttling improved for reads","Bug","2013-05-02 10:29:29.747",0,1,7750,7750
16299,"2010-07-19 23:51:01.628","Created a 3 node cluster.1 Fail the ZK leader2. Let leader election finish. Restart the leader and let it join the 3. Repeat After a few rounds leader election takes anywhere 25- 60 seconds to finish. Note- we didn't have any ZK clients and no new znodes were created.zoo.cfg is shown below:#Mon Jul 19 12:15:10 UTC 2010server.1=192.168.4.12\:2888\:3888server.0=192.168.4.11\:2888\:3888clientPort=2181dataDir=/var/zookeepersyncLimit=2server.2=192.168.4.13\:2888\:3888initLimit=5tickTime=2000I have attached logs from two nodes that took a long time to form the cluster after failing the leader. The leader was down anyways so logs from that node shouldn't matter.Look for ""START HERE"". Logs after that point should be of our interest.","ZOOKEEPER-822","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-10-07 01:03:01.538","Closed","Leader election taking a long time  to complete","Bug","2011-11-24 03:22:27.259",0,2,8126,8126
16301,"2010-07-17 00:00:32.56","When the c unit tests are run sometimes the server doesn't shutdown at the end of the test, this causes subsequent tests (hudson esp) to fail.1) we should try harder to make the server shut down at the end of the test, I suspect this is related to test failing/cleanup2) before the tests are run we should see if the old server is still running and try to shut it down","ZOOKEEPER-820","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-10-21 02:46:25.498","Closed","update c unit tests to ensure ""zombie"" java server processes don't cause failure","Bug","2011-11-24 03:22:45.874",0,0,7800,7818
16316,"2010-07-06 04:35:49.83","I'm seeing this frequently:     [exec] Zookeeper_simpleSystem::testPing : elapsed 18006 : OK     [exec] Zookeeper_simpleSystem::testAcl : elapsed 1022 : OK     [exec] Zookeeper_simpleSystem::testChroot : elapsed 3145 : OK     [exec] Zookeeper_simpleSystem::testAuth ZooKeeper server started : elapsed 25687 : OK     [exec] zktest-mt: /home/phunt/dev/workspace/gitzk/src/c/src/zookeeper.c:1952: zookeeper_process: Assertion `cptr' failed.     [exec] make: *** [run-check] Aborted     [exec] Zookeeper_simpleSystem::testHangingClientMahadev can you take a look?","ZOOKEEPER-804","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-10-21 00:27:44.043","Closed","c unit tests failing due to ""assertion cptr failed""","Bug","2011-11-24 03:22:40.855",0,0,7800,7818
16320,"2010-06-30 07:26:03.448","This happened when I called zoo_add_auth() immediately after zookeeper_init(). It took me a while to figure out that authentication actually failed since zoo_add_auth() returned ZOK. It should return ZINVALIDSTATE instead. --Michi","ZOOKEEPER-800","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-10-22 06:52:01.929","Closed","zoo_add_auth returns ZOK if zookeeper handle is in ZOO_CLOSED_STATE","Bug","2011-11-24 03:22:43.007",0,3,7800,7800
16324,"2010-06-29 06:02:04.839","So currently the pid file has to be tied to the datadirectory when starting zkServer.sh. It would be good to be able to break them up.","ZOOKEEPER-796","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-07-07 05:51:48.099","Closed","zkServer.sh should support an external PIDFILE variable","Bug","2011-11-24 03:22:31.014",0,0,8266,8266
16325,"2010-06-28 18:12:26.211","Hi,I notice a problem with the eventThread located in ClientCnxn.java file.The eventThread isn't shutdown after a connection ""session expired"" event coming (i.e. never receive EventOfDeath).When a session timeout occurs and the session is marked as expired, the connexion is fully closed (socket, SendThread...) expect for the eventThread.As a result, if i create a new zookeeper object and connect through it, I got a zombi thread which will never be kill (as for the previous zookeeper object, the state is already close, calling close again don't do anything).So everytime I will create a new zookeeper connection after a expired session, I will have a one more zombi EventThread.How to reproduce :- Start a zookeeper client connection in debug mode- Pause the jvm enough time to the expired event occur- Watch for example with jvisualvm the list of threads, the sendThread is succesfully killed, but the EventThread go to wait state for a infinity of time- if you reopen a new zookeeper connection, and do again the previous steps, another EventThread will be present in infinite wait state","ZOOKEEPER-795","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-08-18 04:05:18.969","Closed","eventThread isn't shutdown after a connection ""session expired"" event coming","Bug","2013-06-14 02:28:19.759",0,3,8258,8267
16326,"2010-06-26 10:47:15.527","I noticed that ZooKeeper has different behaviors when calling synchronous or asynchronous actions on a closed ZooKeeper client.Actually a synchronous call will throw a ""session expired"" exception while an asynchronous call will do nothing. No exception, no callback invocation.Actually, even if the EventThread receives the Packet with the session expired err code, the packet is never processed since the thread has been killed by the ventOfDeath. So the call back is not invoked.","ZOOKEEPER-794","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-10-21 08:47:39.88","Closed","Callbacks are not invoked when the client is closed","Bug","2011-11-24 03:22:42.367",0,3,8268,8268
16328,"2010-06-25 05:36:54.382","We recently upgraded zookeeper from 3.2.1 to 3.3.1, now we are seeing less client deadlock on session expiration, which is a definite plus!Unfortunately we are seeing memory leak that requires our zk clients to be restarted every half-day. Valgrind result:==8804== 25 (12 direct, 13 indirect) bytes in 1 blocks are definitely lost in loss record 255 of 670==8804==    at 0x4021C42: calloc (vg_replace_malloc.c:418)==8804==    by 0x5047B42: parse_acls (zookeeper.c:369)==8804==    by 0x5047EF6: pyzoo_create (zookeeper.c:1009)==8804==    by 0x40786CC: PyCFunction_Call (in /usr/lib/libpython2.4.so.1.0)==8804==    by 0x40B31DC: PyEval_EvalFrame (in /usr/lib/libpython2.4.so.1.0)==8804==    by 0x40B4485: PyEval_EvalCodeEx (in /usr/lib/libpython2.4.so.1.0)","ZOOKEEPER-792","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-08-23 10:59:54.018","Closed","zkpython memory leak","Bug","2011-11-24 03:22:41.041",0,0,8270,8270
16330,"2010-06-23 00:47:56.833","The leader code is setting the last processed zxid to the first of the new epoch even before connecting to a quorum of followers. Because the leader code sets this value before connecting to a quorum of followers (Leader.java:281) and the follower code throws an IOException (Follower.java:73) if the leader epoch is smaller, we have that when the false leader drops leadership and becomes a follower, it finds a smaller epoch and kills itself.","ZOOKEEPER-790","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-07-30 05:11:57.258","Closed","Last processed zxid set prematurely while establishing leadership","Bug","2011-11-24 03:22:17.505",0,2,7750,7750
16333,"2010-06-11 00:46:55.409","The pom deployed to repo1.maven.org has the project declared like this:<groupId>org.apache.zookeeper</groupId><artifactId>zookeeper</artifactId><packaging>jar</packaging><version>3.3.1</version>But it is deployed here: http://repo2.maven.org/maven2/org/apache/hadoop/zookeeper/3.3.1So either the groupId needs to change or the location it is deployed to needs to be changed because having them different results in bad behavior.  If you specify the correct groupId in your own pom/ivy files you can't even download zookeeper because it's not where your pom says it is and if you use the ""incorrect"" groupId then you can download zookeeper but then ivy complains about:[error] :: problems summary ::[error] :::: ERRORS[error] 		public: bad organisation found in http://repo1.maven.org/maven2/org/apache/hadoop/zookeeper/3.3.1/zookeeper-3.3.1.pom: expected='org.apache.hadoop' found='org.apache.zookeeper'","ZOOKEEPER-787","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-09-15 23:39:19.11","Closed","groupId in deployed pom is wrong","Bug","2011-11-24 03:22:17.017",1,1,NULL,8271
16335,"2010-06-03 06:51:50.629","The following config causes an infinite loop[zoo.cfg]tickTime=2000dataDir=/var/zookeeper/clientPort=2181initLimit=10syncLimit=5server.0=localhost:2888:3888Output:2010-06-01 16:20:32,471 - INFO [main:QuorumPeerMain@119] - Starting quorum peer2010-06-01 16:20:32,489 - INFO [main:NIOServerCnxn$Factory@143] - binding to port 0.0.0.0/0.0.0.0:21812010-06-01 16:20:32,504 - INFO [main:QuorumPeer@818] - tickTime set to 20002010-06-01 16:20:32,504 - INFO [main:QuorumPeer@829] - minSessionTimeout set to -12010-06-01 16:20:32,505 - INFO [main:QuorumPeer@840] - maxSessionTimeout set to -12010-06-01 16:20:32,505 - INFO [main:QuorumPeer@855] - initLimit set to 102010-06-01 16:20:32,526 - INFO [main:FileSnap@82] - Reading snapshot /var/zookeeper/version-2/snapshot.c2010-06-01 16:20:32,547 - INFO [Thread-1:QuorumCnxManager$Listener@436] - My election bind port: 38882010-06-01 16:20:32,554 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@620] - LOOKING2010-06-01 16:20:32,556 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@649] - New election. My id = 0, Proposed zxid = 122010-06-01 16:20:32,558 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@689] - Notification: 0, 12, 1, 0, LOOKING, LOOKING, 02010-06-01 16:20:32,560 - WARN [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@623] - Unexpected exceptionjava.lang.NullPointerExceptionat org.apache.zookeeper.server.quorum.FastLeaderElection.totalOrderPredicate(FastLeaderElection.java:496)at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:709)at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:621)2010-06-01 16:20:32,560 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@620] - LOOKING2010-06-01 16:20:32,560 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@649] - New election. My id = 0, Proposed zxid = 122010-06-01 16:20:32,561 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@689] - Notification: 0, 12, 2, 0, LOOKING, LOOKING, 02010-06-01 16:20:32,561 - WARN [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@623] - Unexpected exceptionjava.lang.NullPointerExceptionat org.apache.zookeeper.server.quorum.FastLeaderElection.totalOrderPredicate(FastLeaderElection.java:496)at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:709)at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:621)2010-06-01 16:20:32,561 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@620] - LOOKING2010-06-01 16:20:32,562 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@649] - New election. My id = 0, Proposed zxid = 122010-06-01 16:20:32,562 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@689] - Notification: 0, 12, 3, 0, LOOKING, LOOKING, 02010-06-01 16:20:32,562 - WARN [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@623] - Unexpected exceptionjava.lang.NullPointerExceptionThings like HBase require that the zookeeper servers be listed in the zoo.cfg. This is a bug on their part, but zookeeper shouldn't null pointer in a loop though.","ZOOKEEPER-785","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-09-15 05:09:31.912","Closed"," Zookeeper 3.3.1 shouldn't infinite loop if someone creates a server.0 line","Bug","2011-11-24 03:22:38.284",0,0,7818,8266
16337,"2010-06-02 03:22:43.91","ZKDatabase.getCommittedLog() returns a reference to the LinkedList<Proposal> committedLog in ZKDatabase. This is then iterated over by at least one caller. I have seen a bug that causes a NPE in LinkedList.clear on committedLog, which I am pretty sure is due to the lack of synchronization. This bug has not been apparent in normal ZK operation, but in code that I have that starts and stops a ZK server in process repeatedly (clear() is called from ZooKeeperServerMain.shutdown()). It's better style to defensively copy the list in getCommittedLog, and to synchronize on the list in ZKDatabase.clear.","ZOOKEEPER-783","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-07-27 06:45:07.929","Closed","committedLog in ZKDatabase is not properly synchronized","Bug","2011-11-24 03:22:08.911",1,0,7886,7886
16338,"2010-06-01 04:47:30.744"," The C API Doxygen documentation states:"" .... If the client is ever disconnected from the service, even if the  disconnection is temporary, the watches of the client will be removed from  the service, so a client must treat a disconnect notification as an implicit  trigger of all outstanding watches.""This is incorrect as of v.3. Watches are only lost and need to be re-registered when a session times out. When a normal disconnection occurs watches are reset automatically on reconnection.The documentation in zookeeper.h needs to be updated to correct this explanation.","ZOOKEEPER-782","Trivial","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-07-15 01:54:01.366","Closed","Incorrect C API documentation for Watches","Bug","2011-11-24 03:22:40.301",0,1,7827,8230
16346,"2010-05-13 05:31:11.733","As title","ZOOKEEPER-774","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-05-15 07:32:45.779","Closed","Recipes tests are slightly outdated: they do not compile against JUnit 4.8","Bug","2011-11-24 03:22:43.242",0,0,8258,8258
16348,"2010-05-10 22:42:21.215","When utilizing the zkpython async get children api with a watch, i consistently get segfaults when the watcher is invoked to process events. ","ZOOKEEPER-772","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-08-12 02:31:53.2","Closed","zkpython segfaults when watcher from async get children is invoked.","Bug","2011-11-24 03:22:11.978",0,0,7886,8060
16351,"2010-05-07 02:01:46.082","In short: it seems leader can treat observers as quorum members.Steps to repro:1. Server configuration: 3 voters, 2 observers (attached).2. Bring up 2 voters and one observer. It's enough for quorum.3. Shut down the one from the quorum who is the follower.As I understand, expected result is that leader will start a new election round so that to regain quorum.But the real situation is that it just says goodbye to that follower, and is still operable. (When I'm shutting down 3rd one -- observer -- leader starts trying to regain a quorum).(Expectedly, if on step 3 we shut down the leader, not the follower, remaining follower starta new leader election, as it should be).","ZOOKEEPER-769","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-05-22 00:23:11.993","Closed","Leader can treat observers as quorum members","Bug","2011-11-24 03:22:27.092",0,3,8258,8258
16354,"2010-05-06 03:04:27.61","Update the forrest recipes docs to point to the recipe implementations (where available).","ZOOKEEPER-766","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-05-06 06:51:25.76","Closed","forrest recipes docs don't mention the lock/queue recipe implementations available in the release","Bug","2011-11-24 03:22:07.005",0,0,7818,7818
16356,"2010-05-05 05:41:07.938","In ZOOKEEPER-690, we noticed that an observer was being elected, and Henry proposed a patch to fix the issue. However, it seems that the patch does not solve the issue one user (Alan Cabrera) has observed. Given that we would like to fix this issue, and to work separately with Alan to determine the problem with his setup, I'm creating this jira and re-posting Henry's patch.","ZOOKEEPER-764","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-05-06 06:27:31.662","Closed","Observer elected leader due to inconsistent voting view","Bug","2011-11-24 03:22:22.736",0,0,7886,7750
16357,"2010-05-04 21:09:43.434","deadlocks occur if we attempt to close a handle while there are any outstanding async requests (aget, acreate, etc). Normally on close both the io thread terminates and the completion thread are terminated and joined, however w\ith outstanding async requests, the completion thread won't be in a joinable state, and we effectively hang when the main thread does the join.afaics ideal behavior would be on close of a handle, to effectively clear out any remaining callbacks and let the completion thread terminate.i've tried adding some bookkeeping to within a python client to guard against closing while there is an outstanding async completion request, but its an imperfect solution since even after the python callback is executed there is still a window for deadlock before the completion thread finishes the callback.a simple example to reproduce the deadlock is attached.","ZOOKEEPER-763","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-05-06 06:02:10.281","Closed","Deadlock on close w/ zkpython / c client","Bug","2011-11-24 03:22:44.17",0,1,7886,8060
16678,"2009-06-09 02:04:40.019","if addauth is called on a new client connection that's never connected to the server, when the client does connect(syncconnected) the auth is not passed to the server. we should ensure we addauth when the client connects or reconnects","ZOOKEEPER-438","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-26 04:42:22.588","Closed","addauth fails to register auth on new client that's not yet connected","Bug","2009-07-09 04:24:05.791",0,0,7829,7818
16362,"2010-04-29 09:28:10.877","Currently when setting an acl, there is a minimal parse to ensure that its a list of dicts, however if one of the dicts is missing a required key, the subsequent usage doesn't check for it, and will segfault.. for example using an acl of [{""schema"":id, ""id"":world, permissions:PERM_ALL}] will segfault if used, because the scheme key is missing (its been purposefully typo'd to schema in example). I've expanded the check_acl macro to include verifying that all keys are present and added some unit tests against trunk in the attachments.","ZOOKEEPER-758","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-05-01 09:14:12.653","Closed","zkpython segfaults on invalid acl with missing key","Bug","2011-11-24 03:22:46.429",0,0,8060,8060
16389,"2010-03-27 07:12:47.541","    /**     * The Asynchronous version of delete. ""The request doesn't  *missing* actually until     * the asynchronous callback is called.""     */    public void delete(final String path, int version, VoidCallback cb, Object ctx) .. Also some information in the javadoc about how to instantiate the callback objects / context would be useful . ","ZOOKEEPER-731","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-09-06 22:12:50.809","Closed","Zookeeper#delete  , #create - async versions miss a verb in the javadoc ","Bug","2011-11-24 03:22:41.993",0,1,8100,8282
16370,"2010-04-23 00:45:27.391","The maven artifacts are currently (3.3.0) put into the toplevel of the release. This causes confusionamonst new users (ie ""which jar do I use?""). Also the naming of the bin jar is wrong for maven (to putonto the maven repo it must be named without the -bin) which adds extra burden for the releasemanager. Putting into a subdir fixes this and makes it explicit what's being deployed to maven repo.","ZOOKEEPER-750","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-04-29 10:26:35.492","Closed","move maven artifacts into ""dist-maven"" subdir of the release (package target)","Bug","2011-11-24 03:22:44.751",0,0,7818,7818
16371,"2010-04-23 00:21:07.105","See this JIRA/comment for background:https://issues.apache.org/jira/browse/ZOOKEEPER-425?focusedCommentId=12859697&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12859697basically the issue is that OSGi metadata is included in the legacy jar (zookeeper-<version>.jar) but not in the binary onlyjar (zookeeper-<version>-bin.jar) which is eventually deployed to the maven repo.","ZOOKEEPER-749","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-04-29 10:02:37.827","Closed","OSGi metadata not included in binary only jar","Bug","2011-11-24 03:22:23.99",0,1,7818,7818
16374,"2010-04-22 06:10:46.489","usability issue, should be in hex:2010-04-21 11:31:13,827 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11354:Learner@95] - Revalidating client: 83353578391797760","ZOOKEEPER-746","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-04-26 12:18:44.892","Closed","learner outputs session id to log in dec (should be hex)","Bug","2011-11-24 03:22:08.382",0,0,7818,7818
16450,"2010-02-12 20:12:01.359","I think we should remove the close call in LedgerInputStream. ","ZOOKEEPER-668","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-02-19 10:43:07.107","Closed","Close method in LedgerInputStream doesn't do anything","Bug","2010-03-27 01:25:10.801",0,0,7750,7750
16378,"2010-04-16 07:21:08.062","On write operations, getting:Fatal Python error: deallocating NoneAbortedThis error happens on write operations only.  Here's the backtrace:Fatal Python error: deallocating NoneProgram received signal SIGABRT, Aborted.0x000000383fc30215 in raise () from /lib64/libc.so.6(gdb) bt#0  0x000000383fc30215 in raise () from /lib64/libc.so.6#1  0x000000383fc31cc0 in abort () from /lib64/libc.so.6#2  0x00002adbd0be8189 in Py_FatalError () from /usr/lib64/libpython2.4.so.1.0#3  0x00002adbd0bc7493 in PyEval_EvalFrame () from /usr/lib64/libpython2.4.so.1.0#4  0x00002adbd0bcab66 in PyEval_EvalFrame () from /usr/lib64/libpython2.4.so.1.0#5  0x00002adbd0bcbfe5 in PyEval_EvalCodeEx () from /usr/lib64/libpython2.4.so.1.0#6  0x00002adbd0bcc032 in PyEval_EvalCode () from /usr/lib64/libpython2.4.so.1.0#7  0x00002adbd0be8729 in ?? () from /usr/lib64/libpython2.4.so.1.0#8  0x00002adbd0be9bd8 in PyRun_SimpleFileExFlags () from /usr/lib64/libpython2.4.so.1.0#9  0x00002adbd0bf000d in Py_Main () from /usr/lib64/libpython2.4.so.1.0#10 0x000000383fc1d974 in __libc_start_main () from /lib64/libc.so.6#11 0x0000000000400629 in _start ()","ZOOKEEPER-742","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-04-22 14:44:05.032","Closed","Deallocatng None on writes","Bug","2011-11-24 03:22:47.255",0,0,7886,8285
16379,"2010-04-15 01:12:17.828","Create /foo using the REST proxy fails.Also upgrade to the latest Jersey/Grizzly while we are at it (fixes for func/security)","ZOOKEEPER-741","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-04-22 13:43:49.921","Closed","root level create on REST proxy fails","Bug","2011-11-24 03:22:23.078",0,0,7818,7818
16382,"2010-04-11 05:17:56.367","/home/y/include/zookeeper/zookeeper.jute.h:96: error: extra semicolon/home/y/include/zookeeper/zookeeper.jute.h:158: error: extra semicolon/home/y/include/zookeeper/zookeeper.jute.h:288: error: extra semicolonthe code generator needs to be updated to not output a naked semi","ZOOKEEPER-738","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-04-27 02:58:09.69","Closed","zookeeper.jute.h fails to compile with -pedantic ","Bug","2011-11-24 03:22:42.64",0,0,8281,7818
16383,"2010-04-11 05:13:23.975","nc closes the write channel as soon as it's sent it's information, for example ""echo stat|nc localhost 2181""in general this is fine, however the server code will close the socket as soon as it receives notice that nc hasclosed it's write channel. if not all the 4 letter word result has been written back to the client yet, this will causesome or all of the result to be lost - ie the client will not see the full result. this was introduced in 3.3.0 as partof a change to reduce blocking of the selector by long running 4letter words.here's an example of the logs from the server during thisecho -n stat | nc localhost 21812010-04-09 21:55:36,124 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@251] - Accepted socket connection from /127.0.0.1:421792010-04-09 21:55:36,124 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@968] - Processing stat command from /127.0.0.1:421792010-04-09 21:55:36,125 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@606] - EndOfStreamException: Unable to read additional data from client sessionid 0x0, likely client has closed socket2010-04-09 21:55:36,125 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1286] - Closed socket connection for client /127.0.0.1:42179 (no session established for client)[phunt@gsbl90850 zookeeper-3.3.0]$ 2010-04-09 21:55:36,126 - ERROR [Thread-15:NIOServerCnxn@422] - Unexpected Exception: java.nio.channels.CancelledKeyException	at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)	at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:59)	at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:395)	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.checkFlush(NIOServerCnxn.java:907)	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.flush(NIOServerCnxn.java:945)	at java.io.BufferedWriter.flush(BufferedWriter.java:236)	at java.io.PrintWriter.flush(PrintWriter.java:276)	at org.apache.zookeeper.server.NIOServerCnxn$2.run(NIOServerCnxn.java:1089)2010-04-09 21:55:36,126 - ERROR [Thread-15:NIOServerCnxn$Factory$1@82] - Thread Thread[Thread-15,5,main] diedjava.nio.channels.CancelledKeyException	at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)	at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:64)	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.wakeup(NIOServerCnxn.java:927)	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.checkFlush(NIOServerCnxn.java:909)	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.flush(NIOServerCnxn.java:945)	at java.io.BufferedWriter.flush(BufferedWriter.java:236)	at java.io.PrintWriter.flush(PrintWriter.java:276)	at org.apache.zookeeper.server.NIOServerCnxn$2.run(NIOServerCnxn.java:1089)","ZOOKEEPER-737","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-05-05 05:51:42.679","Closed","some 4 letter words may fail with netcat (nc)","Bug","2011-12-30 07:08:07.754",0,2,7827,7818
16385,"2010-04-08 05:26:46.532","The test should be fixed so that it runs only if ipv6 is enabled and does not run if ipv6 is not enabled.","ZOOKEEPER-735","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-04-09 02:32:57.946","Closed","cppunit test testipv6 assumes that the machine is ipv6 enabled.","Bug","2011-11-24 03:22:14.514",0,0,7827,7827
16386,"2010-04-07 06:43:57.242","While runniing ""ant test-core-java"" QuorumPeerTestBase.java and ZooKeeperServerMainTest.java fail. The problem seems to be in ZookeeperserverMainTest.java:MainThread():66 and in QuorumPeerBaseTest.java:MainThread:76.FileWriter.write() writes windows path to the conf file. Java does not like windows path. Therefore, the test complains that it cannot find myid and fails. Solution - convert windows path to UNIX path. This worked for me on windows.  Diffs are attached below. Solution not tested on Linux since for some reason build is failing (due to problems not related to this change).vmc-floorb-dhcp116-114:/opt/zksrc/zookeeper-3.3.0/src/java/test/org/apache/zookeeper/server # svn diffIndex: ZooKeeperServerMainTest.java===================================================================--- ZooKeeperServerMainTest.java	(revision 931240)+++ ZooKeeperServerMainTest.java	(working copy)@@ -61,7 +61,8 @@             if (!dataDir.mkdir()) {                 throw new IOException(""unable to mkdir "" + dataDir);             }-            fwriter.write(""dataDir="" + dataDir.toString() + ""\n"");+            String data = dataDir.toString().replace('\\', '/');+            fwriter.write(""dataDir="" + data + ""\n"");              fwriter.write(""clientPort="" + clientPort + ""\n"");             fwriter.flush();Index: quorum/QuorumPeerTestBase.java===================================================================--- quorum/QuorumPeerTestBase.java	(revision 931240)+++ quorum/QuorumPeerTestBase.java	(working copy)@@ -73,7 +73,8 @@             if (!dataDir.mkdir()) {                 throw new IOException(""Unable to mkdir "" + dataDir);             }-            fwriter.write(""dataDir="" + dataDir.toString() + ""\n"");+            String data = dataDir.toString().replace('\\', '/');+            fwriter.write(""dataDir="" + data + ""\n"");              fwriter.write(""clientPort="" + clientPort + ""\n"");             fwriter.write(quorumCfgSection + ""\n"");","ZOOKEEPER-734","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-04-27 04:02:17.001","Closed","QuorumPeerTestBase.java and ZooKeeperServerMainTest.java do not handle windows path correctly","Bug","2011-11-24 03:22:33.484",0,0,8126,8126
16398,"2010-03-22 22:49:42.212","zkServer.sh output the PID of the zookeeper process with:echo -n $! > ""$ZOOPIDFILE""This uses -n which sh's builtin echo does not support. From echo's manpage.<snip>     Some shells may provide a builtin echo command which is similar or identical to this utility.  Most notably, the builtin echo in sh(1) does not accept     the -n option.  Consult the builtin(1) manual page.</snip>This means that echo -n PID > ZOOPIDFILE will mean the contents of ZOOPIDFILE will be ""-n PID"". This stops zkServer.sh stop from working correctly.","ZOOKEEPER-722","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-04-12 14:33:22.841","Closed","zkServer.sh uses sh's builtin echo on BSD, behaves incorrectly.","Bug","2011-11-24 03:22:33.659",0,0,7956,7956
16400,"2010-03-21 16:04:47.971","The artifact with the sources to be published in the Maven repository should be named ${artifactId}-${version}-sources.jar not ${artifactId}-${version}-src.jar.See also: http://maven.apache.org/guides/mini/guide-central-repository-upload.html and ZOOKEEPER-224","ZOOKEEPER-720","Trivial","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-04-12 14:19:52.369","Closed","Use zookeeper-{version}-sources.jar instead of zookeeper-{version}-src.jar to publish sources in the Maven repository","Bug","2011-11-24 03:22:39.695",0,0,8292,8292
16401,"2010-03-20 01:35:11.359","Add throttling to client to control the rate of operations to bookies. ","ZOOKEEPER-719","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-07-10 04:54:36.285","Closed","Add throttling to BookKeeper client","Bug","2011-11-24 03:22:40.477",0,0,7750,7750
16402,"2010-03-20 00:09:05.046","when we moved to ivy, we didn't update the fatjar build.xml to grab libraries out of the new location that ivy uses for downloaded libraries.","ZOOKEEPER-718","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-20 01:28:07.879","Closed","the fatjar is missing libraries","Bug","2010-03-27 01:25:12.397",0,0,7829,7829
16451,"2010-02-12 03:26:56.89","The java client doesn't handle ipv6 numeric addresses as they are colon (:) delmited. After splitting the host/port on : we look for the port as the second entry in the array rather than the last entry in the array.","ZOOKEEPER-667","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-13 02:55:30.138","Closed","java client doesn't allow ipv6 numeric connect string","Bug","2012-06-20 18:39:39.833",0,1,7818,7818
16407,"2010-03-18 23:53:19.576","Hi guys,The following is not a bug report but rather a question - but as I am attaching large files I am posting it here rather than on mailinglist.Today we had major failure in our production environment. Machines in zookeeper cluster gone wild and all clients got disconnected.We tried to restart whole zookeeper cluster but cluster got stuck in leader election phase.Calling stat command on any machine in the cluster resulted in 'ZooKeeperServer not running' messageIn one of logs I noticed 'Invalid snapshot'  message which disturbed me a bit.We did not manage to make cluster work again with data. We deleted all version-2 directories on all nodes and then cluster started up without problems.Is it possible that snapshot/log data got corrupted in a way which made cluster unable to start?Fortunately we could rebuild data we store in zookeeper as we use it only for locks and most of nodes is ephemeral.I am attaching contents of version-2 directory from all nodes and server logs.Source problem occurred some time before 15. First cluster restart happened at 15:03.At some point later we experimented with deleting version-2 directory so I would not look at following restart because they can be misleading due to our actions.I am also attaching zoo.cfg. Maybe something is wrong at this place. As I know look into logs i see read timeout during initialization phase after 20secs (initLimit=10, tickTime=2000).Maybe all I have to do is increase one or other. which one? Are there any downsides of increasing tickTime.Best regards, Łukasz OsipiukPS. due to attachment size limit I used split. to untar use cat nodeX-version-2.tgz-* |tar -xz","ZOOKEEPER-713","Major","ZOOKEEPER","ZooKeeper","ASF","Invalid","2010-03-19 01:41:28.901","Closed","zookeeper fails to start - broken snapshot?","Bug","2010-03-27 01:20:46.795",0,0,NULL,8293
16409,"2010-03-18 19:39:31.14","Originally problem was described on Users mailing list starting with this [post|http://mail-archives.apache.org/mod_mbox/hadoop-zookeeper-user/201003.mbox/<3b910d891003160743k38e2e7c9y830b182d88396d55@mail.gmail.com>].Below I restate it in more organized form.We occasionally (few times a day) observe that our client application disconnects from Zookeeper cluster.Application is written in C++ and we are using libzookeeper_mt library. In version 3.2.2.The disconnects we are observing are probably related to some problems with our network infrastructure - we are observing periods with great packet loss between machines in our DC. Sometimes after client application (i.e. zookeeper library) reconnects to zookeeper cluster we are observing that all subsequent requests return ZSESSIONMOVED error. Restarting client app helps - we always pass 0 as clientid to zookeeper_init function so old session is not reused.On 16-03-2010 we observed few occurences of problem. Example ones:- 22:08; client IP 10.1.112.60 (app1); sessionID 0x22767e1c9630000- 14:21; client IP 10.1.112.61 (app2); sessionID 0x324dcc1ba580085I attach logs of cluster and application nodes (only stuff concerining zookeeper):- [^zookeeper-node1.log.2010-03-16.gz] - logs of zookeepr cluster node 1 10.1.112.62- [^zookeeper-node2.log.2010-03-16.gz] - logs of zookeepr cluster node 2 10.1.112.63- [^zookeeper-node3.log.2010-03-16.gz] - logs of zookeepr cluster node 3 10.1.112.64- [^app1.log.2010-03-16.gz] - application logs of app1 10.1.112.60- [^app2.log.2010-03-16.gz] - application logs of app2 10.1.112.61I also made some analysis of case at 22:08:- Network glitch which resulted in problem occurred at about 22:08.- From what I see since 17:48 node2 was the leader and it did notchange later yesterday.- Client was connected to node2 since 17:50- At around 22:09 client tried to connect to every node (1,2,3).Connections to node1 and node3 were closed with exception ""Exception causing close of session 0x22767e1c9630000due to java.io.IOException: Read error"". Connection to node2 stood alive.- All subsequent operations were refused with ZSESSIONMOVED error.Error visible both on client and on server side.","ZOOKEEPER-710","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-20 01:19:38.921","Closed","permanent ZSESSIONMOVED error after client app reconnects to zookeeper cluster","Bug","2010-03-27 01:25:12.009",0,0,7818,8293
16410,"2010-03-18 13:57:45.772","ant test in bookkeeper results incompile-test:    [javac] Compiling 10 source files to /home/phunt/dev/workspace/gitzk/build/contrib/bookkeeper/test    [javac] /home/phunt/dev/workspace/gitzk/src/contrib/bookkeeper/test/org/apache/bookkeeper/test/BaseTestCase.java:91: cannot find symbol    [javac] symbol  : constructor Factory(java.lang.Integer)    [javac] location: class org.apache.zookeeper.server.NIOServerCnxn.Factory    [javac]         serverFactory = new NIOServerCnxn.Factory(ZooKeeperDefaultPort);    [javac]                         ^    [javac] 1 errorFlavio can you take a look at this one? (patch)","ZOOKEEPER-709","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-18 14:23:51.732","Closed","bookkeeper build failing with missing factory","Bug","2010-03-27 01:25:11.979",0,0,7818,7818
16411,"2010-03-18 13:34:53.476","ant test in zkpython is failing. I think this is due to mahadev's changes to remove unnecessary exports from the client lib.     [exec] ImportError: /home/phunt/dev/workspace/gitzk/build/contrib/zkpython/lib.linux-x86_64-2.6/zookeeper.so: undefined symbol: deallocate_String_vectorMahadev can you take a look?","ZOOKEEPER-708","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-18 14:48:51.292","Closed","zkpython failing due to undefined symbol deallocate_String_vector","Bug","2010-03-27 01:25:11.932",0,0,7827,7818
16420,"2010-03-13 06:22:01.44","in some cases the tests are failing with JMX errors. From the logs I can see that QP was shutdown, however it did not exit it's thread until some time much later. This is causing interference with subsequent tests, causing the test to fail.I have a patch that attempts to verify that the QP was shutdown (by joining the thread). It turns out that tests based on QuorumBase do this check (join) however some of the other tests do not. I believe this will address the issue.","ZOOKEEPER-698","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-13 08:07:28.261","Closed","intermittent JMX test failures due to not verifying QuorumPeer shutdown ","Bug","2010-03-27 01:25:11.891",0,0,7818,7818
16500,"2009-12-11 03:11:11.443","http://hadoop.apache.org/zookeeper/docs/current/zookeeperAdmin.html#sc_zkMulitServerSetup1) the config file is missing line returns2) call out setting up the myid file as it's own bullet, otw it's too easy to miss3) we should make sure the values we use in examples are consistent, and resonable defaults","ZOOKEEPER-617","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-13 02:27:47.303","Closed","improve cluster setup documentation in forrest","Bug","2010-03-27 01:25:08.518",0,0,7818,7818
16422,"2010-03-10 13:41:35.685","seeing the following on the console for http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/729/looks like the cnxn is closed twice? (the second time 'sock' is null). perhaps it's due to client closing and sending session term, then closing socket, server sees the read return -1, so closes cnxn, then sees the session close request (which was queued)?    [junit] 2010-03-10 03:15:53,205 - INFO  [main:NIOServerCnxn@1232] - Closed socket connection for client /127.0.0.1:41285 which had sessionid 0x127461233fc0000    [junit] 2010-03-10 03:15:53,206 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:NIOServerCnxn$Factory@269] - Ignoring unexpected runtime exception    [junit] java.lang.NullPointerException    [junit] 	at org.apache.zookeeper.server.NIOServerCnxn.close(NIOServerCnxn.java:1232)    [junit] 	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:594)    [junit] 	at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:259)","ZOOKEEPER-696","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-11 11:17:29.41","Closed","NPE in the hudson logs, seems nioservercnxn closed twice","Bug","2010-03-27 01:25:11.859",0,0,7818,7818
16425,"2010-03-10 05:40:06.955","See http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/Zookeeper-Patch-h7.grid.sp2.yahoo.net/77/testReport/junit/org.apache.zookeeper.test/ObserverTest/testObserver/     [exec]     [junit] 2010-03-04 00:23:37,803 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11229:FastLeaderElection@683] - Notification time out: 3200     [exec]     [junit] 2010-03-04 00:23:37,804 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11229:FastLeaderElection@689] - Notification: 2, 0, 2, 3, LOOKING, LOOKING, 1ad infinitum. ","ZOOKEEPER-693","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-11 02:52:46.153","Closed","TestObserver stuck in tight notification loop in FLE","Bug","2010-03-27 01:25:11.813",0,2,7750,7886
16427,"2010-03-10 04:40:59.494","BookKeeper starts a ZooKeeper server and needs to create an NIOServer.Factory, but the constructor changed.","ZOOKEEPER-691","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-10 04:50:51.627","Closed","Interface changed for NIOServer.Factory","Bug","2010-03-27 01:25:11.744",0,0,7829,7829
16428,"2010-03-09 09:42:31.306","the hudson test failed on http://hudson.zones.apache.org/hudson/job/Zookeeper-Patch-h1.grid.sp2.yahoo.net/2/testReport/. There are huge set of cancelledkeyexceptions in the logs. Still going through the logs to find out the reason for failure.","ZOOKEEPER-690","Major","ZOOKEEPER","ZooKeeper","ASF","Cannot Reproduce","2011-07-17 01:49:55.918","Closed","AsyncTestHammer test fails on hudson.","Bug","2011-11-24 03:22:30.851",0,1,7886,7827
16429,"2010-03-09 09:23:12.34","ivysettings.xml was added in 3.3.0 but it is not copied into the release artifact via ant ""package"" target of build.xml","ZOOKEEPER-689","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-09 09:50:53.066","Closed","release build broken - ivysettings.xml not copied during ""package""","Bug","2010-03-27 01:25:11.406",0,0,7818,7818
16430,"2010-03-06 01:10:39.028","We are not clear enough (and the diagram we do have seems misleading) on _when_ session expirations are generated. In particular the fact that you only get expirations when the client is connected to the cluster, not when disconnected.we need to detail:1) when do you get expiration2) what is the sequence of events that the watcher sees, from disco state, to getting the expiration (say the expiration happens when the client is disco, what do you see in the watcher while you are getting reconnected)3) we need to give some examples of how to test this. We should be explicit that ""pulling the network cable"" on the client will not show expiration since the cliient will not be reconnected.","ZOOKEEPER-688","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-06 06:03:17.571","Closed","explain session expiration better in the docs & faq","Bug","2010-03-27 01:25:11.366",0,0,7818,7818
16431,"2010-03-05 06:12:23.629","LENonterminateTest fails with the following error:{noformat}2010-03-04 20:26:32,347 - INFO  [Thread-0:LeaderElection@155] - Server address: 0.0.0.0/0.0.0.0:112232010-03-04 20:26:32,348 - WARN  [Thread-0:LeaderElection@195] - Ignoring exception while looking for leaderjava.io.IOException: Network is unreachable	at java.net.PlainDatagramSocketImpl.send(Native Method)	at java.net.DatagramSocket.send(DatagramSocket.java:612)	at org.apache.zookeeper.server.quorum.LeaderElection.lookForLeader(LeaderElection.java:169)	at org.apache.zookeeper.test.LENonTerminateTest$LEThread.run(LENonTerminateTest.java:83){noformat}","ZOOKEEPER-687","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-05 06:56:09.471","Closed","LENonterminatetest fails on some machines.","Bug","2010-03-27 01:25:11.333",0,0,7827,7827
16434,"2010-03-03 18:18:39.22","testNonTermination failed during a Hudson run for ZOOKEEPER-59. After inspecting the output, it looks like server is electing 2 as a leader and leaving. Given that 2 is just a mock server, server 0 remains alone in leader election.","ZOOKEEPER-684","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-10 09:28:19.976","Closed","Race in LENonTerminateTest","Bug","2010-03-27 01:25:11.278",0,0,7886,7750
16435,"2010-03-01 15:04:20.575","LogFormatter fails to parse txn log files - seems the tool was never updated to handle FileHeader.It would be good to update the docs on txn log file to include detail on the file format.","ZOOKEEPER-683","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-04 07:07:51.589","Closed","LogFormatter fails to parse transactional log files","Bug","2010-03-27 01:25:11.219",0,0,7818,7818
16436,"2010-02-25 20:01:43.898","After the event notification response from server is received, the client will convert the server path to the client path if chrooted by:event.setPath(serverPath.substring(chrootPath.length());If chrootPath and serverPath are the same, then the event's path will be set to a null string.But the key of the watcher's map is ""/"", not a null string, so the watcher will not get notified at all.","ZOOKEEPER-682","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-04 09:29:04.515","Closed","Event is not processed when the watcher is set to watch ""/"" if chrooted","Bug","2010-03-27 01:25:11.187",0,1,8302,8302
16437,"2010-02-25 19:25:07.05","Just a small issue, the doc says that ""Setting this to 0 or omitting it entirely removes the limit on concurrent connections."", but we ran without this setting, and saw: WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn$Factory@226] - Too many connections from /10.76.251.190 - max is 10Bug in doc possibly?","ZOOKEEPER-681","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-05 10:27:39.856","Closed","Minor doc issue re unset maxClientCnxns","Bug","2010-03-27 01:25:11.119",0,0,7818,8303
16441,"2010-02-23 02:20:07.985","The c client doesn't handle ipv6 numeric addresses as they are colon : delmited. After splitting the host/port on : we look for the port as the second entry in the array rather than the last entry in the array.","ZOOKEEPER-677","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-11 00:34:04.532","Closed","c client doesn't allow ipv6 numeric connect string","Bug","2010-03-27 01:25:10.981",0,0,7827,7818
16445,"2010-02-17 03:03:13.579","We just need to remove the first two paragraphs of Section 2.","ZOOKEEPER-673","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-02-19 10:08:25.734","Closed","Fix observer documentation regarding leader election","Bug","2010-03-27 01:25:10.932",0,0,7750,7750
16449,"2010-02-13 04:46:04.064","the current tostring method is broken","ZOOKEEPER-669","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-02-24 02:25:59.913","Closed","watchedevent tostring should clearly output the state/type/path","Bug","2010-03-27 01:25:10.867",0,0,7818,7818
16455,"2010-02-03 01:30:51.711","http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/686/java.lang.RuntimeException: Unable to run quorum server 	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:380)	at org.apache.zookeeper.test.ZkDatabaseCorruptionTest.testCorruption(ZkDatabaseCorruptionTest.java:99)Caused by: java.io.IOException: Invalid magic number 0 != 1514884167	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:455)	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:471)	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:438)	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:519)	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:145)	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:193)	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:377)","ZOOKEEPER-663","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-09 09:25:03.717","Closed","hudson failure in ZKDatabaseCorruptionTest","Bug","2010-03-27 01:25:10.309",0,0,7827,7818
16456,"2010-02-02 10:57:18.903","I have a zookeeper cluster with 5 servers, zookeeper version 3.2.1, here is the content in the configure file, zoo.cfg======# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=5# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=2# the directory where the snapshot is stored.dataDir=./data/# the port at which the clients will connectclientPort=8181# zookeeper cluster listserver.100=10.23.253.43:8887:8888server.101=10.23.150.29:8887:8888server.102=10.23.247.141:8887:8888server.200=10.65.20.68:8887:8888server.201=10.65.27.21:8887:8888=====Before the problem happened, the server.200 was the leader. Yesterday morning, I found the there were many sockets with the state of CLOSE_WAIT on the clientPort (8181),  the total was over about 120. Because of these CLOSE_WAIT, the server.200 could not accept more connections from the clients. The only thing I can do under this situation is restart the server.200, at about 2010-02-01 06:06:35. The related log is attached to the issue.","ZOOKEEPER-662","Major","ZOOKEEPER","ZooKeeper","ASF","Duplicate","2011-07-17 01:48:49.976","Closed","Too many CLOSE_WAIT socket state on a server","Bug","2011-11-24 03:22:30.223",1,1,NULL,8217
16461,"2010-01-24 09:27:49.278","Thread.run() used instead of Thread.start() . ","ZOOKEEPER-656","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-25 11:35:57.359","Closed","SledgeHammer test - thread.run() deprecated ","Bug","2010-03-27 01:25:10.217",0,0,8282,8282
16470,"2010-01-16 06:33:58.998","http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/666/testReport/org.apache.zookeeper.test/QuorumTest/testLeaderShutdown/junit.framework.AssertionFailedError: QP failed to shutdown in 30 seconds	at org.apache.zookeeper.test.QuorumBase.shutdown(QuorumBase.java:293)	at org.apache.zookeeper.test.QuorumBase.shutdownServers(QuorumBase.java:281)	at org.apache.zookeeper.test.QuorumBase.tearDown(QuorumBase.java:266)	at org.apache.zookeeper.test.QuorumTest.tearDown(QuorumTest.java:55)Flavio, can you triage this one?","ZOOKEEPER-647","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-21 11:08:33.688","Closed","hudson failure in testLeaderShutdown","Bug","2010-03-27 01:25:09.783",0,0,7750,7818
16473,"2010-01-15 02:43:44.552","the nighthly build has been failing. http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/664/. The problem seems to be {code}BUILD FAILEDjava.lang.NoClassDefFoundError: org/apache/ivy/ant/IvyMakePom$MappingTotal time: 15 minutes 14 seconds{code}","ZOOKEEPER-644","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-16 06:36:06.872","Closed","Nightly build failed on hudson.","Bug","2010-03-27 01:25:09.739",0,0,7818,7827
16480,"2010-01-07 00:26:42.178","The trunk build is failing when Hudson runs it. The problem seems to be that ivy-init is executed only once, but its definitions (in particular ivy:settings) do not persist, and the failure occurs when we run ivy-retrieve a second time, which requires the definition of ivy:settings.It seems that the problem occur with ant 1.7.0, but not with 1.7.1, so it could be an ant issue. ","ZOOKEEPER-637","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-07 16:42:59.974","Closed","Trunk build is failing","Bug","2010-03-27 01:33:22.956",0,0,7750,7750
16487,"2009-12-16 15:39:59.693","There are two identical ObserverTest.java files in trunk. ","ZOOKEEPER-630","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-17 02:30:27.843","Closed","Trunk has duplicate ObserverTest.java files","Bug","2010-03-27 01:25:09.302",0,0,7886,7886
16488,"2009-12-16 13:00:56.204","FLELostMessageTest assumes that the first zxid exchange will be -1 zxid. WIth ZOOKEEPER-596 the zxid would be 0 and not -1. So the corresponding change needs to be made to this test.","ZOOKEEPER-629","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-18 10:20:53.052","Closed","FLELostMessageTest assumes that the first zxid on a startup of quorum is -1.","Bug","2013-05-02 10:29:23.62",0,0,7750,7827
16503,"2009-12-08 07:31:18.603","getClientCnxnCount reads from Factory.ipMap without synchronizing. ","ZOOKEEPER-614","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-12 07:04:14.14","Closed","Improper synchronisation in getClientCnxnCount","Bug","2010-03-27 01:25:08.436",0,1,7886,7886
16675,"2009-06-09 19:32:58.884","The latest version of trunk has a src/c/tests/TestClientRetry.cc file that has the actual file from ZK-336 appended to itself. This causes the compilation to fail due to lots of redeclaration errors. ","ZOOKEEPER-441","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-10 00:46:47.508","Closed","Zk-336 diff got applied twice to TestClientRetry.cc C test, causing compilation failure","Bug","2010-03-27 01:31:23.109",0,1,7886,7886
16490,"2009-12-16 08:45:07.023","Reported on the list:""I'm working on using ZooKeeper for an internal application at Digg.  I've been using the zkpython package and I just noticed that the data I was receiving from a zookeeper.get() call was being truncated.  After some quick digging I found that zookeeper.c limits the data returned to 512 characters (see http://svn.apache.org/viewvc/hadoop/zookeeper/tags/release-3.2.2/src/contrib/zkpython/src/c/zookeeper.c?view=markup line 855).Is there a reason for this?  The only information regarding node size that I've read is that it should not exceed 1MB so this limit seems a bit arbitrary and restrictive.Thanks for the great work!Rich""","ZOOKEEPER-627","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-17 07:28:40.868","Closed","zkpython arbitrarily restricts the size of a 'get' to 512 bytes","Bug","2010-03-27 01:25:09.224",0,1,7886,7886
16491,"2009-12-16 01:54:33.216","Java/c clients should output xid/sessionids (incl ephemeralowner) in hex format    private static void printStat(Stat stat) {        System.err.println(""cZxid = "" + stat.getCzxid());        System.err.println(""ctime = "" + new Date(stat.getCtime()).toString());        System.err.println(""mZxid = "" + stat.getMzxid());        System.err.println(""mtime = "" + new Date(stat.getMtime()).toString());        System.err.println(""pZxid = "" + stat.getPzxid());        System.err.println(""cversion = "" + stat.getCversion());        System.err.println(""dataVersion = "" + stat.getVersion());        System.err.println(""aclVersion = "" + stat.getAversion());        System.err.println(""ephemeralOwner = "" + stat.getEphemeralOwner());        System.err.println(""dataLength = "" + stat.getDataLength());        System.err.println(""numChildren = "" + stat.getNumChildren());    }","ZOOKEEPER-626","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-22 08:49:36.873","Closed","ensure the c/java cli's print xid/sessionid/etc... in hex","Bug","2010-03-27 01:25:09.165",0,0,7818,7818
16493,"2009-12-15 11:17:08.521","I encountered a problem today that the Zookeeper C Client (version 3.2.0) core dump when reconnected and did some operations on the zookeeper server which just restarted. The gdb infomation is like:(gdb) bt#0  0x000000302af71900 in memcpy () from /lib64/tls/libc.so.6#1  0x000000000047bfe4 in ia_deserialize_string (ia=Variable ""ia"" is not available.) at src/recordio.c:270#2  0x000000000047ed20 in deserialize_CreateResponse (in=0x9cd870, tag=0x50a74e ""reply"", v=0x409ffe70) at generated/zookeeper.jute.c:679#3  0x000000000047a1d0 in zookeeper_process (zh=0x9c8c70, events=Variable ""events"" is not available.) at src/zookeeper.c:1895#4  0x00000000004815e6 in do_io (v=Variable ""v"" is not available.) at src/mt_adaptor.c:310#5  0x000000302b80610a in start_thread () from /lib64/tls/libpthread.so.0#6  0x000000302afc6003 in clone () from /lib64/tls/libc.so.6#7  0x0000000000000000 in ?? ()(gdb) f 1#1  0x000000000047bfe4 in ia_deserialize_string (ia=Variable ""ia"" is not available.) at src/recordio.c:270270     in src/recordio.c(gdb) info localspriv = (struct buff_struct *) 0x9cd8d0len = -1rc = Variable ""rc"" is not available.According to the source code,int ia_deserialize_string(struct iarchive *ia, const char *name, char **s){    struct buff_struct *priv = ia->priv;    int32_t len;    int rc = ia_deserialize_int(ia, ""len"", &len);    if (rc < 0)        return rc;    if ((priv->len - priv->off) < len) {        return -E2BIG;    }    *s = malloc(len+1);    if (!*s) {        return -ENOMEM;    }    memcpy(*s, priv->buffer+priv->off, len);    (*s)[len] = '\0';    priv->off += len;    return 0;}the variable len is set by ia_deserialize_int, and the returned len doesn't been checked, so the client segment fault when trying to memcpy -1 byte data.In the source file recordio.c, there are many functions which don't check the returned len. They all might cause segment fault in some kind of  situations.","ZOOKEEPER-624","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-17 08:05:38.296","Closed","The C Client cause core dump when receive error data from Zookeeper Server","Bug","2010-03-27 01:25:08.822",0,1,7827,8217
16494,"2009-12-14 16:49:23.292","Class org.apache.bookkeeper.util.ClientBase requires junit, and when I tried to just compile bookkeeper, no test, with the patch of ZOOKEEPER-534, compilation failed. ","ZOOKEEPER-623","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-13 04:12:51.31","Closed","ClientBase in bookkeeper.util requires junit","Bug","2013-05-02 10:29:22.13",0,0,7750,7750
16495,"2009-12-13 06:22:59.237","Valgrind found:{quote}==2357== Conditional jump or move depends on uninitialised value(s)==2357==    at 0x807FDCA: check_events (zookeeper.c:1180)==2357==    by 0x808043A: zookeeper_process (zookeeper.c:1775)==2357==    by 0x806A21B: Zookeeper_close::testCloseConnected1() (TestZookeeperClose.cc:161)==2357==    by 0x806C6BF: CppUnit::TestCaller<Zookeeper_close>::runTest() (TestCaller.h:166){quote}zookeeper.c:1180 was the first if in send_set_watches.","ZOOKEEPER-622","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-06 13:35:08.681","Closed","Test for pending watches in send_set_watches should be moved","Bug","2010-03-27 01:25:08.724",0,0,7829,8325
16496,"2009-12-13 05:36:22.173","Giri can you take a look? Any recent changes on hudson that could have caused this?http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/621/jute:    [javac] Compiling 38 source files to /grid/0/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build/classes   [clover] Clover Version 2.4.3, built on March 09 2009 (build-756)   [clover] Loaded from: /homes/hudson/tools/clover/latest/lib/clover.jar   [clover] Clover: Open Source License registered to Apache.   [clover] Failed to create temp directory   [clover] ** Error(s) occurred and the instrumentation process can't continue.BUILD FAILED/grid/0/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build.xml:879: The following error occurred while executing this line:/grid/0/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build.xml:199: com.cenqua.clover.CloverException: Failed to create temp directory","ZOOKEEPER-621","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-14 18:42:05.597","Closed","hudson failure ZooKeeper-trunk/621 - clover issue","Bug","2010-03-27 01:25:08.672",0,0,8063,7818
16497,"2009-12-11 05:34:02.88","http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/590/warningsResult/HIGH/If you click on any of these links you will see that these are not compiler warnings:http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/590/warningsResult/HIGH/file.-1602148846/Giri can you take a look at these and resolve?","ZOOKEEPER-620","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-19 18:33:58.053","Closed","hudson is not reporting compiler warning correctly","Bug","2010-03-27 01:25:08.625",0,0,8063,7818
16498,"2009-12-11 05:31:25.782","Hudson is not reporting this correctly but:http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/612/    [junit] 2009-12-03 08:46:27,297 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11301:LeaderElection@109] - 5	-> 2    [junit] Running org.apache.zookeeper.test.QuorumTest    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec    [junit] Test org.apache.zookeeper.test.QuorumTest FAILED (timeout)    [junit] 2009-12-03 08:46:28,390 - INFO  [main:PortAssignment@31] - assigning port 11221    [junit] 2009-12-03 08:46:28,393 - INFO  [main:PortAssignment@31] - assigning port 11222    [junit] Running org.apache.zookeeper.test.QuorumZxidSyncTestbut this makes no sense - how is this a timeout?One concern is - perhaps this is a deadlock?","ZOOKEEPER-619","Critical","ZOOKEEPER","ZooKeeper","ASF","Invalid","2009-12-12 03:09:27.434","Closed","hudson test failure in QuorumTest -- timeout error","Bug","2010-03-27 01:25:08.572",0,0,7818,7818
16502,"2009-12-10 06:17:48.258","The javadoc for create with a sequence flag mentions a suffix of ""\_i"" but the true suffix is just ""i"" and no ""\_"".","ZOOKEEPER-615","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-12 03:39:02.18","Closed","wrong javadoc for create with a sequence flag","Bug","2010-03-27 01:25:08.475",0,0,7827,7827
16506,"2009-12-07 09:47:46.084","The hudson build failure failed again on http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/582/","ZOOKEEPER-611","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-09 04:21:07.679","Closed","hudson build failiure","Bug","2010-03-27 01:25:08.326",0,0,7827,7827
16507,"2009-12-04 14:50:58.738","There are a number of places where we have non-final fields that could (should) be declared as final.","ZOOKEEPER-610","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-12 06:57:06.381","Closed","cleanup final fields, esp those used for locking","Bug","2010-03-27 01:25:08.285",0,0,7818,7818
16508,"2009-12-04 03:32:32.128","ObserverTest failed running on 8coreI ran the test as:ant -Dtest.junit.output.format=xml -Dtest.output -Dtestcase=AsyncHammerTest clean test-core-java &> test.out","ZOOKEEPER-609","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-17 02:29:54.651","Closed","ObserverTest failure ""zk should not be connected expected not same""","Bug","2010-03-27 01:25:08.232",0,0,7886,7818
16511,"2009-12-02 09:10:23.536","the scripts in bin fail under cygwin due to spaces not handled properly","ZOOKEEPER-606","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-18 06:09:57.241","Closed","bin scripts don't work in cygwin (spaces in paths)","Bug","2010-03-27 01:25:07.845",0,0,7818,7818
16513,"2009-12-02 05:01:58.757","Currently the zookeeper seems to be exporting symbols not in the api. An example of this seems to be the symbol hash, which interferes with me using memcached and zookeeper in the same program.","ZOOKEEPER-604","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-13 05:44:22.323","Closed","zk needs to prevent export of any symbol not listed in their api","Bug","2010-09-04 01:21:25.59",0,2,7827,8266
16514,"2009-12-02 03:27:49.461","The general pattern is that the construction of a collection might fail, but the module is not freeing the memory that it has already allocated. Exceptions that are raised during this process aren't always propagated back to the Python side either. ","ZOOKEEPER-603","Major","ZOOKEEPER","ZooKeeper","ASF","Duplicate","2010-08-13 06:29:15.142","Closed","zkpython should do a better job of freeing memory under error conditions","Bug","2011-11-24 03:22:16.222",0,1,8270,7886
16517,"2009-12-01 06:22:37.305","I suppose the TODO below is referring to the ""path"" variable which is passed in as an output variable to PyArg_ParseTuple right below.  The TODO may be removed, since the code is right.  Code using PyArg_ParseTuple will borrow the reference from the calling code, since there's a stack behind the call to the enclosing function (pyzoo_get_children in this case) which won't go away until the function returns.Index: src/contrib/zkpython/src/c/zookeeper.c===================================================================--- src/contrib/zkpython/src/c/zookeeper.c	(revision 885582)+++ src/contrib/zkpython/src/c/zookeeper.c	(working copy)@@ -774,8 +774,6 @@  static PyObject *pyzoo_get_children(PyObject *self, PyObject *args) {-  // TO DO: Does Python copy the string or the reference? If it's the former-  // we should free the String_vector   int zkhid;   char *path;   PyObject *watcherfn = Py_None;","ZOOKEEPER-600","Trivial","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-18 07:39:01.213","Closed","TODO pondering about allocation behavior in zkpython may be removed","Bug","2010-03-27 01:25:07.704",0,1,8220,8220
16519,"2009-11-26 03:53:54.553","Typo in thread constructor. Oops. ","ZOOKEEPER-598","Trivial","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-30 20:04:55.425","Closed","LearnerHandler is misspelt in the thread's constructor","Bug","2010-03-27 01:28:56.369",0,2,7886,7886
16520,"2009-11-26 02:18:59.253","ASyncHammerTest is failing intermittently on hudson trunk. There is no clear reason why this is happening, butit seems from the logs that a session connection to a follower is failing during session establishment - thefailure seems to be a problem either on the follower or leader. The server gets the session create request, butit stalls in the request processor pipeline. (we see it go in, but we do not see it com eout)unfortunately all efforts to reproduce this on non-hudson trunk have failed. Even trying to reproduce byrunning on hudson host itself (manually) has failed.We need to instrument the client session creation code in the test to dump the thread stack if thesession creation fails.","ZOOKEEPER-597","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-09 03:15:29.506","Closed","ASyncHammerTest is failing intermittently on hudson trunk","Bug","2010-03-27 01:25:07.593",0,0,7829,7818
16569,"2009-10-09 06:59:10.754","ZooKeeperException is not being added to the zookeeper module in zookeeper.c (zkpython). The other exceptionsare added but not ZooKeeperException. Sorry, I missed this in my previous change, I got all the subclasses but not zkex itself.","ZOOKEEPER-548","Critical","ZOOKEEPER","ZooKeeper","ASF","Invalid","2009-10-09 12:35:00.095","Closed","zookeeper.ZooKeeperException not added to the module in zkpython","Bug","2010-03-27 01:25:02.32",0,0,7818,7818
16521,"2009-11-25 06:51:29.258","It is possible that the last loggged zxid as reported by all the servers during leader election is not the last zxid that the server can upload data to. It is very much possible that some transaction or snapshot gets corrupted and the servers actually do not have valid data till last logged zxid. We need to make sure that what the servers report as there last logged zxid, they are able to load data till that zxid.","ZOOKEEPER-596","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-18 10:20:28.755","Closed","The last logged zxid calculated by zookeeper servers could cause problems in leader election if data gets corrupted.","Bug","2013-05-02 10:29:23.616",0,0,7827,7827
16524,"2009-11-25 00:54:45.672","The java client api does not allow the client to access the negotiated session timeout (c does allow this).In some cases the client may not get the requested timeout (server applies a min/max bound) in which casethe client user code may want to examine the timeout it did receive.","ZOOKEEPER-593","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-26 10:19:37.257","Closed","java client api does not allow client to access negotiated session timeout","Bug","2010-03-27 01:25:04.373",0,0,7818,7818
16525,"2009-11-24 17:48:39.02","When trying to compile bookkeeper, the compiler complains that it can't find junit. I suspect that this is related to the fact that zookeeper now fetches junit using ivy.","ZOOKEEPER-592","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-02-02 22:47:42.145","Closed","BookKeeper cannot find junit","Bug","2010-03-27 01:31:34.433",0,0,NULL,7750
16526,"2009-11-23 17:35:37.425","The following code produce a situation, where the C Client can not exit properly,#include ""include/zookeeper.h""void default_zoo_watcher(zhandle_t *zzh, int type, int state, const char *path, void* context){    int zrc = 0;    struct String_vector str_vec = {0, NULL};    printf(""in the default_zoo_watcher\n"");    zrc = zoo_wget_children(zzh, ""/mytest"", default_zoo_watcher, NULL, &str_vec);    printf(""zoo_wget_children, error: %d\n"", zrc);    return;}int main(){    int zrc = 0;    int buff_len = 10;     char buff[10] = ""hello"";    char path[512];    struct Stat stat;    struct String_vector str_vec = {0, NULL};    zhandle_t *zh = zookeeper_init(""10.81.20.62:2181"", NULL, 30000, 0, 0, 0);     zrc = zoo_create(zh, ""/mytest"", buff, 10, &ZOO_OPEN_ACL_UNSAFE, 0, path, 512);    printf(""zoo_create, error: %d\n"", zrc);    zrc = zoo_wget_children(zh, ""/mytest"", default_zoo_watcher, NULL, &str_vec);    printf(""zoo_wget_children, error: %d\n"", zrc);    zrc = zoo_create(zh, ""/mytest/test1"", buff, 10, &ZOO_OPEN_ACL_UNSAFE, 0, path, 512);    printf(""zoo_create, error: %d\n"", zrc);    zrc = zoo_wget_children(zh, ""/mytest"", default_zoo_watcher, NULL, &str_vec);    printf(""zoo_wget_children, error: %d\n"", zrc);    zrc = zoo_delete(zh, ""/mytest/test1"", -1);    printf(""zoo_delete, error: %d\n"", zrc);    zookeeper_close(zh);    return 0;}running this code can cause the program hang at zookeeper_close(zh);(line 38). using gdb to attach the process, I found that the main thread is waiting for do_completion thread to finish,(gdb) bt#0  0x000000302b806ffb in pthread_join () from /lib64/tls/libpthread.so.0#1  0x000000000040de3b in adaptor_finish (zh=0x515b60) at src/mt_adaptor.c:219#2  0x00000000004060ba in zookeeper_close (zh=0x515b60) at src/zookeeper.c:2100#3  0x000000000040220b in main ()and the thread which handle the zoo_wget_children(in the default_zoo_watcher) is waiting for sc->cond. (gdb) thread 2[Switching to thread 2 (Thread 1094719840 (LWP 25093))]#0  0x000000302b8089aa in pthread_cond_wait@@GLIBC_2.3.2 ()   from /lib64/tls/libpthread.so.0(gdb) bt#0  0x000000302b8089aa in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/tls/libpthread.so.0#1  0x000000000040d88b in wait_sync_completion (sc=0x5167f0) at src/mt_adaptor.c:82#2  0x00000000004082c9 in zoo_wget_children (zh=0x515b60, path=0x40ebc0 ""/mytest"", watcher=0x401fd8 <default_zoo_watcher>, watcherCtx=Variable ""watcherCtx"" is not available.)    at src/zookeeper.c:2884#3  0x0000000000402037 in default_zoo_watcher ()#4  0x000000000040d664 in deliverWatchers (zh=0x515b60, type=4, state=3, path=0x515100 ""/mytest"", list=0x5177d8) at src/zk_hashtable.c:274#5  0x0000000000403861 in process_completions (zh=0x515b60) at src/zookeeper.c:1631#6  0x000000000040e1b5 in do_completion (v=Variable ""v"" is not available.) at src/mt_adaptor.c:333#7  0x000000302b80610a in start_thread () from /lib64/tls/libpthread.so.0#8  0x000000302afc6003 in clone () from /lib64/tls/libc.so.6#9  0x0000000000000000 in ?? ()here, a deadlock presents.","ZOOKEEPER-591","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-18 09:45:01.927","Closed","The C Client cannot exit properly in some situation","Bug","2010-03-27 01:25:04.29",0,0,7827,8217
16528,"2009-11-23 10:47:54.828","In the comments of client C API which associated with creating znode, eg. zoo_acreate, it is said that the initial ACL of the node ""if null, the ACL of the parent will be used"". However, the it doesn't work. When execute this kind of request at the server side, it raises InvalidACLException. The source code show that, the function fixupACL return false when it get a null ACL. ","ZOOKEEPER-589","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-02-09 06:38:36.722","Closed","When create a znode, a NULL ACL parameter cannot be accepted","Bug","2010-03-27 01:25:04.242",0,0,7829,8217
16529,"2009-11-22 07:23:59.586","Why are we logging this? It's unnecessary and just annoying afaict. We should remove it entirely.2009-11-18 05:37:29,312 WARN org.apache.zookeeper.server.Request: Ignoring exception during toStringjava.nio.BufferUnderflowException	at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)	at java.nio.ByteBuffer.get(ByteBuffer.java:675)	at org.apache.zookeeper.server.Request.toString(Request.java:199)	at java.lang.String.valueOf(String.java:2827)	at java.lang.StringBuilder.append(StringBuilder.java:115)	at org.apache.zookeeper.server.quorum.CommitProcessor.processRequest(CommitProcessor.java:167)	at org.apache.zookeeper.server.quorum.FollowerRequestProcessor.run(FollowerRequestProcessor.java:68)","ZOOKEEPER-588","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-12 04:24:51.682","Closed","remove unnecessary/annoying log of tostring error in Request.toString()","Bug","2010-05-08 01:39:56.954",0,0,7818,7818
16530,"2009-11-22 07:21:29.076","The ZK client should log the timeout negotiated with the server if the time is different than the timeout parameter specified by the client.","ZOOKEEPER-587","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-12 05:46:49.171","Closed","client should log timeout negotiated with server","Bug","2010-05-08 01:40:07.587",0,0,7818,7818
16531,"2009-11-21 09:18:13.682","the c client fails to compile under cygwin","ZOOKEEPER-586","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-17 07:44:10.126","Closed","c client does not compile under cygwin","Bug","2010-03-27 01:25:04.01",0,0,7818,7818
16535,"2009-11-18 04:56:40.406","when zookeeper starts up it will restore the most recent state (latest zxid) it finds in the data directory. unfortunately, in the quorum version of zookeeper updates are logged using an epoch based on the latest log file in a directory. if there is a snapshot with a higher epoch than the log files, the zookeeper server will start logging using an epoch one higher than the highest log file.so if a data directory has a snapshot with an epoch of 27 and there are no log files, zookeeper will start logging changes using epoch 1. if the cluster restarts the state will be restored from the snapshot with the epoch of 27, which in effect, restores old data.normal operation of zookeeper will never result in this situation.this does not effect standalone zookeeper.a fix should make sure to use an epoch one higher than the current state, whether it comes from the snapshot or log, and should include a sanity check to make sure that a follower never connects to a leader that has a lower epoch than its own.","ZOOKEEPER-582","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-21 06:30:51.013","Closed","ZooKeeper can revert to old data when a snapshot is created outside of normal processing","Bug","2010-03-27 01:25:03.963",0,1,7827,7829
16541,"2009-11-12 02:08:30.506","the handling and implications of session moved exception should be documented.","ZOOKEEPER-576","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-21 03:47:49.897","Closed","docs need to be updated for session moved exception and how to handle it","Bug","2010-03-27 01:25:03.851",0,0,7829,7827
16543,"2009-11-11 08:43:34.137","I believe it's 100k, not 10k-----------------------snapCount(Java system property: zookeeper.snapCount)Clients can submit requests faster than ZooKeeper can process them, especially if there are a lot of clients. To prevent ZooKeeper from running out of memory due to queued requests, ZooKeeper will throttle clients so that there is no more than globalOutstandingLimit outstanding requests in the system. The default limit is 1,000.ZooKeeper logs transactions to a transaction log. After snapCount transactions are written to a log file a snapshot is started and a new transaction log file is started. The default snapCount is 10,000.","ZOOKEEPER-574","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-23 08:31:18.53","Closed","the documentation on snapcount in the admin guide has the wrong default","Bug","2010-03-27 01:25:03.805",0,0,7818,7818
16547,"2009-11-08 14:37:46.422","the asynchammertest is not validating the rc in the callback, more serious is that it is using path in the create callbackto delete the node, rather than name (which is important in the case of a sequential node creation as in this case)","ZOOKEEPER-570","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-09 06:46:18.25","Closed","AsyncHammerTest is broken, callbacks need to validate rc parameter","Bug","2010-03-27 01:25:03.667",0,0,7818,7818
16548,"2009-11-07 03:47:54.202","It is possible for basic LeaderElection to enter a situation where it never terminates. As an example, consider a three node cluster A, B and C.1. In the first round, A votes for A, B votes for B and C votes for C2. Since C > B > A, all nodes resolve to vote for C in the second round as there is no first round winner3. A, B vote for C, but C fails.4. C is not elected because neither A nor B hear from it, and so votes for it are discarded5. A and B never reset their votes, despite not hearing from C, so continue to vote for it ad infinitum. Step 5 is the bug. If A and B reset their votes to themselves in the case where the heard-from vote set is empty, leader election will continue.I do not know if this affects running ZK clusters, as it is possible that the out-of-band failure detection protocols may cause leader election to be restarted anyhow, but I've certainly seen this in tests. I have a trivial patch which fixes it, but it needs a test (and tests for race conditions are hard to write!)","ZOOKEEPER-569","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-02-20 22:27:43.351","Closed","Failure of elected leader can lead to never-ending leader election","Bug","2010-03-27 01:25:03.609",0,1,7886,7886
16549,"2009-11-03 09:38:35.407","Noticed the following issues in SyncRequestProcessor1) logCount is incremented even for non-log events (say getData)txnlog should return indication if request was logged or not (if hdr ==null it returns)also:2) move r.nextInt below logCount++ (ie if an actual log event)3) fix indentation after txnlog.append (for some reason has unnecessary 4 char indent)","ZOOKEEPER-568","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-07 16:08:51.589","Closed","SyncRequestProcessor snapping too frequently - counts non-log events as log events","Bug","2010-03-27 01:25:03.565",0,0,7818,7818
16550,"2009-11-01 12:41:05.35","the javadoc/cdoc for getchildren2 needs to mention that the methods are ""new in 3.3.0""","ZOOKEEPER-567","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-13 04:03:05.301","Closed","javadoc for getchildren2 needs to mention ""new in 3.3.0""","Bug","2010-03-27 01:25:03.526",0,0,7818,7818
16551,"2009-10-29 13:27:26.951","the four letter word ""reqs"" doesn't do anything - it always returns empty data. Seems that ""outstanding"" field is always empty and never set.we should remove outstanding and also update the reqs code to correctly output the outstanding requests (if not possible then remove the cmd and update docs - although this is very useful command, hate to see us lose it)","ZOOKEEPER-566","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-13 03:54:26.128","Closed","""reqs"" four letter word (command port) returns no information","Bug","2010-03-27 01:25:03.483",0,0,7818,7818
16554,"2009-10-29 01:09:27.189","With ZOOKEEPER-529 checked in, ant test for recipes broke. Its a minor change to the build for including librariries from the new location where jars are downloaded by ivy.","ZOOKEEPER-563","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-10-29 12:02:41.653","Closed","ant test for recipes is broken.","Bug","2010-03-27 01:25:03.355",0,0,7827,7827
16555,"2009-10-27 04:45:28.818","The c client can flood the server with pings if the tcp queue is filled.Say the cluster is overloaded and shuts down the recv processinga c client can send a ping, but since last_send is only updated on successful pushing of data into the socket, if flush_send_queue fails to send any data (send_buffer returns 0) then last_send is not updatedand zookeeper_interest will again send a ping the next time it is woken - which could be 0 if recv_to is closeto 0, easily could happen if server is not sending data to the client.","ZOOKEEPER-562","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-10-30 05:35:25.286","Closed","c client can flood server with pings if tcp send queue filled","Bug","2010-03-27 01:25:03.321",0,0,7829,7818
16559,"2009-10-25 08:08:18.545","the server and connection ""sent"" stat is not being updated. if you run ""stat"" on the client port the sent packets is much lower than it should beseems that sendbuffer is not updating the stats when it shortcircuits the send.","ZOOKEEPER-558","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-07 08:48:26.834","Closed","server ""sent"" stats not being updated","Bug","2010-03-27 01:25:02.68",0,0,7818,7818
16560,"2009-10-25 05:03:25.725","There are many cool release of hadoop zookeeper and this project is an apache project, as the maven project.But the released jars must be download manually and then deploy to a private repository before they can be used by developer using maven2.Please could you upload the zookeeper  jars on the public maven2 repository ?Of course, we can help to deploy those artifact if necessary.","ZOOKEEPER-557","Major","ZOOKEEPER","ZooKeeper","ASF","Duplicate","2009-11-07 05:24:26.289","Closed","Upload Zookeeper jars to a public maven repository","Bug","2010-03-27 01:25:02.638",1,2,NULL,8332
16563,"2009-10-15 06:55:06.725","C client returns NULL for stat object for deleted nodes. zookeeper.c blindly dereferences it. Segfault. ","ZOOKEEPER-554","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-10-20 03:43:10.973","Closed","zkpython can segfault when statting a deleted node","Bug","2010-03-27 01:25:02.531",0,0,7886,7886
16566,"2009-10-13 12:10:33.048","The java client is sending a SetWatches message even on a new session (always empty). Additionally SetWatches is calledeven in the case of re-establishing session, however no watches are set. The code should check for watches beforesending this (ie don't send empty setwatches). I see this on java, investigate c as well.","ZOOKEEPER-551","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-09 05:42:07.642","Closed","unnecessary SetWatches message on new session","Bug","2010-03-27 01:25:02.48",0,0,7818,7818
16570,"2009-10-09 06:01:41.532","We need to put some sanity checks in QuorumCnxnManager and the other quorum port for rogue clients. Sometimes a clients might get misconfigured and they might send random characters on such ports. We need to make sure that such rogue clients do not bring down the clients and need to put in some sanity checks with respect to packet lengths and deserialization.","ZOOKEEPER-547","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-18 07:39:20.373","Closed","Sanity check in QuorumCnxn Manager and quorum communication port.","Bug","2010-03-27 01:25:02.277",0,0,7827,7827
16575,"2009-10-06 09:54:02.151","Due to a mismatch between zookeeper_interest() and zookeeper_process(), when the zookeeper server is unresponsive the client can spin when reconnecting to the server.In particular, zookeeper_interest() adds ZOOKEEPER_WRITE whenever there is data to be sent, but flush_send_queue() only writes the data if the state is ZOO_CONNECTED_STATE.  When in ZOO_ASSOCIATING_STATE, this results in spinning.This probably doesn't affect production, but I had a runaway process in a development deployment that caused performance issues on the node.  This is easy to reproduce in a single node environment by doing a kill -STOP on the server and waiting for the session timeout.Patch to be added.","ZOOKEEPER-542","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-10-08 02:11:37.163","Closed","c-client can spin when server unresponsive","Bug","2010-03-27 01:25:02.036",0,0,744,744
16576,"2009-10-06 08:04:47.423","zkpython is currently limited to a max of 256 total handles - not 256 open handles, but rather 256 total handles createdover the lifetime of the python application.In general this isn't a real issue, however in the case of a long lived application which polls the cluster periodically (closingthe session btw calls) this is an issue.it would be great if the slots could be reused? or perhaps a more complex structure, such as a linked list, which would allowdynamic growth/shrinkage of the handle list.Also see ZOOKEEPER-540","ZOOKEEPER-541","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-10-14 07:14:58.861","Closed","zkpython limited to 256 handles","Bug","2010-03-27 01:25:01.988",0,0,7886,7818
16577,"2009-10-02 06:02:41.568","I was getting a python segfault in one of my scripts. Turns out I was closing a session handle and then reusing it (async call). This was causing python to segfault.zkpython should track handle state and complain, rather than crash, if the handle is invalid (closed).","ZOOKEEPER-540","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-10-09 08:43:13.328","Closed","zkpython needs better tracking of handle validity","Bug","2010-03-27 01:25:01.959",0,0,7886,7818
16579,"2009-09-30 14:00:21.552","Henry, can you take a look at this, am I doing it right?calling         zookeeper.async(self.handle, path)causes python to segfault.see: http://github.com/phunt/zk-smoketest/blob/master/zk-smoketest.py","ZOOKEEPER-538","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-10-02 08:26:42.428","Closed","zookeeper.async causes python to segfault","Bug","2010-03-27 01:25:01.8",0,1,7886,7818
16580,"2009-09-26 06:25:43.547","This is a problem if you use zookeeper as a dependency in maven because for whatever reason the maven compiler plugin will pick up the java files in the jar and compile them to the output directory. From there they will land in the generated jar file for whatever project happens to depend on zookeeper thus introducing duplicate classes (once in zookeeper.jar, once in the project's artifact).","ZOOKEEPER-537","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-07 05:20:40.635","Closed","The zookeeper jar includes the java source files","Bug","2010-03-27 01:25:01.735",1,1,8336,8336
16582,"2009-09-25 01:56:42.616","Ant resolves the same dependencies multiple times if multiple targets are run on the command line:""ant b c"", where b and c both depend on a, results in a being executed twice. However if you have atarget d which depends on both b and c, ""ant d"" will only result in a being executed once.say ""ant jar compile-test"" is run, this will currently fail as ivy-init is run twice, resulting in the taskdef failing.Rather we need a guard on the ivy-init target itself to ensure it isn't run twice.ie: put an unless attrib on ivy-init, then set that property in the body of the target","ZOOKEEPER-535","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-09-25 02:52:50.878","Closed","ivy task does not enjoy being defined twice (build error)","Bug","2010-03-27 01:25:01.672",0,0,7818,7818
16583,"2009-09-24 02:46:40.469","The test target in contib/bookkeeper does not depend on jar target. So the ant test target gives compilation errors if the main is not compiled which can be prevented if it depends on jar. It can then check if main has been compiled or not and throw out a reasonable error.","ZOOKEEPER-534","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-12-17 07:46:55.637","Closed","The test target in contib/bookkeeper does not depend on jar target.","Bug","2013-05-02 10:29:22.126",0,0,7818,7827
16584,"2009-09-24 01:41:41.689","if clean is run twice in a row (ie already clean, then run clean) an error is generated.","ZOOKEEPER-533","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-09-24 05:54:20.424","Closed","ant error running clean twice","Bug","2010-03-27 01:25:01.454",0,0,7818,7818
16585,"2009-09-23 04:09:46.334","The jars released in 3.2.1 will not run on Java 1.5.  With a small build change, it is possible to generate jars that will run on Java 1.5.","ZOOKEEPER-532","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-19 01:30:38.174","Closed","java compiler should be target Java 1.5","Bug","2010-03-27 01:25:01.409",0,0,8337,8337
16586,"2009-09-22 01:09:13.81","Flavio, can you take a look, this is very unusual. This test seems to be failing due to interrupt being received and not handled properly:http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/470/testReport/org.apache.zookeeper.test/HierarchicalQuorumTest/testHierarchicalQuorum/I don't know why the interrupt would be received though...here's an example:2009-09-21 10:46:47,681 - WARN  [Thread-8:QuorumCnxManager$SendWorker@539] - Interrupted while waiting for message on queuejava.lang.InterruptedException	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1934)	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:317)	at org.apache.zookeeper.server.quorum.QuorumCnxManager$SendWorker.run(QuorumCnxManager.java:533)","ZOOKEEPER-531","Blocker","ZOOKEEPER","ZooKeeper","ASF","Cannot Reproduce","2010-01-13 04:12:06.394","Closed","Hudson trunk failure in heirarchical quorum test (interrupt problem)","Bug","2010-03-27 01:25:00.911",0,0,7750,7818
16587,"2009-09-21 17:17:08.927","I tried to run zookeeper c-client on a machine with IPv6 enabled. When connecting to the IPv6 address a connect(...) gave a ""Address family not supported by protocol"" error. The reason was, that a few lines earlier, the socket was opened with PF_INET instead of PF_INET6. Changing that the following way:{code}           if (zh->addrs[zh->connect_index].sa_family == AF_INET) {            	zh->fd = socket(PF_INET, SOCK_STREAM, 0);            } else {            	zh->fd = socket(PF_INET6, SOCK_STREAM, 0);            }{code}turned the error message into ""Invalid argument"". When printing out sizeof(struct sockaddr), sizeof(struct sockaddr_in) and sizeof(struct sockaddr_in6) I got sockaddr: 16, sockaddr_in: 16 and sockaddr_in6: 28. So in the code calling {code}           connect(zh->fd, &zh->addrs[zh->connect_index], sizeof(struct sockaddr_in));{code}the parameter address_len is too small.Same applies to how IPv6 addresses are handled in the function getaddrs(zhandle_t *zh).(Big Thanks+kiss to Thilo Fromm for helping me debug this.)","ZOOKEEPER-530","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-10-22 05:01:11.669","Closed","Memory corruption: Zookeeper c client IPv6 implementation does not honor struct sockaddr_in6 size","Bug","2010-03-27 01:25:00.858",0,1,8339,8339
16589,"2009-09-17 02:46:54.068","If I create 100k nodes on /misc then      CPPUNIT_ASSERT_EQUAL(0, zoo_get_children(zh2, ""/misc"", 0, &children));      for (int i = 0; i < children.count; i++) {        sprintf(path, ""/misc/%s"", children.data[i]);        CPPUNIT_ASSERT_EQUAL(0, zoo_exists(zh2, path, 1, &stat));        CPPUNIT_ASSERT_EQUAL(0, zoo_wexists(zh3, path, watcher, &ctx3, &stat));      }around 47k or so through the loop the client fails with -4 (connection loss), the client timeout is 30 seconds. The server command port shows the following, so it looks like it's not the server but some issue with watcher reg on the c client?phunt@valhalla:~$ echo stat | nc localhost 22181Zookeeper version: 3.3.0--1, built on 07/22/2009 23:55 GMTClients: /127.0.0.1:45729[1](queued=0,recved=100024,sent=0) /127.0.0.1:50229[1](queued=0,recved=0,sent=0) /127.0.0.1:45731[1](queued=0,recved=47116,sent=0) /127.0.0.1:45730[1](queued=0,recved=47117,sent=1)Latency min/avg/max: 0/196/1026Received: 194257Sent: 1Outstanding: 0Zxid: 0x186a4Mode: standaloneNode count: 100005729 is a separate client - the one that created the nodes originally.731 and 730 are zh2/zh3 in the code.","ZOOKEEPER-528","Critical","ZOOKEEPER","ZooKeeper","ASF","Invalid","2009-10-15 01:27:20.785","Closed","c client exists() call with watch on large number of nodes (>100k) causes connection loss","Bug","2010-03-27 01:25:00.758",0,0,7818,7818
16590,"2009-09-17 01:34:23.101","the following test failed:http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/462/testReport/org.apache.zookeeper.test/QuorumQuotaTest/testQuotaWithQuorum/here's the interesting log:2009-09-16 10:35:52,728 - WARN  [CommitProcessor:1:DataTree@409] - Quota exceeded: /a bytes=1808 limit=10002009-09-16 10:36:34,000 - INFO  [SessionTracker:SessionTrackerImpl@133] - Expiring session 0x423c26c1d2200002009-09-16 10:36:12,725 - WARN  [main-SendThread(localhost:11225):ClientCnxn$SendThread@969] - Exception closing session 0x423c26c1d220000 to sun.nio.ch.SelectionKeyImpl@a7dd39java.io.IOException: TIMED OUT	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:908)looks like a create call hung and the session eventually expiredperhaps updating the quota had some issue? deadlock or ...","ZOOKEEPER-527","Critical","ZOOKEEPER","ZooKeeper","ASF","Cannot Reproduce","2009-12-08 05:13:00.75","Closed","hudson trunk failure in  quota test","Bug","2010-03-27 01:25:00.701",0,0,7827,7818
16614,"2009-08-07 09:06:12.307","there is a race condition between the zookeeper completion thread and the bookeeper processing queue during create. if the zookeeper completion thread falls behind due to scheduling, the action counter of the create operation may go backwards.","ZOOKEEPER-503","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-27 07:18:04.086","Closed","race condition in asynchronous create","Bug","2013-05-02 10:29:21.209",0,1,7829,7829
16593,"2009-09-11 07:36:16.869","DBSizeTest looks like it should be testing latency, but it doesn't seem to do it (assert is commented out).We need to decide if this test should be fixed, or just dropped.Also note: this test takes 40seconds on my system. Way too long. Perhaps async create operations should be usedto populate the database. I also noticed that data size has a big impact on overall test time (1k vs 5 bytes is somethinglike a 2x time diff for time to run the test).","ZOOKEEPER-524","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-02-20 09:21:54.126","Closed","DBSizeTest is not really testing anything","Bug","2010-03-27 01:25:00.668",0,0,7829,7818
16598,"2009-08-26 07:49:16.387","We noticed this in our tests -{code}java.net.SocketException: Broken pipe        at java.net.SocketOutputStream.socketWrite0(Native Method)        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)        at java.io.DataOutputStream.write(DataOutputStream.java:90)        at java.io.FilterOutputStream.write(FilterOutputStream.java:80)        at org.apache.jute.BinaryOutputArchive.writeBuffer(BinaryOutputArchive.java:122)        at org.apache.zookeeper.server.DataNode.serialize(DataNode.java:126)        at org.apache.jute.BinaryOutputArchive.writeRecord(BinaryOutputArchive.java:126)        at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:878)        at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:890)        at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:890)        at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:890)        at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:890)        at org.apache.zookeeper.server.DataTree.serializeNode(DataTree.java:890)        at org.apache.zookeeper.server.DataTree.serialize(DataTree.java:940)        at org.apache.zookeeper.server.util.SerializeUtils.serializeSnapshot(SerializeUtils.java:102)        at org.apache.zookeeper.server.ZooKeeperServer.serializeSnapshot(ZooKeeperServer.java:269)        at org.apache.zookeeper.server.quorum.FollowerHandler.run(FollowerHandler.java:263){code}So the followerhandler got an exception while writing to the socket but the follower was still waiting on the socket for a read and got a read timeout after 60 seconds or so. To just make sure we handle this rightly, we should close the socket at the followerhandler when we get an excpetion, so that the follower immediately recognizes that its disconnected from the leader.","ZOOKEEPER-519","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-19 01:45:10.415","Closed","Followerhandler should close the socket if it gets an exception on a write.","Bug","2010-03-27 01:25:00.536",0,0,7827,7827
16603,"2009-08-25 01:44:28.577","http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/424/testReport/org.apache.zookeeper.test/CnxManagerTest/testCnxManager/Flavio can you take a look?","ZOOKEEPER-514","Major","ZOOKEEPER","ZooKeeper","ASF","Won't Fix","2009-08-25 03:04:11.131","Closed","test failure on trunk in testCnxManager - NPE","Bug","2009-09-06 06:36:24.128",0,0,NULL,7818
16605,"2009-08-20 08:11:07.541","I was doing some fault injection testing of 3.2.1 with ZOOKEEPER-508 patch applied and noticed that after some time the ensemble failed to re-elect a leader.See the attached log files - 5 member ensemble. typically 5 is the leaderNotice that after 16:23:50,525 no quorum is formed, even after 20 minutes elapses w/no quorumenvironment:I was doing fault injection testing using aspectj. The faults are injected into socketchannel read/write, I throw exceptions randomly at a 1/200 ratio (rand.nextFloat() <= .005 => throw IOExceptionYou can see when a fault is injected in the log via:2009-08-19 16:57:09,568 - INFO  [Thread-74:ReadRequestFailsIntermittently@38] - READPACKET FORCED FAILvs a read/write that didn't force fail:2009-08-19 16:57:09,568 - INFO  [Thread-74:ReadRequestFailsIntermittently@41] - READPACKET OKotw standard code/config (straight fle quorum with 5 members)also see the attached jstack trace. this is for one of the servers. Notice in particular that the number of sendworkers != the number of recv workers.","ZOOKEEPER-512","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-10-28 05:14:23.923","Closed","FLE election fails to elect leader","Bug","2010-03-27 01:25:00.406",0,0,7750,7818
16606,"2009-08-20 06:35:23.035","in FollowerHandler if sendPackets gets an ioexception on writeRecord the send thread will exit, however the socket isn't necessarily closed.2009-08-19 15:28:46,869 - WARN  [Sender-/127.0.0.1:58179:FollowerHandler@131] - Unexpected exception	at org.apache.zookeeper.server.quorum.FollowerHandler.sendPackets(FollowerHandler.java:128)	at org.apache.zookeeper.server.quorum.FollowerHandler.access$0(FollowerHandler.java:107)	at org.apache.zookeeper.server.quorum.FollowerHandler$1.run(FollowerHandler.java:325)This results in the follower taking a very long time to recover and rejoin the quorum.","ZOOKEEPER-511","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-11 22:06:33.095","Closed","bad error handling in FollowerHandler.sendPackets","Bug","2010-03-27 01:25:00.34",0,0,7827,7818
16607,"2009-08-20 05:13:42.756","The current zkpython bindings always throw ""IOError(""text"")"" exceptions, even for ZK specific exceptions such as NODEEXISTS. This makes it difficult (error prone) to handle exceptions in python code. You can't easily pickup a connection loss vs a node exists for example. Of course you could match the error string, but this seems like a bad idea imo.We need to add specific exception types to the python binding that map directly to KeeperException/java types. It would also be useful to include the information provided by the KeeperException (like path in some cases), etc... as part of the error thrown to the python code. Would probably be a good idea to stay as close to java api as possible wrt mapping the errors.","ZOOKEEPER-510","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-10-14 08:14:27.077","Closed","zkpython lumps all exceptions as IOError, needs specialized exceptions for KeeperException types","Bug","2010-03-27 01:25:00.299",0,0,7886,7818
16608,"2009-08-20 01:41:14.865","The truncating of logs does not work right because we use BufferedInputStream and the truncation happens at the buffered read which is greater than the actual truncate position.","ZOOKEEPER-509","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-25 13:57:32.629","Closed","Truncating of logs does not work right at the followers.","Bug","2009-09-06 06:36:20.856",0,0,7829,7827
16609,"2009-08-18 02:39:08.372","The proposals and commits sent by the leader after it asks the followers to truncate there logs or starts sending a diff has missing messages which causes out of order commits messages and causes the followers to shutdown because of these out of order commits.","ZOOKEEPER-508","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-25 13:56:53.356","Closed","proposals and commits for DIFF and Truncate messages from the leader to followers is buggy.","Bug","2009-09-06 06:36:20.814",0,0,7827,7827
16612,"2009-08-09 04:57:35.547","The test case testAsyncCreateClose is badly broken. I was wondering why all the unit tests are passing inspite of having found so many different problems with LedgerManagementProcessor. There is a big try-catch block sitting in the test case that catches all exception, prints their stack trace, and exits, thereby allowing the test to pass. In general, unit tests shouldnt catch exceptions unless it is something you are expecting that will happen.Another problem is that the same ControlObject is used for synchronization throughout. Since we already have the problem of callbacks being called multiple times (ZOOKEEPER-502), notify() on the control object is called too many times, resulting in the unit test not waiting for certain callbacks.Thus the test never waits for the asyncOpenLedger() to finish, and hence still succeeds. I believe asyncOpenLedger() has never worked right. ","ZOOKEEPER-505","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-27 07:17:08.221","Closed","testAsyncCreateClose is badly broken","Bug","2010-03-27 01:25:00.115",0,0,8343,8343
16613,"2009-08-08 01:35:50.307","java.lang.ClassCastException: org.apache.bookkeeper.client.LedgerManagementProcessor$OpenLedgerOp cannot be cast to org.apache.bookkeeper.client.LedgerManagementProcessor$CloseLedgerOp	at org.apache.bookkeeper.client.LedgerManagementProcessor.processResult(LedgerManagementProcessor.java:1083)This seems to be happening because its a nested switch case statement. And the OPEN: case, doesn't ever call a break. It only calls a break from the inner switch-case and hence falls through into the CLOSE: case.","ZOOKEEPER-504","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-27 07:17:35.752","Closed","ClassCastException in LedgerManagementProcessor","Bug","2010-03-27 01:24:59.939",0,0,8343,8343
16618,"2009-08-06 04:32:20.565","there's a regression in 3.2 - electionAlg is no longer defaulting to 3 (incorrectly defaults to 0)also - need to have tests to validate this","ZOOKEEPER-499","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-11 06:25:56.646","Closed","electionAlg should default to FLE (3) - regression","Bug","2009-09-06 06:36:20.663",0,0,7818,7818
16619,"2009-08-05 05:18:51.605","In a WAN configuration, ZooKeeper is endlessly electing, terminating, and re-electing a ZooKeeper leader. The WAN configuration involves two groups, a central DC group of ZK servers that have a voting weight = 1, and a group of servers in remote pods with a voting weight of 0.What we expect to see is leaders elected only in the DC, and the pods to contain only followers. What we are seeing is a continuous cycling of leaders. We have seen this consistently with 3.2.0, 3.2.0 + recommended patches (473, 479, 481, 491), and now release 3.2.1.","ZOOKEEPER-498","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-12 05:27:36.545","Closed","Unending Leader Elections : WAN configuration","Bug","2009-09-06 06:36:20.601",0,0,7750,8344
16622,"2009-08-04 02:20:00.198","The C client logs this error message when zookeeper_init is called with chroot. 2009-08-03 18:14:29,130:6624(0x5e66e950):ZOO_ERROR@sub_string@730: server path  does not include chroot path /chrootI'll attach a simple program to reproduce this.Thanks!--Michi","ZOOKEEPER-495","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-26 11:11:29.677","Closed","c client logs an invalid error when zookeeper_init is called with chroot","Bug","2010-05-08 01:40:33.54",0,0,7818,7800
16624,"2009-08-01 17:40:58.787","the command line ""setquota"" tries to use argument 3 as both a path and a value","ZOOKEEPER-493","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-05 05:39:21.749","Closed","patch for command line setquota ","Bug","2009-09-06 06:36:20.521",0,0,8345,8345
16626,"2009-07-30 02:51:10.921","This is a fix to prevent zero-weight servers from being elected leaders. This will allow in wide-area scenarios to restrict the set of servers that can lead the ensemble.","ZOOKEEPER-491","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-05 02:17:48.772","Closed","Prevent zero-weight servers from being elected","Bug","2013-05-02 10:29:26.972",0,0,7750,7750
16627,"2009-07-30 02:27:29.243","the javadoc for ZooKeeper constructor says:     * The client object will pick an arbitrary server and try to connect to it.     * If failed, it will try the next one in the list, until a connection is     * established, or all the servers have been tried.the ""or all server tried"" phrase is misleading, it should indicate that we retry until success, con closed, or session expired. we also need ot mention that connection is async, that constructor returns immed and you need to look for connection event in watcher","ZOOKEEPER-490","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-08 02:17:54.909","Closed","the java docs for session creation are misleading/incomplete","Bug","2009-09-06 06:36:20.441",0,0,7818,7818
16628,"2009-07-28 14:40:27.767","for some reason hudson.zones violations report has jumped up to 2000+ findbugs issues in just the last couple of weeks. It'sjumped up from 0 violations so something is weird...http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/Giri can you take a look?","ZOOKEEPER-489","Minor","ZOOKEEPER","ZooKeeper","ASF","Cannot Reproduce","2009-09-29 17:59:38.121","Closed","findbugs on hudson.zones.a.o has jumped way up (2k+)","Bug","2010-03-27 01:24:55.969",0,0,8063,7818
16630,"2009-07-25 04:38:00.747","setdata on root ""/"" crashes the servers with the followimg exception. Unfortunately we never had a setdata test on root. The following is the exception. This happens with 3.1.1 as well. We might want to consider releasing 3.1.2 just for this jira.{code}java.lang.IllegalArgumentException: Invalid path /        at org.apache.zookeeper.common.PathTrie.findMaxPrefix(PathTrie.java:255)        at org.apache.zookeeper.server.DataTree.setData(DataTree.java:543)        at org.apache.zookeeper.server.DataTree.processTxn(DataTree.java:701)        at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:94)        at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:127)        at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:75){code}","ZOOKEEPER-487","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-01 03:08:57.458","Closed","setdata on root (/) crashes the servers.","Bug","2009-09-06 06:36:20.359",0,0,7827,7827
16632,"2009-07-24 05:47:03.446","We need ops documentation detailing what to do if the ZK server VM fails - by fail I mean the jvm processexits/dies/crashes/etc...In general a supervisor process should be used to start/stop/restart/etc... the ZK server vm.Something like daemontools http://cr.yp.to/daemontools.html could be used, or more simply a wrapper scriptshould monitor the status of the pid and restart if the jvm fails. It's up to the operator, if this is not doneautomatically then it will have to be done manually, by operator restarting the ZK server jvmThe inherent behavior of ZK wrt to failures - ie that it automatically recovers as long as quorum is maintained - fits into this nicely.","ZOOKEEPER-485","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-02-25 04:04:26.455","Closed","need ops documentation that details supervision of ZK server processes","Bug","2010-03-27 01:24:55.794",0,1,7818,7818
16633,"2009-07-24 05:09:54.34","When a client is connected to follower and get disconnected and connects to a leader it gets SESSION MOVED excpetion. This is beacuse of a bug in the new feature of ZOOKEEPER-417 that we added in 3.2. All the releases before 3.2 DO NOT have this problem. The fix is to make sure the ownership of a connection gets changed when a session moves from follower to the leader. The workaround to it in 3.2.0 would be to swithc off connection from clients to the leader. take a look at *leaderServers* java property in http://hadoop.apache.org/zookeeper/docs/r3.2.0/zookeeperAdmin.html.","ZOOKEEPER-484","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-07 08:39:36.438","Closed","Clients get SESSION MOVED exception when switching from follower to a leader.","Bug","2009-09-06 06:36:20.326",0,1,7827,7827
16634,"2009-07-24 04:40:49.653","here are the part of the log whereby my zookeeper instance crashed, taking 3 out of 5 down, and thus ruining the quorum for all clients:2009-07-23 12:29:06,769 WARN org.apache.zookeeper.server.NIOServerCnxn: Exception causing close of session 0x52276d1d5161350 due to java.io.IOException: Read error2009-07-23 12:29:00,756 WARN org.apache.zookeeper.server.quorum.Follower: Exception when following the leaderjava.io.EOFException        at java.io.DataInputStream.readInt(DataInputStream.java:375)        at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)        at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:65)        at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)        at org.apache.zookeeper.server.quorum.Follower.readPacket(Follower.java:114)        at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:243)        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:494)2009-07-23 12:29:06,770 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5161350 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.168:39489]2009-07-23 12:29:06,770 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb0578 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.159:46797]2009-07-23 12:29:06,771 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa013e NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.153:33998]2009-07-23 12:29:06,771 WARN org.apache.zookeeper.server.NIOServerCnxn: Exception causing close of session 0x52276d1d5160593 due to java.io.IOException: Read error2009-07-23 12:29:06,808 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e02bb NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.158:53758]2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa13e4 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.154:58681]2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691382 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59967]2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb1354 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.163:49957]2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa13cd NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.150:34212]2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691383 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.159:46813]2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb0350 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59956]2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e139b NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.156:55138]2009-07-23 12:29:06,809 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e1398 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.167:41257]2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5161355 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.153:34032]2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d516011c NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.155:56314]2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e056b NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.155:56322]2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d516011f NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.157:49618]2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e11ea NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.10.20.42:55483]2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e02ba NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.157:49632]2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb1355 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.169:58824]2009-07-23 12:29:06,810 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691378 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.161:40973]2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691380 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59944]2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e0311 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.160:56167]2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e690374 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.169:58815]2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e139f NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.151:51396]2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e139c NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.155:56315]2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e69137b NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59859]2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5160594 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.151:51370]2009-07-23 12:29:06,811 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e69137a NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.159:46682]2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5160347 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.165:35722]2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e69137f NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.159:46754]2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5160121 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.155:56307]2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb0126 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.154:58688]2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa05fc NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.152:45067]2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e0316 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.169:58800]2009-07-23 12:29:06,812 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e69137e NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.159:46737]2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e69137d NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.159:46733]2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa13df NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.156:55137]2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb134e NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.166:40443]2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691381 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.161:41086]2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5161356 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.165:35719]2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb1349 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.158:53770]2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x12276d15dfb0352 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.165:35718]2009-07-23 12:29:06,813 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691379 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59823]2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d516000e NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.150:34216]2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x32276d15d2e1397 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.169:58829]2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e69137c NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59862]2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa0140 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.155:56271]2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x42276d1d3fa13e1 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.157:49608]2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x22276d15e691377 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.162:59789]2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x52276d1d5160593 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.165:35703]2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.FinalRequestProcessor: shutdown of request processor complete2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.quorum.FollowerRequestProcessor: FollowerRequestProcessor exited loop!2009-07-23 12:29:06,814 INFO org.apache.zookeeper.server.quorum.CommitProcessor: CommitProcessor exited loop!2009-07-23 12:29:06,815 INFO org.apache.zookeeper.server.quorum.Follower: shutdown calledjava.lang.Exception: shutdown Follower        at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:427)        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:498)2009-07-23 12:29:06,815 WARN org.apache.zookeeper.server.NIOServerCnxn: Ignoring exceptionjava.nio.channels.CancelledKeyException        at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)        at sun.nio.ch.SelectionKeyImpl.readyOps(SelectionKeyImpl.java:69)        at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:201)2009-07-23 12:29:06,815 INFO org.apache.zookeeper.server.quorum.QuorumPeer: LOOKING2009-07-23 12:29:06,817 WARN org.apache.zookeeper.server.NIOServerCnxn: Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running2009-07-23 12:29:06,817 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x0 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.156:55206]2009-07-23 12:29:06,818 WARN org.apache.zookeeper.server.NIOServerCnxn: Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running2009-07-23 12:29:06,818 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x0 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.155:56331][elided lots of the same]2009-07-23 12:29:33,008 INFO org.apache.zookeeper.server.NIOServerCnxn: closing session:0x0 NIOServerCnxn: java.nio.channels.SocketChannel[connected local=/10.20.20.151:2181 remote=/10.20.20.152:59458]2009-07-23 12:29:33,011 FATAL org.apache.zookeeper.server.SyncRequestProcessor: Severe unrecoverable error, exitingjava.net.SocketException: Socket closed        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:99)        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)        at org.apache.zookeeper.server.quorum.Follower.writePacket(Follower.java:100)        at org.apache.zookeeper.server.quorum.SendAckRequestProcessor.flush(SendAckRequestProcessor.java:52)        at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:131)        at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:76)The good news is when I restarted the downed zookeepers, everything returned to normal.","ZOOKEEPER-483","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-25 13:58:27.204","Closed","ZK fataled on me, and ugly","Bug","2009-09-06 06:36:20.202",0,2,7829,8347
16635,"2009-07-23 06:44:53.929","The testRetry test silently exits for me periodically, especially, it seems, on newer hardware.  It also spits out from log messages clutter the test output.The silent exits turn out to be because SIGPIPE is sometimes delivered during the sleep(1) in createClient(), the second time createClient() is called.  Since SIGPIPE is not being ignored and there is no signal handler, the process exists immediately.  This leaves the test suite in a broken state, with the test ZooKeeper process still running because ""zkServer.sh stop"" is not run by tearDown().  You have to manually kill the ZK server and retry the tests; sometimes they succeed and sometimes they don't.I described SIGPIPE handling a little in ZOOKEEPER-320.  The appropriate thing, I think, is for the client application to ignore or handle SIGPIPE.  In this case, that falls to the test processes.  The attached patch fixes the issue for me with testRetry.The patch uses sigaction() to ignore SIGPIPE in TestClientRetry.cc and, for good measure (although I never saw it actually fail for me), TestClient.cc, since that file also uses sleep() extensively.I also removed a couple of unused functions and a macro definition from TestClientRetry.cc, just to simply matters, and turned off log output, which makes the testRetry output much, much cleaner (otherwise you get a lot of log output spamming into the nice clean cppunit output :-).","ZOOKEEPER-482","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-07-31 06:34:09.641","Closed","ignore sigpipe in testRetry to avoid silent immediate failure","Bug","2009-09-06 06:36:20.09",0,0,8348,8348
16636,"2009-07-20 21:43:29.024","Currently we rely on TCP for reliable delivery of FLE messages. However, as we concurrently drop and create new connections, it is possible that a message is sent but never received. With this patch, cnx manager keeps a list of last messages sent, and resends the last one sent. Receiving multiples copies is harmless. ","ZOOKEEPER-481","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-04 05:32:33.955","Closed","Add lastMessageSent to QuorumCnxManager","Bug","2013-05-02 10:29:26.976",0,0,7750,7750
16637,"2009-07-20 21:33:29.116","As a server may join leader election while others have already elected a leader, it is necessary that a server handles some special cases of leader election when notifications are from servers that are either LEADING or FOLLOWING. In such special cases, we check if we have received a message from the leader to declare a leader elected. This check does not consider the case that the process performing the check might be a recently elected leader, and consequently the check fails.This patch also adds a new case, which corresponds to adding a vote to recvset when the notification is from a process LEADING or FOLLOWING. This fixes the case raised in ZOOKEEPER-475.","ZOOKEEPER-480","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-05 02:07:54.375","Closed","FLE should perform leader check when node is not leading and add vote of follower","Bug","2013-05-02 10:29:26.724",0,0,7750,7750
16638,"2009-07-20 21:00:17.084","QuorumHierarchical::containsQuorum should not verify if all groups represented in the input set have more than half of the total weight. Instead, it should check only for an overall majority of groups. ","ZOOKEEPER-479","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-04 06:30:03.796","Closed","QuorumHierarchical does not count groups correctly","Bug","2013-05-02 10:29:26.953",0,0,7750,7750
16640,"2009-07-18 03:05:17.098","the zkCleanup.sh script is buggy in two ways:1) it doesn't actually pass through the snapshot count, so it doesn't work2) it assumes that there is only dataDir, it doesn't support dataLogDirAnd it can use cleanup, so that it doesn't blindly call eval from the config file..","ZOOKEEPER-477","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-11 08:30:22.172","Closed","zkCleanup.sh is flaky","Bug","2009-09-06 06:36:19.876",0,0,8349,8349
16642,"2009-07-17 05:39:26.859","THe flenewepochtest failed on one of the nightly builds -http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/377.","ZOOKEEPER-475","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-05 02:18:50.494","Closed","FLENewEpochTest failed on nightly builds.","Bug","2013-05-02 10:29:26.742",0,0,7750,7827
16647,"2009-07-15 02:25:45.671","Include unistd.h for sleep() calls in C tests to ensure successful compilation on some platforms.","ZOOKEEPER-470","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-07-21 06:55:48.866","Closed","include unistd.h for sleep() in c tests","Bug","2009-09-06 06:36:19.706",0,0,8348,8348
16648,"2009-07-15 02:10:28.889","When compiling with --with-cppunit-prefix, CPPUNIT_CFLAGS is set by the AM_PATH_CPPUNIT macro.  In configure.ac, it is then reset in order to set the -DZKSERVER_CMD command line argument.  Instead, that argument should be added to CPPUNIT_CFLAGS so that things like a custom -I include location set by AM_PATH_CPPUNIT are not lost.  Otherwise, a custom cppunit installation is not properly supported, despite the --with-cppunit-prefix option.","ZOOKEEPER-469","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-07-17 07:30:08.437","Closed","make sure CPPUNIT_CFLAGS isn't overwritten","Bug","2009-09-06 06:36:19.63",0,0,8348,8348
16649,"2009-07-15 01:44:34.179","Older compilers may complain that rc may be used without initialization in send_auth_info(), if -Wall is specified.  The fix is a simple initialization.","ZOOKEEPER-468","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-07-17 01:42:28.213","Closed","avoid compile warning in send_auth_info()","Bug","2009-09-06 06:36:19.564",0,0,8348,8348
16650,"2009-07-14 15:47:19.2","I have accidentally left a log.warn in BookieHandle. This is a pretty simple patch, so I would appreciate if we could review it quickly.","ZOOKEEPER-467","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-07-31 05:54:43.72","Closed","Change log level in BookieHandle","Bug","2009-09-06 06:36:19.491",0,0,7750,7750
16651,"2009-07-14 11:25:12.925","The free_auth_info() function calls deallocate_Buffer(&auth->auth) on every element in the auth list; that function frees any memory pointed to by auth->auth.buff if that field is non-NULL.In zoo_add_auth(), when certLen is zero (or cert is NULL), auth.buff is set to 0, but then not assigned to authinfo->auth when auth.buff is NULL.  The result is uninitialized data in auth->auth.buff in free_auth_info(), and potential crashes.The attached patch adds a test which attempts to duplicate this error; it works for me but may not always on all systems as it depends on the uninitialized data being non-zero; there's not really a simple way I can see to trigger this in the current test framework.  The patch also fixes the problem, I believe.","ZOOKEEPER-466","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-04 09:59:43.957","Closed","crash on zookeeper_close() when using auth with empty cert","Bug","2013-05-02 10:29:26.344",0,0,8348,8348
16656,"2009-07-10 00:39:28.078","the followng code failed on hudsonhttp://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/371/      watchctx_t ctx1, ctx2;      zhandle_t *zk1 = createClient(&ctx1);      CPPUNIT_ASSERT_EQUAL(true, ctx1.waitForConnected(zk1));      zhandle_t *zk2 = createClient(&ctx2);      zookeeper_close(zk1);      CPPUNIT_ASSERT_EQUAL(true, ctx2.waitForConnected(zk2));there's a problem with this test, it assumes that close(1) can be called before createclient(2) gets connected.this is not correct: createclient is an async call an in some cases the connection can be established beforecreate client returns.this shows a failure in this case because client1 was created, then client2 attempted to connectbut failed due to this on the server (max conn exceeded):        sprintf(cmd, ""export ZKMAXCNXNS=1;%s startClean %s"", ZKSERVER_CMD, getHostPorts());conn 2 failed and therefore the following assert eventually failed.this code should not assume that close(1) will beat connect(2)Henry can you take a look?","ZOOKEEPER-460","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-04 05:35:05.603","Closed","bad testRetry in cppunit tests (hudson failure)","Bug","2009-09-06 06:36:19.327",0,0,7827,7818
16659,"2009-07-02 02:58:04.938","hi folks, we have made some changes to zookeeper to facilitate providing an embedded zk client in our own hbase client.  This will allow our users to use 1 shell to manipulate both hbase things and zookeeper things.  It requires making a few things public, and in the process rearranging how some static things are initialized.  It's fairly trivial refactoring, hopefully you guys approve!Thanks!","ZOOKEEPER-457","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-01 04:32:59.844","Closed","Make ZookeeperMain public, support for HBase (and other) embedded clients","Bug","2009-09-06 06:36:19.256",0,2,8347,8347
16661,"2009-07-01 07:44:02.115","the zookeeper c client crashes with chroot specified in the string. This does not fail on 2.6 but on ubuntu where the malloced memory is nto initialized to \0.","ZOOKEEPER-455","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-07-02 00:39:51.884","Closed","zookeeper c client crashes with chroot specified in the string.","Bug","2009-07-09 04:24:06.304",0,0,7827,7827
16662,"2009-07-01 04:53:41.678","we no longer officially support jdk1.5 however it still compiles -- except for a recent @override that was added for an interface method. jdk1.6allows this but jdk1.5 does not (must be superclass). small patch will address this. 3.3 we will drop 1.5 concerns entirely but for now...","ZOOKEEPER-454","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-07-01 04:57:08.607","Closed","allow compilation with jdk1.5","Bug","2009-07-09 04:24:06.262",0,0,7818,7818
16663,"2009-07-01 00:25:34.81","This causes a problem when we crash and restart a replica R because other replicas believe that they still have a connection to R after restart.","ZOOKEEPER-453","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-07-01 04:47:21.049","Closed","Worker is not removed in QuorumCnxManager upon crash","Bug","2009-07-09 04:24:06.218",0,0,7750,7750
16664,"2009-06-30 09:17:21.687","THe image is wrong and should have percentage of reads on the x axis.. ","ZOOKEEPER-452","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-30 09:29:40.603","Closed","zookeeper performance graph should have percentage of reads rather than percentage of writes -  zkperfRW-3.2.jpg","Bug","2009-07-09 04:24:06.171",0,0,7827,7827
16666,"2009-06-29 23:29:01.098","The session move patch broke ephemeral cleanup during session expiration. tragically, we didn't have test coverage to detect the bug.","ZOOKEEPER-450","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-30 09:32:40.301","Closed","emphemeral cleanup not happening with session timeout","Bug","2009-07-09 04:24:06.111",0,0,7829,7829
16667,"2009-06-27 04:36:25.024","sesssionmoved in java code and ZCLOSING in C have the same value. We need to assign a new value to ZSESSIONMOVED.","ZOOKEEPER-449","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-27 05:16:14.183","Closed","sesssionmoved in java code and ZCLOSING in C have the same value.","Bug","2009-07-09 04:24:06.073",0,0,7827,7827
16668,"2009-06-27 02:39:41.212","png images are not compatible with forrest generating pdf. We can them to jpg to get them into pdfs.","ZOOKEEPER-448","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-27 02:51:50.325","Closed","png files do nto work with forrest.","Bug","2009-07-09 04:24:06.028",0,0,7827,7827
16670,"2009-06-20 06:44:19.189","the host auth scheme was removed because it used a blocking call in an async pipeline. however, tragically, the blocking call was not removed including a couple of other stray classes.","ZOOKEEPER-446","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-23 04:55:03.574","Closed","some traces of the host auth scheme left","Bug","2009-07-09 04:24:05.963",0,0,7829,7829
16672,"2009-06-16 02:48:24.615","the perms_all definition in Java is PERMS.ALL and does not include ADMIN perms but in c the PERMS_ALL def includes the ADMIN perms. We should make it consistent to include or not include the admin perms in both c and java.","ZOOKEEPER-444","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-18 01:37:22.743","Closed","perms definition for PERMS_ALL differ in C and java","Bug","2009-07-09 04:24:05.917",0,0,7827,7827
16681,"2009-06-06 04:42:24.128","the server has a ""super"" digest based auth user that enables administrative access (ie has access to znodes regardlessof acl settings) but the password is not configurable1) make the default digest null, ie turn off ""super"" by default2) if a command line option is specified when starting server then use the provided digest for supereg. java -Dzookeeper.DigestAuthenticationProvider.superDigest=xkxkxkxkx ....also this is not documented in the forrest docs - need to add that along with tests as part of the patch.","ZOOKEEPER-435","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-09 13:21:48.863","Closed","allow ""super"" admin digest based auth to be configurable","Bug","2009-07-09 04:24:05.715",0,0,7818,7818
16683,"2009-06-06 01:57:28.757","running a ""getAcl"" on the root znode ""/"" fails with the following:Fri Jun  5 10:21:17 2009: 2009-06-05 10:21:17,072 - ERROR [CommitProcessor:3:FinalRequestProcessor@243] - Failed to process sessionid:0x321b16868f40003 type:getACL cxid:0x3 zxid:0xfffffffffffffffe txntype:unknown n/a Fri Jun  5 10:21:17 2009: java.lang.NullPointerExceptionFri Jun  5 10:21:17 2009:       at java.util.ArrayList.<init>(ArrayList.java:131)Fri Jun  5 10:21:17 2009:       at org.apache.zookeeper.server.DataTree.getACL(DataTree.java:622)Fri Jun  5 10:21:17 2009:       at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:216)Fri Jun  5 10:21:17 2009:       at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:74)Fri Jun  5 10:21:17 2009: 2009-06-05 10:21:17,073 - ERROR [CommitProcessor:3:FinalRequestProcessor@250] - Dumping request buffer: 0x00012fWe need to support getting/setting the root acl in particular -- not being able to control acls on this node makes multi-tenancy a non-starter.","ZOOKEEPER-433","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-18 09:09:34.861","Closed","getacl on root znode (/) fails","Bug","2009-07-09 04:24:05.624",0,0,7818,7818
16689,"2009-06-03 05:01:41.077","I am running a 5 node ZooKeeper cluster and I noticed that one of them has very high CPU usage: PID   USER      PR  NI  VIRT  RES  SHR S   %CPU %MEM    TIME+   COMMAND  6883  infact       22   0   725m  41m  4188 S   95       0.5          5671:54  javaIt is not ""doing anything"" application-wise at this point, so I was wondering why the heck it's using up so much CPU.","ZOOKEEPER-427","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-19 04:33:45.617","Closed","ZooKeeper server unexpectedly high CPU utilisation","Bug","2010-10-11 15:34:54.398",0,0,7750,8355
16695,"2009-05-29 02:39:39.1","the scripts in the test directory of zkpython are missing #! headersProbably:#!/bin/shfor shell scripts and #!/usr/bin/pythonfor .py scripts?Also include a shell script that will svn chmod the *.py scripts so that they can be executed individually from the command line (shortcutrather than (python foo.py).","ZOOKEEPER-421","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-02 05:00:43.506","Closed","zkpython run_tests.sh is missing #!","Bug","2009-07-09 04:24:05.038",0,0,7886,7818
16696,"2009-05-29 02:36:00.203","Currently you cannot just build and test the zkpython contrib, you need to actually install the zookeeper client c library as wellas the zkpython lib itself.There really needs to be 2 steps:1) build/test zkpython ""encapsulated"" within the src repository, there should be no requirement to actually install anything(this is esp the case for automated processes and for review by PMC during release time for example)2) build an egg that can be distributed/installed by end user","ZOOKEEPER-420","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-10-02 05:38:07.47","Closed","build/test should not require install in zkpython","Bug","2010-03-27 01:24:54.946",0,2,7886,7818
16697,"2009-05-28 08:24:09.455","Due to reference counts being incremented incorrectly for stat-based calls, Python's GC occasionally aborts.","ZOOKEEPER-419","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-29 02:31:50.201","Closed","Reference counting bug in Python bindings causes abort errors","Bug","2009-07-09 04:24:04.976",0,2,7886,7886
16698,"2009-05-27 15:10:50.257","It would be very nice to have a browser that would allow the state of a Zoo to be examined.  Even nice would be such a utility that showed changes in real time.","ZOOKEEPER-418","Major","ZOOKEEPER","ZooKeeper","ASF","Won't Fix","2010-12-13 09:03:09.426","Closed","Need nifty zookeeper browser","Bug","2011-11-24 03:22:13.346",2,4,7925,7925
16699,"2009-05-27 02:10:27.539","There is  a possibility for stray messages from a previous connection to violate ordering and generally cause problems. Here is a scenario: we have a client, C, two followers, F1 and F2, and a leader, L. The client is connected to F1, which is a slow follower. C sends setData(""/a"", ""1"") to F1 and then loses the connection, so C reconnects to F2 and sends setData(""/a"", ""2"").  it is possible, if F1 is slow enough and the setData(""/a"", ""1"") got onto the network before the connection break, for F1 to forward the setData(""/a"", ""1"") to L after F2 forwards setData(""/a"", ""2"").to fix this, the leader should keep track of which follower last registered a session for a client and drop any requests from followers for clients for whom they do not have a registration. ","ZOOKEEPER-417","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-27 05:09:06.304","Closed","stray message problem when changing servers","Bug","2009-07-09 04:24:04.93",0,0,7829,7829
16700,"2009-05-26 23:51:26.92","I was checking the bookkeper jar, and I found that it includes some unnecessary files related to junit, such as:{noformat}     0 Tue May 12 19:00:14 PDT 2009 tmp/     0 Tue May 12 19:00:00 PDT 2009 tmp/test14667.junit.dir/     0 Tue May 12 19:00:08 PDT 2009 tmp/test14667.junit.dir/version-2/     0 Tue May 12 19:00:10 PDT 2009 tmp/test16109.junit.dir/     0 Tue May 12 19:00:16 PDT 2009 tmp/test16109.junit.dir/version-2/     0 Tue May 12 19:00:14 PDT 2009 tmp/test16113.junit.dir/     0 Tue May 12 19:00:16 PDT 2009 tmp/test16113.junit.dir/version-2/  2256 Tue May 12 18:59:08 PDT 2009 logs/TEST-org.apache.bookkeeper.test.BookieClientTest.txt167046 Tue May 12 19:00:00 PDT 2009 logs/TEST-org.apache.bookkeeper.test.BookieReadWriteTest.txt 13035 Tue May 12 19:00:08 PDT 2009 logs/TEST-org.apache.bookkeeper.test.CloseTest.txt 25036 Tue May 12 19:00:16 PDT 2009 logs/TEST-org.apache.bookkeeper.test.LedgerRecoveryTest.txt   896 Tue May 12 19:00:16 PDT 2009 logs/TEST-org.apache.bookkeeper.test.NIOServerFactoryTest.txt{noformat}","ZOOKEEPER-416","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-28 12:55:48.645","Closed","BookKeeper jar includes unnecessary files","Bug","2009-07-09 04:24:04.896",0,0,7750,7750
16703,"2009-05-21 05:59:15.378","1) createClient in testclient.cc (check all tests) is not correctly waiting for syncconnected to the server2) there are some instances of while(xxx); in the test code, this could cause problems, really we need tohave some limit on the number of iterations (other than just the test, which may never return false), also theloop should have some sort of sleep(100msec) (whatever time) in order to limit cpu use.","ZOOKEEPER-413","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-26 10:57:04.337","Closed","two flaws need addressing in the c tests that can cause false positive failures","Bug","2010-03-27 01:24:54.882",0,0,7818,7818
16705,"2009-05-20 06:39:10.378","     [exec] Zookeeper_simpleSystem::testAsyncWatcherAutoReset : assertion     [exec]      [exec] /grid/0/gs/gridre/hudson/workspace/zootestbuild/trunk/src/c/tests/TestClient.cc:499: Assertion: assertion failed [Expression: ctx.waitForDisconnected(zk)]     [exec] Failures !!!     [exec] Run: 32   Failure total: 1   Failures: 1   Errors: 0     [exec] make: *** [run-check] Error 1","ZOOKEEPER-411","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-21 06:41:43.947","Closed","Building zookeeper fails on RHEL 5 64 bit during test-cppunit","Bug","2009-07-09 04:24:04.814",0,0,7827,8362
16711,"2009-05-19 01:24:02.263","The java client shell does not handle null return data and throws out null pointer exception.","ZOOKEEPER-405","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-21 01:59:28.637","Closed","nullpointer exception in zookeeper java shell.","Bug","2009-07-09 04:24:04.541",0,0,7827,7827
16739,"2009-04-16 06:10:59.558","Giri, can you take a look at this:1) looks like autoreconf is always run, which means that a re-configure/re-make is run each time the tests are run using ant2) tabs were introduced to the build when cppunit changes were made, please only use spaces (would be great if you could fix this too -- hard to read in my editor)Thanks!","ZOOKEEPER-376","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-08 06:38:26.892","Closed","ant test target re-compiles cppunit code every time","Bug","2009-07-09 04:24:03.062",0,0,7818,7818
16712,"2009-05-19 00:50:10.007","the nightly build failed with the following errorcompile:     [echo] contrib: zkpythonBUILD FAILED/home/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/build.xml:444: The following error occurred while executing this line:/home/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/src/contrib/build.xml:39: The following error occurred while executing this line:/home/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/src/contrib/build-contrib.xml:79: srcdir ""/home/hudson/hudson-slave/workspace/ZooKeeper-trunk/trunk/src/contrib/zkpython/src/java"" does not exist!Total time: 32 secondsPublishing JavadocRecording test resultsRecording fingerprintsPublishing Clover coverage report...Sending e-mails to: zookeeper-dev@hadoop.apache.orgfinished: FAILURE","ZOOKEEPER-404","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-20 05:27:46.718","Closed","nightly build failed on hudson.","Bug","2009-07-09 04:24:04.499",0,0,7886,7827
16714,"2009-05-14 06:36:40.611","the zookeeper c client library seg faults on data being null for a zoo node. ","ZOOKEEPER-402","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-22 05:22:46.615","Closed","zookeeper c library segfaults on data for a node in zookeeper being null.","Bug","2009-07-09 04:24:03.939",0,1,7827,7827
16716,"2009-05-13 05:26:25.814","When running a few experiments with BookKeeper, I found a couple of issues with closing a ledger:* On ClientCBWorker::run(), we should call setAddConfirmed before invoking the callback method. Otherwise, it is possible that an application closes a ledger before ClientCBWorker modifies the last confirmed operation, and the value written to ZooKeeper won't be the last one written;* LedgerHandle should write the last add confirmed instead of the last counter. The last attribute counts the operations issued, and we use it to determine the id of the next entry. If an application calls close upon receiving all callbacks, then with the previous modification, the last confirmed add must be equal to (last-1). However, if an application invokes close before receiving all callbacks, the ledger may be left in an inconsistent state because the last entry written to ZooKeeper may be an operation that hasn't completed yet. Although the modifications required are simple, they are important.","ZOOKEEPER-400","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-14 16:22:07.971","Closed","Issues with procedure to close ledger","Bug","2013-05-02 10:29:24.357",0,0,7750,7750
16720,"2009-05-09 08:41:05.72","There is a race condition in zoopkeeper client library wherein if the application calls zookeeper_close() and zoo_anysynchronouscall simultaneously, sometimes the zoo_sync api call gets hung waiting for a notification whcih will never come.We might want to create another bugfix release for this.","ZOOKEEPER-396","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-09 04:39:31.726","Closed","race condition in zookeeper client library between zookeeper_close and zoo_synchronous api","Bug","2009-07-09 04:24:03.871",0,0,7827,7827
16724,"2009-05-07 02:25:18.675","Bookeeper mainline code is calling printStackTrace, it should be using logging instead.","ZOOKEEPER-391","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-13 05:00:00.143","Closed","bookeeper mainline code should not be calling printStackTrace","Bug","2009-07-09 04:24:03.69",0,0,7750,7818
16740,"2009-04-16 00:03:31.116","zoo_add_auth doesn't maintain a list of auths - it only stores the most recent auth send to the server. As a result on re-sync to the cluster it will lose (not reregister) any auths prior to the most recent.This code should maintain a list of auths similar to the java code. Be sure to free the memory in close.","ZOOKEEPER-375","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-10 03:01:20.471","Closed","zoo_add_auth only retains most recent auth on re-sync","Bug","2009-07-09 04:24:02.981",0,0,7827,7818
16812,"2009-02-06 20:40:17.588","Some characters are not allowed in ObjectName values and need quoting, see http://java.sun.com/javase/6/docs/api/javax/management/ObjectName.html.This came up with IPv6 addresses which contain a colon character.","ZOOKEEPER-302","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-02-07 05:21:47.681","Closed","Quote values in JMX ObjectNames ","Bug","2010-03-24 00:45:01.245",0,0,7818,8367
16730,"2009-04-28 06:38:07.438","http://hudson.zones.apache.org/hudson/job/Zookeeper-Patch-vesta.apache.org/53/testReport/org.apache.zookeeper.server/CRCTest/testChecksums/crctest failed withError MessageUnreasonable length = 518291091Stacktracejava.io.IOException: Unreasonable length = 518291091	at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:101)	at org.apache.zookeeper.server.DataNode.deserialize(DataNode.java:116)	at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:109)	at org.apache.zookeeper.server.DataTree.deserialize(DataTree.java:954)	at org.apache.zookeeper.server.util.SerializeUtils.deserializeSnapshot(SerializeUtils.java:91)	at org.apache.zookeeper.server.persistence.FileSnap.deserialize(FileSnap.java:125)	at org.apache.zookeeper.server.CRCTest.testChecksums(CRCTest.java:146)","ZOOKEEPER-385","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-22 15:43:59.76","Closed","crctest failed on hudson patch test","Bug","2009-07-09 04:24:03.558",0,0,7827,7818
16731,"2009-04-28 04:55:46.582","keeper exceptions thrown by the java client don't include path, having path helps in debugging.","ZOOKEEPER-384","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-05 12:55:50.081","Closed","keeper exceptions missing path","Bug","2009-07-09 04:24:03.511",0,0,7818,7818
16733,"2009-04-28 02:30:04.537","the zookeeper c tests fail on 64 bit machines with gcc 4.1.2 with the following error [exec] /workspace/zootestbuild/trunk/src/c/tests/TestClient.cc: In static member function 'static void Zookeeper_simpleSystem::statCompletion(int, const Stat*, const void*)':      [exec] /workspace/zootestbuild/trunk/src/c/tests/TestClient.cc:273: error: cast from 'const void*' to 'int' loses precision      [exec]/workspace/zootestbuild/trunk/src/c/tests/TestClient.cc: In static member function 'static void Zookeeper_simpleSystem::voidCompletion(int, const void*)':      [exec] /workspace/zootestbuild/trunk/src/c/tests/TestClient.cc:291: error: cast from 'const void*' to 'int' loses precision  ","ZOOKEEPER-382","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-30 03:12:04.402","Closed","zookeeper cpp tests fails on 64 bit machines with gcc 4.1.2","Bug","2009-07-09 04:24:03.412",0,0,7827,7827
16736,"2009-04-21 14:30:02.226","patch test build failed with "" equality assertion failed""link: http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/Zookeeper-Patch-vesta.apache.org/44/consoleMahadev,Could you take a look?Tnx!","ZOOKEEPER-379","Major","ZOOKEEPER","ZooKeeper","ASF","Invalid","2009-05-22 08:27:44.284","Closed","cppunit tests failed during hudson patch tests ","Bug","2009-07-09 04:24:03.245",0,1,7827,8063
16738,"2009-04-18 02:14:36.925","Giri, can you take a look at this?I ran ""ant test""looks like this test failed:     [exec] Zookeeper_operations::testOperationsAndDisconnectConcurrently1 : assertion then later...     [exec]      [exec] /home/phunt/dev/workspace/svnzk_apache/src/c/tests/TestOperations.cc:551: Assertion: equality assertion failed [Expected: -4, Actual  : 0, ZCONNECTIONLOSS != rc]     [exec] Failures !!!     [exec] Run: 38   Failure total: 1   Failures: 1   Errors: 0     [exec] make: *** [run-check] Error 1     [exec] Result: 2test-core:test-contrib:BUILD SUCCESSFULTotal time: 15 minutes 39 seconds","ZOOKEEPER-377","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-21 06:16:43.271","Closed","running ant cppunit tests, a failure still results in BUILD SUCCESSFUL","Bug","2009-07-09 04:24:03.14",0,0,8063,7818
16741,"2009-04-10 17:28:43.049","nitay-joffes-macbook-pro:c nitay$ pwd/Users/nitay/code/zookeeper/src/cnitay-joffes-macbook-pro:c nitay$ makemake  all-am/bin/sh ./libtool --tag=CC   --mode=compile gcc -DHAVE_CONFIG_H -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O2 -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c -o zookeeper.lo `test -f 'src/zookeeper.c' || echo './'`src/zookeeper.clibtool: compile:  gcc -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -Wall -Werror -g -O2 -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c src/zookeeper.c  -fno-common -DPIC -o .libs/zookeeper.occ1: warnings being treated as errorssrc/zookeeper.c: In function 'zoo_add_auth':src/zookeeper.c:2378: warning: 'auth.buff' may be used uninitialized in this functionsrc/zookeeper.c:2378: warning: 'auth.len' may be used uninitialized in this functionmake[1]: *** [zookeeper.lo] Error 1make: *** [all] Error 2Need to set auth.buff and auth.len to zero.","ZOOKEEPER-374","Trivial","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-17 04:28:46.027","Closed","Uninitialized struct variable in C causes warning which is treated as an error","Bug","2009-07-09 04:24:02.933",0,0,7818,8352
16745,"2009-04-08 05:26:40.851","There are a couple of problems pointed out by findbugs and that we can easily fix.","ZOOKEEPER-370","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-09 05:10:22.161","Closed","Fix critical problems reported by findbugs","Bug","2009-07-09 04:24:02.764",0,0,7750,7750
16748,"2009-04-04 08:29:32.882","during local testing I received the attached recoverytest failure","ZOOKEEPER-367","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-18 02:12:25.201","Closed","RecoveryTest failure - ""unreasonable length"" IOException","Bug","2009-07-09 04:24:02.716",0,0,7827,7818
16750,"2009-04-04 05:56:01.546","Note: the javadoc is wrong here:    /**     * Returns the last entry identifier submitted and increments it.     * @return long     */    long setLast(long last){also would be great to have javadoc for the legerrecoverymonitor getNextHint method. I was reviewing this code and it would have been helpful to know what to expect of this method. (possible return values, etc...)","ZOOKEEPER-365","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-30 03:16:57.997","Closed","javadoc is wrong for setLast in LedgerHandle","Bug","2009-07-09 04:24:02.675",0,0,7750,7818
16752,"2009-04-03 19:08:05.832","When recovering a ledger, LedgerRecoveryMonitor currently start from the entry preceding the hint. if the hint is zero, then it causes an access out of the bounds of the bookie array in QuorumEngine, leading to the mentioned NPE.","ZOOKEEPER-363","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-07 10:43:59.731","Closed","NPE when recovering ledger with no hint ","Bug","2009-07-09 04:24:02.642",0,0,7750,8351
16753,"2009-04-03 17:04:29.436","I have been able to identify two reasons that cause FLENewEpochTest to fail:1- There is a race condition that is triggered when two peers try to establish a connection to each other for leader election. Basically, if they start roughly at the same time, the server with highest id will try to open two connections. The two competing connections will lead to one notification message to be lost. This message happens to be critical for this two process scenario; 2- The code to shut down a peer is not working well with the unit tests. For this particular unit test, we need to be able to shut down a peer completely to check the situation the test tries to reproduce. However, it seems that in some runs timing causes the other peers to believe it is still alive, and end up electing it. This peer, however, eventually shuts down and leader election fails.","ZOOKEEPER-362","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-04 06:12:11.412","Closed","Issues with FLENewEpochTest","Bug","2009-07-09 04:24:02.606",0,0,7750,7750
16759,"2009-03-27 13:09:51.521","make  validatePath non public in Zookeeper client api.","ZOOKEEPER-355","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-10 05:03:07.502","Closed","make  validatePath non public in Zookeeper client api.","Bug","2009-07-09 04:24:01.648",0,0,7818,7827
16766,"2009-03-27 03:11:45.43","In 3.0.1, I could create a sequence node like this:/nodes/0000001like this:string path = ""/nodes/"";string value = ""data"";int rc = zoo_acreate(zh, path.c_str(), value.c_str(), value.length(), &ZOO_OPEN_ACL_UNSAFE, ZOO_EPHEMERAL | ZOO_SEQUENCE, &czoo_created, &where);In 3.1.1, this fails with error -8 (ZBADARGUMENTS).Adding something after the ""/"" in the path makes the code work fine:string path = ""/nodes/n"";I assume something is checking if the path ends in ""/"" but not checking the sequence flag.","ZOOKEEPER-348","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-10 05:04:17.191","Closed","Creating node with path ending in ""/"" with sequence flag set","Bug","2009-07-09 04:24:01.314",0,0,7818,8364
16767,"2009-03-26 08:13:38.819","I'm getting an error compiling zkfuse:g++ -DHAVE_CONFIG_H -I. -I..    -I/home/nitay/code/zookeeper-git/src/contrib/zkfuse/../../c/include -I/home/nitay/code/zookeeper-git/src/contrib/zkfuse/../../c/generated -I../include -I/usr/include -D_FILE_OFFSET_BITS=64 -D_REENTRANT -g -O2 -MT zkfuse.o -MD -MP -MF .deps/zkfuse.Tpo -c -o zkfuse.o zkfuse.cczkfuse.cc: In function 'int main(int, char**)':zkfuse.cc:4282: error: 'String' does not name a typezkfuse.cc:4283: error: 'file' was not declared in this scopemake[2]: *** [zkfuse.o] Error 1make[2]: Leaving directory `/home/nitay/code/zookeeper-git/src/contrib/zkfuse/src'make[1]: *** [all-recursive] Error 1make[1]: Leaving directory `/home/nitay/code/zookeeper-git/src/contrib/zkfuse'make: *** [all] Error 24279     /**4280      * Initialize log4cxx 4281      */4282     const String file(""log4cxx.properties"");4283     PropertyConfigurator::configureAndWatch( file, 5000 );4284     LOG_INFO(LOG, ""Starting zkfuse"");String is not standard, we should change it to std::string.","ZOOKEEPER-347","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-09 06:39:43.718","Closed","zkfuse uses non-standard String","Bug","2009-07-09 04:24:01.265",0,0,7818,8352
16768,"2009-03-24 07:34:45.137","we should just remove the kill command from the client port. Its a security risk (though we do not have much security right now) to be able to kill the server from a tcp port without any authentication... ","ZOOKEEPER-346","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-18 04:48:51.845","Closed","remove the kill command fro mthe client port.","Bug","2009-07-09 04:24:01.179",0,0,7818,7827
16770,"2009-03-19 02:45:48.301","I have been having a problem with zookeeper 3.0.1 and now with 3.1.0 where I see a lot of expired sessions.  I am using a 16 node cluster which is all on the same local network.  There is a single zookeeper instance (these are benchmarking runs).The problem appears to be correlated with either run time or system load.\Personally I think that it is system load because I have session session expired events under a Windows platform running zookeeper and the application (i.e., everthing is local) when the application load suddenly spikes.  To me this suggests that the client is not able to renew (ping) the zookeeper service in a timely manner and is expired.  But the log messages below with the ""read error"" suggest that maybe there is something else going on?Zookeeper Configuration#Wed Mar 18 12:41:05 GMT-05:00 2009clientPort=2181dataDir=/var/bigdata/benchmark/zookeeper/1syncLimit=2dataLogDir=/var/bigdata/benchmark/zookeeper/1tickTime=2000Some representative log messages are below.Client side messages (from our app)ERROR [main-EventThread] com.bigdata.zookeeper.ZLockImpl$ZLockWatcher.process(ZLockImpl.java:400) 2009-03-18 13:35:40,335 - Session expired: WatchedEvent: Server state change. New state: Expired : zpath=/benchmark/jobs/com.bigdata.service.jini.benchmark.ThroughputMaster/test_1/client1160/locknodeERROR [main-EventThread] com.bigdata.zookeeper.ZLockImpl$ZLockWatcher.process(ZLockImpl.java:400) 2009-03-18 13:35:40,335 - Session expired: WatchedEvent: Server state change. New state: Expired : zpath=/benchmark/jobs/com.bigdata.service.jini.benchmark.ThroughputMaster/test_1/client1356/locknodeServer side messages: WARN [NIOServerCxn.Factory:2181] org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:417) 2009-03-18 13:06:57,252 - Exception causing close of session 0x1201aac14300022 due to java.io.IOException: Read error WARN [NIOServerCxn.Factory:2181] org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:417) 2009-03-18 13:06:58,198 - Exception causing close of session 0x1201aac1430000f due to java.io.IOException: Read error","ZOOKEEPER-344","Major","ZOOKEEPER","ZooKeeper","ASF","Invalid","2009-05-07 01:04:50.486","Closed","doIO in NioServerCnxn: Exception causing close of session : cause is ""read error""","Bug","2009-07-09 04:24:01.048",0,1,7818,8330
16773,"2009-03-19 00:11:12.554","ZOOKEEPER 330/336 caused a regression in QuorumPeerMain -- cannot reliably start a cluster due to missing tickTime.","ZOOKEEPER-341","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-03-19 00:38:38.002","Closed","regression in QuorumPeerMain, tickTime from config is lost, cannot start quorum","Bug","2009-07-09 04:24:00.901",0,0,7818,7818
16774,"2009-03-18 06:16:39.773","binaryinputarchive throws out runtimeexceptions for unreasonable length datastructures. We should change that to be IOExceptions so that we can handle partial writes to logs,, machine powerdown better.","ZOOKEEPER-340","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-18 07:19:49.048","Closed","binaryinputarchive throws out runtimeexceptions for unreasonable length datastructures.","Bug","2009-07-09 04:24:00.817",0,0,7827,7827
16779,"2009-03-05 02:55:48.57","currently the zookeeper followers do not commit the new leader election. This will cause problems in a failure scenarios with a follower acking to the same leader txn id twice, which might be two different intermittent leaders and allowing them to propose two different txn's of the same zxid.","ZOOKEEPER-335","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2011-06-14 13:14:31.874","Closed","zookeeper servers should commit the new leader txn to their logs.","Bug","2011-11-24 03:22:30.01",0,5,7829,7827
16780,"2009-02-28 07:55:44.282","bookkeeper benchmark (testclient.java) has compiling errors.","ZOOKEEPER-334","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-03-04 06:49:12.631","Closed","bookkeeper benchmark (testclient.java) has compiling errros.","Bug","2009-07-09 04:24:00.637",0,0,7827,7827
16781,"2009-02-28 05:56:25.242","helgrind generated a number of issues, I pulled a bunch of them. Most are related to the test, some are really issues with the mt zk client code though:valgrind --tool=helgrind --log-file=helgrind_mt.out ./zktest-mt==31294== Thread #2: pthread_cond_{timed}wait called with un-held mutex==31294==    at 0x4027F8F: pthread_cond_wait@* (hg_intercepts.c:560)==31294==    by 0x404D881: pthread_cond_wait@GLIBC_2.0 (in /lib/tls/i686/cmov/libpthread-2.8.90.so)==31294==    by 0x4028037: pthread_cond_wait@* (hg_intercepts.c:574)==31294==    by 0x809EBB7: pthread_cond_wait (PthreadMocks.cc:54)==31294==    by 0x80ABCF6: notify_thread_ready (mt_adaptor.c:136)==31294==    by 0x80ABE90: do_io (mt_adaptor.c:277)==31294== Possible data race during write of size 4 at 0x42E9A58==31294==    at 0x8050D83: terminateZookeeperThreads(_zhandle*) (ZKMocks.cc:518)==31294==    by 0x805543B: DeliverWatchersWrapper::call(_zhandle*, int, int, char const*, watcher_object_list**) (ZKMocks.cc:261)==31294==    by 0x80520F7: __wrap_deliverWatchers (ZKMocks.cc:220)==31294==    by 0x80A287B: process_completions (zookeeper.c:1393)==31294==    by 0x80ABDAA: do_completion (mt_adaptor.c:332)==31294== Possible data race during write of size 4 at 0xBEFF5F30==31294==    at 0x80589AF: Zookeeper_watchers::ConnectionWatcher::~ConnectionWatcher() (TestWatchers.cc:54)==31294==    by 0x805D062: Zookeeper_watchers::testDefaultSessionWatcher1() (TestWatchers.cc:438)==31294==    by 0x805608C: CppUnit::TestCaller<Zookeeper_watchers>::runTest() (TestCaller.h:166)==31294== Possible data race during write of size 4 at 0x42EB104==31294==    at 0x80A03EE: queue_completion (zookeeper.c:1776)==31294==    by 0x80A3A44: zookeeper_process (zookeeper.c:1598)==31294==    by 0x80AC00B: do_io (mt_adaptor.c:309)==31294== Thread #29: pthread_cond_{timed}wait called with un-held mutex==31294==    at 0x4027F8F: pthread_cond_wait@* (hg_intercepts.c:560)==31294==    by 0x404D881: pthread_cond_wait@GLIBC_2.0 (in /lib/tls/i686/cmov/libpthread-2.8.90.so)==31294==    by 0x4028037: pthread_cond_wait@* (hg_intercepts.c:574)==31294==    by 0x809EBB7: pthread_cond_wait (PthreadMocks.cc:54)==31294==    by 0x80AB9B3: wait_sync_completion (mt_adaptor.c:82)==31294==    by 0x80A1E82: zoo_wget (zookeeper.c:2517)==31294==    by 0x80A1F13: zoo_get (zookeeper.c:2497)","ZOOKEEPER-333","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-03-12 08:31:45.4","Closed","helgrind thread issues identified in mt c client code","Bug","2009-07-09 04:24:00.602",0,1,7827,7818
16782,"2009-02-28 04:04:12.025","Attaching valgrind log files.1)  getpwuid_r doesn't seem like it's due to us2) the rest seem to be valid","ZOOKEEPER-332","Blocker","ZOOKEEPER","ZooKeeper","ASF","Invalid","2009-03-05 02:25:40.689","Closed","c client issues (memory leaks) reported by valgrind","Bug","2009-07-09 04:24:00.569",0,0,NULL,7818
16784,"2009-02-25 09:30:37.304","ZOOKEEPER-326 made a change to zookeeperservermain.java that broke the starting of zookeeperserver with just the port and datadir.","ZOOKEEPER-330","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-02-28 03:05:59.433","Closed","zookeeper standalone server does not startup with just a port and datadir.","Bug","2009-07-09 04:24:00.49",0,0,7827,7827
16788,"2009-02-21 04:12:15.11","When using the ZooKeeper server in standalone mode, it ignores the tickTime setting in the configuration file and uses the DEFAULT_TICK_TIME of 3000 coded into ZooKeeperServer.java.","ZOOKEEPER-326","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-02-24 07:56:36.498","Closed","standalone server ignores tickTime configuration","Bug","2009-07-09 04:24:00.347",0,0,8348,8348
16789,"2009-02-20 08:04:55.731","FLENewEpochTest fails quite frequently on my machine. ","ZOOKEEPER-325","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-04-08 07:08:22.418","Closed","FLENewEpoch test fails.","Bug","2009-07-09 04:24:00.284",0,0,NULL,7827
16794,"2009-02-18 06:48:01.243","If a client calls zoo_add_auth() with an invalid scheme (e.g., ""foo"") the ZooKeeper server will mark their session expired and close the connection.  However, the C client has returned immediately after queuing the new auth data to be sent with a ZOK return code.If the client then waits for their auth completion function to be called, they can wait forever, as no session event is ever delivered to that completion function.  All other completion functions are notified of session events by free_completions(), which is called by cleanup_bufs() in handle_error() in handle_socket_error_msg().In actual fact, what can happen (about 50% of the time, for me) is that the next call by the IO thread to flush_send_queue() calls send() from within send_buffer(), and receives a SIGPIPE signal during this send() call.  Because the ZooKeeper C API is a library, it properly does not catch that signal.  If the user's code is not catching that signal either, they experience an abort caused by an untrapped signal.  If they are ignoring the signal -- which is common in context I'm working in, the Apache httpd server -- then flush_send_queue()'s error return code is EPIPE, which is logged by handle_socket_error_msg(), and all non-auth completion functions are notified of a session event.  However, if the caller is waiting for their auth completion function, they wait forever while the IO thread tries repeatedly to reconnect and is rejected by the server as having an expired session.So, first of all, it would be useful to document in the C API portion of the programmer's guide that trapping or ignoring SIGPIPE is important, as this signal may be generated by the C API.Next, the two attached patches call the auth completion function, if any, in free_completions(), which fixes this problem for me.  The second attached patch includes auth lock/unlock function, as per ZOOKEEPER-319.","ZOOKEEPER-320","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-02-28 04:05:19.651","Closed","call auth completion in free_completions()","Bug","2009-07-09 04:24:00.186",0,0,8348,8348
16795,"2009-02-18 06:32:27.602","Looking over the zookeeper.c code it appears to me that the zoo_add_auth() function may be called at any time by the user in their ""main"" thread.  This function alters the elements of the auth_info structure in the zhandle_t structure.Meanwhile, the IO thread may read those elements at any time in such functions as send_auth_info() and auth_completion_func().  It seems important, then, to add a lock which prevents data being read by the IO thread while only partially changed by the user's thread.  The attached patch add such a lock.","ZOOKEEPER-319","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-02-28 03:49:16.302","Closed","add locking around auth info in zhandle_t","Bug","2009-07-09 04:24:00.116",0,0,8348,8348
16815,"2009-02-06 07:22:28.577","JMX enabled by defaultreadlink: illegal option -- fusage: readlink [-n] [file ...]./zkEnv.sh no such file.if you run bin/zkServer.sh start this is the error i get.if I run it from the bin directory then it seems to be fine.","ZOOKEEPER-299","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-02-13 03:05:47.298","Closed","server startup scripts fail on a mac.","Bug","2009-07-09 04:23:57.063",0,0,NULL,7827
16816,"2009-02-06 06:23:42.006","The configure scripts in src/c, the zkServer.sh in src/c/test, and some of the other scripts are not marked as executable when running ""ant tar"". The build.xml should be updated to mark appropriately.","ZOOKEEPER-298","Major","ZOOKEEPER","ZooKeeper","ASF","Invalid","2009-05-20 06:19:33.231","Closed","some excecutables (scripts typ.) are not marked as such in tar generated by ""ant tar""","Bug","2009-07-09 04:23:57.013",0,0,NULL,7818
16796,"2009-02-18 04:52:02.03","From a review of zk_hashtable.c it appears to me that all functions which manipulate the hashtables are called from the IO thread, and therefore any need for locking is obviated.If I'm wrong about that, then I think at a minimum collect_keys() should acquire a lock in the same manner as collect_session_watchers().  Both iterate over hashtable contents (in the latter case using copy_table()).However, from what I can see, the only function (besides the init/destroy functions used when creating a zhandle_t) called from the completion thread is deliverWatchers(), which simply iterates over a ""delivery"" list created from the hashtables by collectWatchers().  The activateWatcher() function contains comments which describe it being called by the completion thread, but in fact it is called by the IO thread in zookeeper_process().I believe all calls to collectWatchers(), activateWatcher(), and collect_keys() are made by the IO thread in zookeeper_interest(), zookeeper_process(), check_events(), send_set_watches(), and handle_error().  Note that queue_session_event() is aliased as PROCESS_SESSION_EVENT, but appears only in handle_error() and check_events().Also note that handle_error() is called only in zookeeper_process() and handle_socket_error_msg(), which is used only by the IO thread, so far as I can see.","ZOOKEEPER-318","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-03-06 08:45:04.527","Closed","remove locking in zk_hashtable.c or add locking in collect_keys()","Bug","2009-07-09 04:24:00.012",0,0,8348,8348
16805,"2009-02-11 06:21:50.884","The zookeeper_process() function incorrectly calls the c.acl_result member of the completion_list_t structure when handling the completion from a synchronous zoo_get_acl() request.  The c.acl_result member is set to SYNCHRONOUS_MARKER, which is a null pointer.The attached patch removes this call.","ZOOKEEPER-309","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-03-12 08:49:32.492","Closed","core dump using zoo_get_acl() ","Bug","2009-07-09 04:23:57.35",0,0,7827,8348
16814,"2009-02-06 07:24:03.442","remove printStackTrace from zk jmx code (review the rest of the code at the same time)","ZOOKEEPER-300","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-15 03:16:56.444","Closed","zk jmx code is calling printStackTrace when creating bean name (should not be)","Bug","2009-07-09 04:23:57.121",0,0,7818,7818
16821,"2009-02-06 02:43:18.358","ZOOKEEPER-255 fixed an issue with zoo_set not providing access to stat structure, however this has broken b/w compatibility with previous releases.We need to:1) revert zoo_set to not have stat parameter (keep b/w compat)2) add zoo_set2 method with stat param added to the function signature3) add a version.h file to src/c/include that provides zoo version detail to clientsie.> #define ZOO_MAJOR_VERSION 3> #define ZOO_MINOR_VERSION 1> #define ZOO_PATCH_VERSION 0(a new jira should be added to centralize version numbering, we now have the version number in 3 places in the source)","ZOOKEEPER-293","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-02-06 05:19:54.38","Closed","zoo_set needs to be abi compatible (3.1 changed the signature), fix this by adding zoo_set2","Bug","2009-02-14 05:18:54.594",0,0,7818,7818
16823,"2009-02-05 03:23:29.833","In 246 the old constants were deprecated and replace with enum. In the process usage of the orig constants was broken for switch statements, cases require compiletime constants.","ZOOKEEPER-291","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-02-05 04:01:27.661","Closed","regression for legacy code using KeeperException.Code constants (due to 246)","Bug","2009-02-14 05:18:54.562",0,0,7818,7818
16826,"2009-02-03 13:26:24.285","a user reported that a long running server, part of a 2 server ensemble, started using 100%cpu (1 server of the ensemble, the other was fine).mahadev tracked it down to a thread in the server running epoll in a tight loop - the thread was the nio server factory thread that selects on client fds.","ZOOKEEPER-287","Critical","ZOOKEEPER","ZooKeeper","ASF","Duplicate","2009-10-15 00:59:57.491","Closed","high cpu utilization caused by nioserver factory thread","Bug","2010-03-27 01:24:54.562",0,0,NULL,7818
16828,"2009-02-02 19:56:46.7","the stat command is always returning ""standalone"" it should return the actual mode it is in.","ZOOKEEPER-285","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-02-03 06:46:38.495","Closed","the stat command does not return the correct mode","Bug","2009-02-14 05:18:54.446",0,0,7829,7829
16829,"2009-02-02 19:55:40.928","The client port in JMX is always zero. it should be getting the client port from the cnxnfactory. it's a pretty easy fix. the really problem is that we have a setter for the clientPort as well, which is a bit more complicated to implement correctly. do we need a setter for the clientPort? i think we should make it a readonly attribute.","ZOOKEEPER-284","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-02-03 06:51:33.417","Closed","JMX doesn't get the clientPort correctly","Bug","2009-02-14 05:18:54.397",0,0,7818,7829
16832,"2009-01-29 22:44:23.832","> autoreconf -i -f -vautoreconf-2.63: Entering directory `.'autoreconf-2.63: configure.ac: not using Gettextautoreconf-2.63: running: aclocal --force configure.ac:21: error: AC_SUBST: `DX_FLAG_[]DX_CURRENT_FEATURE' is not a valid shell variable nameacinclude.m4:77: DX_REQUIRE_PROG is expanded from...acinclude.m4:117: DX_ARG_ABLE is expanded from...acinclude.m4:178: DX_INIT_DOXYGEN is expanded from...configure.ac:21: the top levelautom4te-2.63: /usr/bin/m4 failed with exit status: 1aclocal-1.10: autom4te failed with exit status: 1autoreconf-2.63: aclocal failed with exit status: 1>","ZOOKEEPER-281","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-03-06 06:34:24.589","Closed","autoreconf fails for /zookeeper-3.0.1/src/c/","Bug","2009-07-09 04:23:56.861",0,0,7818,8323
16837,"2009-01-15 23:18:00.44","I found an execution in which leader election does not make progress. Here is the problematic scenario:- We have an ensemble of 3 servers, and we start only 2;- We let them elect a leader, and then crash the one with lowest id, say S_1 (call the other S_2);- We restart the crashed server.Upon restarting S_1, S_2 has its logical clock more advanced, and S_1 has its logical clock set to 1. Once S_1 receives a notification from S_2, it notices that it is in the wrong round and it advances its logical clock to the same value as S_1. Now, the problem comes exactly in this point because in the current code S_1 resets its vote to its initial vote (its own id and zxid). Since S_2 has already notified S_1, it won't do it again, and we are stuck. The patch I'm submitting fixes this problem by setting the vote of S_1 to the one received if it satisfies the total order predicate (""received zxid"" is higher or ""received zxid is the same and received id is higher"").Related to this problem, I noticed that by trying to avoid unnecessary notification duplicates, there could be scenarios in which a server fails before electing a leader and restarts before leader election succeeds. This could happen, for example, when there isn't enough servers available and one available crashes and restarts. I fixed this problem in the attached patch by allowing a server to send a new batch of notifications if there is at least one outgoing queue of pending notifications empty. This is ok because we space out consecutive batches of notifications. ","ZOOKEEPER-275","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-01-30 07:11:41.724","Closed","Bug in FastLeaderElection","Bug","2009-02-14 05:18:54.307",0,0,7750,7750
16839,"2009-01-15 08:36:43.647","One should be able to build Zookeeper C client libs on a machine without CPPUNIT installation.A simple fix is to remove from configure.ac the following line:M_PATH_CPPUNIT(1.10.2)","ZOOKEEPER-273","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-01-27 01:56:45.748","Closed","Zookeeper c client build should not depend on CPPUNIT","Bug","2009-02-18 03:02:54.152",0,0,7818,8370
16840,"2009-01-14 02:15:32.799","Zookeeper allows creation of an abritrary number of children, yet if the String array of children names exceeds 4,194,304 bytes a getChildren will fail because ClientCnxn$SendThread.readLength() throws an exception on line 490.  Mahadev Konar questioned this byte limit's need.  In any case consistency of create children, get children should exist.","ZOOKEEPER-272","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-01-30 07:22:12.204","Closed","getChildren can fail for large numbers of children","Bug","2009-02-14 05:18:54.162",0,1,7827,8371
16844,"2009-01-08 02:05:01.266","Jute still causing problems with tostring operations on generated code, need to review/cleanup the toCSV codeFrom user Kevin Burton:---------------------------------------------Creating this node with this ACL:Created /foosetAcl /foo world:anyone:wCauses the exception included below.It's an infinite loop so it's just called over and over again filling myconsole.I'm just doing an exists( path, true ); ... setting a watch still causes theproblem.java.lang.NullPointerException        at org.apache.jute.Utils.toCSVBuffer(Utils.java:234)        atorg.apache.jute.CsvOutputArchive.writeBuffer(CsvOutputArchive.java:101)        atorg.apache.zookeeper.proto.GetDataResponse.toString(GetDataResponse.java:48)        at java.lang.String.valueOf(String.java:2827)        at java.lang.StringBuilder.append(StringBuilder.java:115)        atorg.apache.zookeeper.ClientCnxn$Packet.toString(ClientCnxn.java:230)        at java.lang.String.valueOf(String.java:2827)        at java.lang.StringBuilder.append(StringBuilder.java:115)        atorg.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:586)        atorg.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:626)        atorg.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:852)java.lang.NullPointerException        at org.apache.jute.Utils.toCSVBuffer(Utils.java:234)        atorg.apache.jute.CsvOutputArchive.writeBuffer(CsvOutputArchive.java:101)        atorg.apache.zookeeper.proto.GetDataResponse.toString(GetDataResponse.java:48)        at java.lang.String.valueOf(String.java:2827)        at java.lang.StringBuilder.append(StringBuilder.java:115)        atorg.apache.zookeeper.ClientCnxn$Packet.toString(ClientCnxn.java:230)        at java.lang.String.valueOf(String.java:2827)        at java.lang.StringBuilder.append(StringBuilder.java:115)        atorg.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:586)        atorg.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:626)        atorg.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:852)","ZOOKEEPER-268","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-01-27 03:28:37.283","Closed","tostring on jute generated objects can cause NPE","Bug","2009-02-14 05:18:54.013",0,0,7818,7818
16845,"2009-01-06 06:56:46.325","The java client (and someone should also review the c client) is generating a syncdisconnected even if the client is currently in the disconnected state. We saw this with a user running the java client against a down standalone server (server not running, connection refused) - the ""syncdisconnected"" is generated by the client lib each time a connection attempt (fails) is made. Should only be generated once.","ZOOKEEPER-267","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-01-28 08:24:25.802","Closed","java client incorrectly generating syncdisconnected event when in disconnected state","Bug","2009-02-14 05:18:53.987",0,0,7818,7818
16846,"2009-01-06 04:40:27.538","KeeperState is missing documentation of the states. Should provide some basic details and refer users to the appropriate forrest doc for more detail (most likely prog guide).","ZOOKEEPER-266","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-01-09 05:22:25.566","Closed","KeeperState missing javadoc for values","Bug","2009-02-14 05:18:53.961",0,0,7818,7818
16885,"2008-11-19 03:50:16.637","Recent gcc compilers issue warnings when function declarations for functions with no arguments don't specific ""void"".  The attached patch fixes one such warning for create_buffer_oarchive() in recordio.h.","ZOOKEEPER-227","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-11-20 04:54:19.895","Closed","gcc warning from recordio.h","Bug","2009-02-14 05:18:50.199",0,0,8348,8348
16854,"2008-12-17 06:50:09.224","The docs incorrectly state the max client timeout as 60 seconds.http://hadoop.apache.org/zookeeper/docs/r3.0.1/zookeeperProgrammers.html#ch_zkSessionsthe current server code has the following logic:        if (sessionTimeout < zk.tickTime * 2) {            sessionTimeout = zk.tickTime * 2;        }        if (sessionTimeout > zk.tickTime * 20) {            sessionTimeout = zk.tickTime * 20;        }So really the docs should say max is 20*tickTime","ZOOKEEPER-258","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-01-09 05:41:13.749","Closed","docs incorrectly state max client timeout as 60 seconds (it's based on server ticktime)","Bug","2009-02-14 05:18:53.765",0,0,7818,7818
16855,"2008-12-16 08:49:00.334","Some of the WARN/ERROR log messages are incorrectly leveled (too high - many often should be INFO/DEBUG)","ZOOKEEPER-257","Major","ZOOKEEPER","ZooKeeper","ASF","Duplicate","2009-01-07 08:22:53.391","Closed","Review logging level for WARN/ERROR log messages, some misclassified","Bug","2009-02-14 05:18:53.73",0,0,7818,7818
16857,"2008-12-13 07:18:21.067","the zoo_set() api does not return the stat datastructure. the java counterpart returns a stat with set api. ","ZOOKEEPER-255","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-12-23 05:40:29.546","Closed","zoo_set() api does not return stat datastructure.","Bug","2009-02-14 05:18:53.617",0,0,8326,7827
16858,"2008-12-13 05:44:22.946","There's currently no way for a user to test session expiration in their code.We don't  have any unit/system tests that verify our code handles session expiration properly.There should be a way to test session expiration.I did notice that we have the ability to terminate a session using JMX mbean interface, however I'm not sure if this is useful in an automated testing context. Even if it is we should provide a wrapper for testing purposes - and add tests to our codebase which uses it.","ZOOKEEPER-254","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-01-23 06:39:13.543","Closed","there is currently no way for a user to test session expiration in their code","Bug","2009-02-14 05:18:53.586",0,0,7829,7818
16886,"2008-11-19 03:27:26.721","An NPE will be generated on the server (and resulting in client getting MarshallingException) if exists() is called on a node with null data.workaround is to create the node with non-null data.be sure to update the tests for this case","ZOOKEEPER-226","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-11-19 07:46:52.84","Closed","exists calls fails on server if node has null data","Bug","2009-02-14 05:18:50.174",0,0,7827,7818
16859,"2008-12-12 02:45:28.52","the example code has           case SyncConnected:               // Everything is happy. Lets kick things off               // again by checking the existence of the znode               break;this is misleading - it should indicate that the watches are automatically reset and therefor no call to exists is necessaryalso fix this in the same doc (looks like its old detail, no longer valid), indicate that autoreset will happen on reconnect.If the client-side ZooKeeper libraries can reestablish the communication channel to ZooKeeper, DataMonitor simply kicks everything off again with the call to ZooKeeper.exists(). If it gets an event for a znode, it calls ZooKeeper.exists() to find out what has changed. ","ZOOKEEPER-253","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-01-28 08:47:31.856","Closed","documentation of DataWatcher state transition is misleading regarding auto watch reset on reconnect","Bug","2009-02-14 05:18:53.539",1,1,7818,7818
16860,"2008-12-11 07:06:29.551","org.apache.zookeeper.server.PurgeTxnLog class has not been updated to handle the new directory structure imposed by the upgrade from v2 to v3 of ZooKeeper. In particular the dataDir now has a ""version-2"" subdirectory that stores all of the snaps/transactionallogs for version2 of the persistence layer.I also note that the documentation of this class is particularly poor. I'm working on ZOOKEEPER-229 and would like to point to the API docs for this class regarding usage but they api docs are nonexistentAlso - I think it's important for the user to be able to specify the number of ""backup"" snaps and logs that should be kept -- right now it seems we delete all but the current snaps/txlogs. Either by count or by date -- ie ""remove anything 5 days or older, with a minum of 3 most recents snaps (and accompanying txlogs)"" seems like a pretty common user case (assuming the operator is doing system backups every X days, etc...)in general this class needs some tlc - the formatting should also be cleaned up.Also - the API docs for this and LogFormatter are not included in the build.xml ""javadoc"" target. These are user utilities so javadoc for these two classes should be included. I will fix this issue as part of ZOOKEEPER-229. I'm also updateing the forrest documention in 229 so don't worry about that either.","ZOOKEEPER-252","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-01-31 09:19:11.386","Closed","PurgeTxnLog is not handling the new dataDir directory structure","Bug","2009-02-14 05:18:53.513",0,0,7827,7818
16861,"2008-12-09 05:29:01.013","See the following thread for the original report:http://mail-archives.apache.org/mod_mbox/hadoop-zookeeper-user/200812.mbox/browserSteps to reproduce:1) Start a replicated zookeeper service consisting of 3 zookeeper (3.0.1) servers all running on the same host (of course, all using their own ports and log directories)2) Create one znode in this ensemble (using the zookeeper client console, I issued 'create /node1 node1data').3) Stop, then restart a single zookeeper server; moving onto the next one a few seconds later. 4) Go back to 3. After 4-5 iterations, the following should occur, with the failing server exiting:java.lang.NullPointerException        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:447)        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:358)        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:333)        at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:250)        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:102)        at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:183)        at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:245)        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:421)2008-12-08 14:14:24,880 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:2183:Leader@336] - Shutdown calledjava.lang.Exception: shutdown Leader! reason: Forcing shutdown        at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:336)        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:427)Exception in thread ""QuorumPeer:/0:0:0:0:0:0:0:0:2183"" java.lang.NullPointerException        at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:339)        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:427)The inputStream field is null, apparently because next is being called at line 358 even after next returns false. Having very little knowledge about the implementation, I don't know if the existence of hdr.getZxid()  >= zxid is supposed to be an invariant across all invocations of the server; however the following change to FileTxnLog.java seems to make the problem go away.diff FileTxnLog.java /tmp/FileTxnLog.java358c358,359<                 next();--- >               if (!next()) >                   return;447c448,450<                 inputStream.close();--- >               if (inputStream != null) { >                   inputStream.close(); >               }","ZOOKEEPER-251","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-12-11 05:55:46.963","Closed","NullPointerException stopping and starting Zookeeper servers","Bug","2009-02-14 05:18:53.47",0,1,7827,8373
16862,"2008-12-09 02:11:32.295","the isvalidsnapshot will fail with negative seek if 0 snapshot files exist. We should just return false in case the size of the snapshot is less than 5 bytes.","ZOOKEEPER-250","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-12-12 05:17:21.182","Closed","isvalidsnapshot should handle the case of 0 snapshot files better.","Bug","2009-02-14 05:18:53.377",0,0,7827,7827
16863,"2008-12-08 17:10:31.923","This was changed in SVN 700690:http://svn.apache.org/viewvc/hadoop/zookeeper/trunk/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java?r1=700690&r2=700689&pathrev=700690","ZOOKEEPER-249","Trivial","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-12-09 13:10:11.994","Closed","QuorumPeer.getClientPort() always returns -1","Bug","2009-02-14 05:18:50.905",0,0,8352,8352
16865,"2008-12-06 02:22:48.637","formatting of C api is wrong in acl section of prog guide","ZOOKEEPER-247","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-12-10 08:42:29.586","Closed","fix formatting of C API in ACL section of programmer guide","Bug","2009-02-14 05:18:50.784",0,0,7818,7818
16867,"2008-12-05 05:08:20.01","Owen O'Malley mentioned:---------we need to change both the README and quick start to assume you are working with a release instead of a source tarball. Apache releases are the approved way of getting the project. Documentation that assumes they are getting source themselves doesn't reflect that. ---------","ZOOKEEPER-245","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-12-10 08:22:34.458","Closed","update readme/quickstart to be release tar, rather than source, based","Bug","2009-02-14 05:18:50.714",0,0,7818,7818
16868,"2008-12-04 05:18:19.214","Attaching output log file.","ZOOKEEPER-244","Minor","ZOOKEEPER","ZooKeeper","ASF","Cannot Reproduce","2009-05-20 06:22:55.071","Closed","AsyncOpsTest fails when running consecutively","Bug","2009-07-09 04:23:56.633",0,0,NULL,7750
16871,"2008-11-27 03:50:08.315","When the ZooKeeper distro is downloaded, if the clean target is run, it deletes the .revision file:{quote}clean:  [delete] Deleting directory /home/foo/zookeeper-3.0.1/build  [delete] Deleting directory /home/foo/zookeeper-3.0.1/src/java/generated  [delete] Deleting directory /home/foo/zookeeper-3.0.1/src/c/generated  [delete] Deleting directory /home/foo/zookeeper-3.0.1/.revision{quote}This causes subsequent builds in the distro to fail with:{quote}compile-main:   [javac] Compiling 73 source files to /home/foo/zookeeper-3.0.1/build/classes   [javac] /home/foo/zookeeper-3.0.1/src/java/main/org/apache/zookeeper/Version.java:21: package org.apache.zookeeper.version does not exist   [javac] public class Version implements org.apache.zookeeper.version.Info{   [javac]                                                             ^   ...{quote}","ZOOKEEPER-241","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-12-10 06:16:28.117","Closed","Build of a distro fails after clean target is run","Bug","2009-02-14 05:18:50.585",0,0,7818,8221
16872,"2008-11-27 00:26:54.095","java.lang.NullPointerException	at org.apache.jute.Utils.toCSVBuffer(Utils.java:234)	at org.apache.jute.CsvOutputArchive.writeBuffer(CsvOutputArchive.java:101)	at org.apache.zookeeper.proto.GetDataResponse.toString(GetDataResponse.java:48)	at java.lang.String.valueOf(String.java:2827)	at java.lang.StringBuilder.append(StringBuilder.java:115)	at org.apache.zookeeper.ClientCnxn$Packet.toString(ClientCnxn.java:230)	at java.lang.String.valueOf(String.java:2827)	at java.lang.StringBuilder.append(StringBuilder.java:115)	at org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:586)	at org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:626)	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:852)","ZOOKEEPER-240","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-01-14 08:56:44.975","Closed","Yet another npe","Bug","2009-02-14 05:18:50.515",0,0,7818,7750
16874,"2008-11-25 05:31:13.606","I think the way the HostAuthenticationProvider is implemented could cause serious performance problems if DNS is slow or broken. The problem is that we need to do a reverse hostname resolution during connection establishment. I suggest it be removed.","ZOOKEEPER-238","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-08-27 08:05:10.922","Closed","HostAuthenicationProvider should be removed","Bug","2009-09-06 06:36:18.824",0,0,7829,7829
16884,"2008-11-19 04:02:06.38","one of the test files is missing apache headers ... ","ZOOKEEPER-228","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-11-19 04:13:48.977","Closed","apache header missing in DBtest","Bug","2009-02-14 05:18:50.225",0,0,7827,7827
16903,"2008-10-28 07:22:00.361","The quoruom servers throw a nullpointer exception and still keep running. We should atleast have a nice debug message and quit... ","ZOOKEEPER-209","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-11-19 01:25:27.154","Closed","nullpointerexception if election port is not specified.","Bug","2013-05-02 10:29:17.572",0,0,7750,7827
16891,"2008-11-13 07:21:29.647","the following code is missing conditional log call based on result of remove call (size > 0)    // XXX This shouldn't be needed, but just in case>     synchronized (existWatches) {>         addTo(existWatches.remove(path), result);>         LOG.warn(""We are triggering an exists watch for delete! Shouldn't happen!"");>     }> ","ZOOKEEPER-221","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-11-19 18:24:43.038","Closed","log message in ZkWatchManager.materialize missing conditional","Bug","2009-02-14 05:18:49.995",0,1,7818,7818
16893,"2008-11-12 06:01:12.233","in watchertest there are some event.poll calls that have 1milli timeouts        e = localWatcher.events.poll(1, TimeUnit.MILLISECONDS);this is showing falure in some cases under hudson (I assume when it's under load from other tests running for other proj)We should review the poll calls and verify adequate timeouts.","ZOOKEEPER-219","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-11-18 08:13:06.531","Closed","events.poll timeout in watcher test too short","Bug","2009-02-14 05:18:49.927",0,0,7818,7818
16894,"2008-11-11 06:24:08.04","Report from user Murali Vemulapati. The wiki recipe page also should be updated to handle multiple clients using the barrier on the same system (multiple processes, etc...). Currently the docs/example support only a single user of the barrier per host.------------------I believe there is a typo in the barrier example given at:http://hadoop.apache.org/zookeeper/docs/current/zookeeperTutorial.htmlWith the following fix, the program runs as expected:==============83c83<                 this.name = new String(InetAddress.getLocalHost().getCanonicalHostName().toString());--->                 name = new String(InetAddress.getLocalHost().getCanonicalHostName().toString());100c100<                     CreateMode.EPHEMERAL_SEQUENTIAL);--->                     CreateMode.EPHEMERAL);==============The first change assigns the name to the instance variable 'name' of Barrier class (otherwise the 'name' instance variable will have a value of 'null'when calling zk.create to create the child node under the root barrier node).The second change lets us run multiple processes on the same machine.thanksmurali","ZOOKEEPER-218","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-11-19 07:14:13.079","Closed","error in barrier recipe example code","Bug","2009-02-14 05:18:49.852",0,0,7818,7818
16899,"2008-10-29 14:02:37.807","seehttp://hadoop.apache.org/zookeeper/docs/current/zookeeperProgrammers.html#Zookeeper+C+client+APIand compare with src/c/zookeeper.h","ZOOKEEPER-213","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-11-18 07:51:11.218","Closed","programmer guide C api docs are out of sync with latest zookeeper.h ","Bug","2009-02-14 05:18:49.704",0,0,7818,7818
16900,"2008-10-29 05:54:57.444","the snapshot in 3.0 is syunchronous. this will affect performance of the system.","ZOOKEEPER-212","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-11-18 07:22:13.674","Closed","snapshot is synchronous in 3.0","Bug","2009-02-14 05:18:49.677",0,0,7827,7827
16904,"2008-10-27 13:31:11.364","The Zookeeper C client library uses gethostbyname and strtok, both of which are not safe to use from multiple threads.The problem is resolved by using getaddrinfo and strtok_r in place of the older API.","ZOOKEEPER-208","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-11-20 04:49:20.865","Closed","Zookeeper C client uses API that are not thread safe, causing crashes when multiple instances are active","Bug","2009-02-14 05:18:49.473",0,0,8228,8228
16906,"2008-10-24 03:20:56.803","Feedback from Doug Cutting on 3.0.0 documentation: - The ""Zookeeper Documentation"" tab should contain the version number. - ""Informal Documentation"" might better be named ""Other Documentation"". - The ""Other Info"" page should be removed, since it contains nothing. ","ZOOKEEPER-206","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-11-19 07:36:02.218","Closed","documentation  tab should contain the version number (and other small site changes)","Bug","2009-02-14 05:18:49.448",0,0,7818,7818
16907,"2008-10-23 05:46:32.175","When running ant with the code from the release tarball, I get the following messages and stack trace:{noformat}svn-revision:     [exec] svn: '.' is not a working copyversion-info:     [java] All version-related parameters must be valid integers!     [java] Exception in thread ""main"" java.lang.NumberFormatException: For input string: """"     [java]     at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)     [java]     at java.lang.Integer.parseInt(Integer.java:468)     [java]     at java.lang.Integer.parseInt(Integer.java:497)     [java]     at org.apache.zookeeper.version.util.VerGen.main(VerGen.java:111)     [java] Java Result: 1{noformat}This seems to be because the code is not coming from svn, so it can't find the version information.","ZOOKEEPER-205","Minor","ZOOKEEPER","ZooKeeper","ASF","Duplicate","2008-12-06 03:48:46.043","Closed","Error on version-info when compiling from the tarball distribution","Bug","2009-02-14 05:18:49.402",0,0,7827,7750
16908,"2008-10-22 14:06:58.57","When the ZooKeeper java client  makes a connection it queues a SetWatches  request. The problem is that it puts the request at the end of the outgoing requests. This means that watches for requests that were queued before the connection is made and after the disconnect may improperly get their watches triggered.","ZOOKEEPER-204","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-11-19 18:23:06.108","Closed","SetWatches needs to be the first message after auth messages to the server","Bug","2009-02-14 05:18:49.349",0,0,7829,7829
16909,"2008-10-22 11:43:50.686","typo in releasenotes note on datalog/data dirs.","ZOOKEEPER-203","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-22 11:47:24.291","Closed","fix datadir typo in releasenotes","Bug","2008-10-26 09:10:44.64",0,0,7818,7818
16910,"2008-10-22 05:24:55.487","One of our users has observed that an ephemeral znode had gone away once its creator had disconnected according to the leader, but one follower believed that it existed long after the znode had been deleted. Apparently the follower was never going to delete it. Because the leader wouldn't recognize the znode as an existing one, any attempt to delete the znode failed.  We have to investigate if this is related to any known bug, although, to my knowledge, this is the first time it happens. It is important to note that the user was running an older version of our code.","ZOOKEEPER-202","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-23 01:17:44.502","Closed","Phantom ephemeral node","Bug","2010-03-27 01:28:11.532",0,0,7750,7750
16911,"2008-10-21 12:55:16.77","The snapshot and transaction log files are not validating the magic numbers when read.Mahadev, can you update the code and tests for this? Possible for 3.0 or wait post 3.0? (feel free to fix now or reassign version)Please add tests for this.","ZOOKEEPER-201","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-22 02:55:54.325","Closed","validate magic number when reading snapshot and transaction logs","Bug","2008-10-26 09:10:44.615",0,0,7827,7818
16912,"2008-10-21 12:39:07.798","the magic number for the snapshot and transaction logs are currently the same - they should be different. Also the magic numbers should also be more indicative of the type of file (currently ""AK47"" for both, not very useful in determining type of file)","ZOOKEEPER-200","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-22 01:30:36.675","Closed","the magic number for snapshot and log must be different (currently same)","Bug","2008-10-26 09:10:44.568",0,0,7818,7818
16913,"2008-10-21 07:59:08.321","there are 2 log messages during server startup that are misleading:2008-10-20 16:36:41,135 - INFO  [main:FileTxnLog$FileTxnIterator@441] - EOF excepton java.io.EOFException: Failed to read2008-10-20 16:36:41,120 - ERROR [main:FileTxnSnapLog@114] - 2(higestZxid) >= 2(next log) for type 1the first log should be DEBUG and should be changed to say something like eof reached in <file>, reading next filewhile the second log seems to indicate an error - however the server is still starting. either this is not an error, or it is an error and it's not being handled correctly.","ZOOKEEPER-199","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-21 12:36:49.806","Closed","fix log messages in persistence code","Bug","2008-10-26 09:10:44.543",0,0,7827,7818
16914,"2008-10-21 04:46:36.347","add license to file & run the rat tool to verify svn","ZOOKEEPER-198","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-21 05:09:01.126","Closed","apache license header missing from FollowerSyncRequest.java","Bug","2008-10-26 09:10:44.486",0,0,7818,7818
16931,"2008-10-09 23:45:23.063","Some Source Forge Documents did not get moved over: javaExamplezookeeperTutorial(These two I will move over directly) zookeeperLoggingzookeeperAtomicBroadcast(These two I will roll up into a new document: zookeeperInternals)We can expand zookeeperInterals as new sections are needed, maybe one day morphing it into the ""complete guide for zookeeper contributers""","ZOOKEEPER-181","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-10 21:58:38.709","Closed","Some Source Forge Documents did not get moved over: javaExample, zookeeperTutorial, zookeeperInternals","Bug","2008-10-26 09:10:43.593",0,0,7818,8377
16916,"2008-10-20 20:12:56.64","In zookeeper.h: * \param state connection state. If the type is ZOO_SESSION_EVENT, the state value  * will be one of the *_STATE constants, otherwise -1.but for this sequence: 1. zoo_awexists(name) 2. zoo_acreate(name)we've got a watcher callback with type=ZOO_CREATED_EVENT and state!=-1I think the comment should be altered to underline the difference between zookeeper_init() callback usage and others (""the getter API functions with the ""w"" prefix in their names"") for the new ""watcher object"" style.It looks like the type and path argument values are useless for the former (because type is always ZOO_SESSION_EVENT, and path is always empty), and the state is useless for the latter (it is considered to be -1).And more,  the state of the legacy style should be commented - will it be marked as obsolete? Or will it be supported in the future?I wonder if there are any plans to split current watcher_fn callback to something like:1. new watcher_fn: typedef void (*watcher_fn)(zhandle_t *zh, int type, const char *path, void *watcherCtx);2. connection_fn: typedef void (*watcher_fn)(zhandle_t *zh, int state, void *context);Because, you see, the usage is different and there is no any common set of arguments apart from zh (which is common for API) and context.","ZOOKEEPER-196","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-06-09 05:31:14.344","Closed","doxygen comment for state argument of watcher_fn typedef and implementation differ (""...one of the *_STATE constants, otherwise -1"")","Bug","2009-07-09 04:23:56.48",0,0,7829,8323
16919,"2008-10-16 08:14:40.072","In particular the pkg structure has changed.","ZOOKEEPER-193","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-17 04:09:46.303","Closed","update java example doc to compile with latest zookeeper","Bug","2008-10-26 09:10:44.221",0,0,7818,7818
16920,"2008-10-16 06:49:54.477","a config file with trailing whitespace can cause number format exceptionfor example a line such asclientPort=2181where 2181 is followed by a space character, will fail to parse with number format excetion ""2181 "" (notice the space).We need to trim whitespace when parsing numbers","ZOOKEEPER-192","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-05-23 03:02:44.509","Closed","trailing whitespace in config file can cause number format exceptions","Bug","2009-07-09 04:23:56.415",0,0,7818,7818
16922,"2008-10-15 07:42:35.097","Right now, on the main documentation page, and also in the left-hand nav, the docs are in a single flat list. Let's reorganize them into related groups.  Maybe, for example, something like this:Overview  - Overview  - Getting StartedDeveloper Docs- Programmer's Guide- Recipes- Java Example- Barrier and Queues Tutorial- APIAdministrtor Docs- Administrators GuideContributor Docs- Zookeeper InternalsMiscellaneous- FAQ- Wiki- Other Information","ZOOKEEPER-190","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-15 12:05:50.1","Closed","Reorg links to docs and navs to docs into related sections","Bug","2008-10-26 09:10:44.148",0,0,7818,8377
16923,"2008-10-15 06:39:52.587","It's possible to build the documentation without validating the XML of the source document. ","ZOOKEEPER-189","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-15 07:45:36.324","Closed","forrest build not validated xml of input documents","Bug","2008-10-26 09:10:44.122",0,0,7818,8377
16925,"2008-10-15 02:52:10.614","End user API docs are missing for CreateMode. (dev api docs are fine)","ZOOKEEPER-187","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-16 00:17:55.189","Closed","CreateMode api docs missing","Bug","2008-10-26 09:10:44.052",0,0,7818,7818
16928,"2008-10-13 19:52:48.576","Some compilation environments provide implicit inclusion of certain system headers.But any way it's not a reason to exploit it in platform-independent projects.TestHashtable.cc and LibCMocks.h from src/c/tests/ use those functions without including corresponding system headers.Modern versions of GCC are very strict.You cannot build the code like this with the help of GCC version 4.3.","ZOOKEEPER-184","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-15 04:29:23.166","Closed","tests: An explicit include derective is needed for the usage of memcpy(), memset(), strlen(), strdup() and free() functions.","Bug","2008-10-26 09:10:43.93",0,0,8323,8323
16929,"2008-10-11 00:05:24.415","Having:        char buf[4096*16];Present:        buf[sizeof(buf)]=0;Must be:        buf[sizeof(buf)-1]=0;","ZOOKEEPER-183","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-15 04:17:47.177","Closed","Array subscript is above array bounds in od_completion(), src/cli.c.","Bug","2008-10-26 09:10:43.683",0,0,8323,8323
16930,"2008-10-10 23:49:42.022","Please, add this test to src/c/tests/TestZookeeperInit.cc to reproduce this:    void testEmptyAddressString()    {        zh=zookeeper_init("""",0,0,0,0,0);        CPPUNIT_ASSERT(zh==0);    }","ZOOKEEPER-182","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-15 04:10:19.068","Closed","zookeeper_init accepts empty host-port string and returns valid pointer to zhandle_t.","Bug","2008-10-26 09:10:43.634",0,0,8323,8323
16953,"2008-10-06 21:15:47.573","In the default implementation of leader election, there are two undesirable cases that need to be covered:1- If there is a leader elected and this leader is supported by at least  quorum of peers, then it can happen that one peer disconnects from the leader, and initiates a new leader election. As it is a new leader election, we increment the logical clock of this peer, and according to the current implementation, this peer won't accept any vote from a peer with a lower LE turn (corresponds to the value of the logical clock of the voting peer). The attached patch corrects this problem by allowing a peer to go back to a previous epoch in the case a majority votes for a leader, and the peer also receives a vote from the leader. This feature allows a peer to correct a false suspicion of the current leader;2- If a peer  advances to a new turn before others, then it may end up voting for a peer that either does not have the highest server id or the the highest zxid. The attached patch fixes this problem by resetting the vote a peer when it updates its logical clock upon receiving a notification with a higher turn value.","ZOOKEEPER-159","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-09 02:00:53.507","Closed","Cover two corner cases of leader election","Bug","2008-10-26 09:10:43.435",0,0,7750,7750
17004,"2008-08-01 10:48:45.706","The current implementation of sync is broken. There is a race condition that causes a follower to return operations out of order, causing clients to drop their connections to a server.I'll be attaching a patch to fix this problem shortly.","ZOOKEEPER-108","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-08-09 13:02:52.437","Closed","sync implementation reorders operations","Bug","2008-10-26 09:10:41.712",0,0,7750,7750
16955,"2008-10-05 06:20:35.858","In the patch of JIRA 127, I forgot to set the state of a peer when this peer is looking for a leader and it receives a message from the current leader. In this patch, I have fixed this problem, and also returned to what we had previously. With this current patch, when a peer joins and there is already a leader elected, the joining peer will only recognize the new leader as the leader once it receives a confirmation from a majority. The alternative is to set the leader once we receive a message from a peer claiming to be the leader (what we have on trunk now, although broken because we don't set the state of the peer), but there could be cases in which a peer believes to be leader, although it is not the leader any longer, and the joining peer would select this false leader to be its leader. Eventually, the false leader would timeout, and both processes would select the correct leader. This small fix gets rid of such problems, though.","ZOOKEEPER-157","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-05 19:11:35.998","Closed","Peer can't find existing leader","Bug","2008-10-26 09:10:43.382",0,0,7750,7750
16957,"2008-10-02 21:28:03.438","In ""the zookeeper project"" section ofhttp://hadoop.apache.org/zookeeper/docs/current/zookeeperOver.htmlwe should remove the hod reference (doesn't use zk) and also update the second paragraph, perhaps remove? Since we are already on the apache zk page.","ZOOKEEPER-155","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-04 07:26:47.793","Closed","improve ""the zookeeper project"" section of overview doc","Bug","2008-10-26 09:10:43.331",0,0,7818,7818
16958,"2008-10-02 21:20:45.391","from question on the user mailing list:the ""Reliability in the Presence of Errors"" graph on http://hadoop.apache.org/zookeeper/docs/current/zookeeperOver.html does not list how many ZooKeeper quorum nodes are in use, or the fraction of reads/writes.ben mentioned:Here is the missing text:To show the behavior of the system over time as failures are injected weran a ZooKeeper service made up of 7 machines. We ran the samesaturation benchmark as before, but this time we kept the writepercentage at a constant 30\%, which is a conservative ratio of ourexpected workloads.","ZOOKEEPER-154","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-04 07:27:05.969","Closed","reliability graph diagram in overview doc needs context","Bug","2008-10-26 09:10:43.291",0,0,7818,7818
16961,"2008-10-02 04:23:05.126","The patch of jira 127 changed the format of server configuration files, but it didn't change the documentation. ","ZOOKEEPER-151","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-17 05:55:42.628","Closed","Document change to server configuration","Bug","2008-10-26 09:10:43.253",0,0,7750,7750
16962,"2008-10-02 01:28:34.343","the build is broekn with ZOOKEEPER-38 checked in.","ZOOKEEPER-150","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-02 03:06:15.031","Closed","zookeeper build broken","Bug","2008-10-26 09:10:43.227",0,0,7827,7827
16972,"2008-09-13 18:02:12.317","Frequently the servers deadlock in QuorumCnxManager:initiateConnection ons.read(msgBuffer) when reading the challenge from the peer.Calls to initiateConnection and receiveConnection are synchronized, so only one or the other can be executing at a time. This prevents two connections from opening between the same pair of servers.However, it seems that this leads to deadlock, as in this scenario:{noformat}A (initiate --> B)B (initiate --> C)C (initiate --> A){noformat}initiateConnection can only complete when receiveConnection runs on the remote peer and answers the challenge. If all servers are blocked in initiateConnection, receiveConnection never runs and leader election halts.","ZOOKEEPER-140","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-01 17:21:19.212","Closed","Deadlock in QuorumCnxManager","Bug","2008-10-26 09:10:43.012",1,1,7750,7750
16974,"2008-09-09 05:03:47.9","if a single watcher (A) is registered on a single node for both a getdata and exists watch the second watch event may be lost:1) getdata(""node"", A)2) setdata(""node""...)3) exists(""node"", A)4) delete(""node""...)if watch events for 2 is processed on the client (zookeeper.java, zkwatcher) after 3 completes then the zkwatcher process event method will clear the watch and the subsequent operation's (4) event will be ignored","ZOOKEEPER-138","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-22 09:23:13.858","Closed","c client watcher objects can lose events","Bug","2008-10-26 09:10:42.874",0,0,7829,7818
16975,"2008-09-09 05:02:48.333","if a single watcher (A) is registered on a single node for both a getdata and exists watch the second watch event may be lost:1) getdata(""node"", A)2) setdata(""node""...)3) exists(""node"", A)4) delete(""node""...)if watch events for 2 is processed on the client (zookeeper.java, zkwatcher) after 3 completes then the zkwatcher process event method will clear the watch and the subsequent operation's (4) event will be ignored","ZOOKEEPER-137","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-09-25 05:05:52.297","Closed","client watcher objects can lose events","Bug","2008-10-26 09:10:42.572",0,0,7818,7818
16976,"2008-09-06 02:27:46.086","The attached test causes all of the followers of a quorum to hang. Leader continues to function correctly.","ZOOKEEPER-136","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-03 21:12:35.381","Closed","sync causes hang in all followers of quorum","Bug","2008-10-26 09:10:42.542",0,0,7829,7818
16979,"2008-09-04 04:46:12.674","There is a bug in the ClientTest.java unit test, timing issue in ""withWatcherObj"" test. Patch forthcoming.","ZOOKEEPER-133","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-09-05 22:39:54.361","Closed","hudson tests failing intermittently","Bug","2008-10-26 09:10:42.461",0,0,7818,7818
16981,"2008-09-03 01:27:07.647","I think there is a race condition that is probably easy to get into with the old leader election and a large number of servers:1) Leader dies2) Followers start looking for a new leader before all Followers have abandoned the Leader3) The Followers looking for a new leader see votes of Followers still following the (now dead) Leader and start voting for the dead Leader4) The dead Leader gets reelected.For the old leader election a server should not vote for another server that is not nominating himself.","ZOOKEEPER-131","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-09-19 09:25:22.448","Closed","Old leader election can elect a dead leader over and over again","Bug","2008-10-26 09:10:42.382",0,1,7829,7829
16985,"2008-08-27 21:18:34.883","In QuorumCnxManager.toSend there is a call to create a connection as follows:    channel = SocketChannel.open(new InetSocketAddress(addr, port));Unfortunately ""addr"" is the ip address of a remote server while ""port"" is the electionPort of *this* server.As an example, given this configuration (taken from my zoo.cfg)  server.1=10.20.9.254:2881  server.2=10.20.9.9:2882  server.3=10.20.9.254:2883Server 3 was observed trying to make a connection to host 10.20.9.9 on port 2883 and obviously failing.In tests where all machines use the same electionPort this bug would not manifest itself.","ZOOKEEPER-127","Critical","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-01 16:40:15.366","Closed","Use of non-standard election ports in config breaks services","Bug","2009-11-12 23:23:30.842",0,2,7750,991
16988,"2008-08-25 09:00:45.638","StatCallback appears to be broken in trunk. I'll attach a patch for AsyncTest that triggers the behaviour.","ZOOKEEPER-124","Major","ZOOKEEPER","ZooKeeper","ASF","Invalid","2008-09-27 02:02:33.605","Closed","StatCallback is broken in trunk","Bug","2008-10-26 09:10:42.171",0,1,NULL,7992
16990,"2008-08-19 08:05:24.976","java.lang.NullPointerException        at org.apache.jute.Utils.toCSVString(Utils.java:128)        at org.apache.jute.CsvOutputArchive.writeString(CsvOutputArchive.java:94)        at org.apache.zookeeper.proto.WatcherEvent.toString(WatcherEvent.java:60)        at java.lang.String.valueOf(String.java:2827)        at java.lang.StringBuilder.append(StringBuilder.java:115)        at com.liveoffice.mailindex.watchers.SuicidalWatcher.process(SuicidalWatcher.java:11)        at org.apache.zookeeper.ZooKeeper.processWatchEvent(ZooKeeper.java:157)        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:268)","ZOOKEEPER-122","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-08-20 02:01:28.783","Closed","NPE in jute's Utils.toCSVString","Bug","2008-10-26 09:10:42.1",0,0,8224,8224
16991,"2008-08-13 02:57:23.342","The SyncRequestProcessor is not closing log stream during shutdown. See FIXMEs with this ID in ClientBase.java -- I've commented out the assertion for the time being (checking for logs being deleted), as part of this fix re-enable these asserts and also verify tests on a Windows system.","ZOOKEEPER-121","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-17 06:39:59.054","Closed","SyncRequestProcessor is not closing log stream during shutdown","Bug","2011-06-24 12:53:24.394",0,0,7827,7818
16994,"2008-08-12 02:07:56.473","followerrequestprocessor:is the case statement for SYNC supposed to fall through?                switch (request.type) {                case OpCode.sync:                    zks.pendingSyncs.add(request);                case OpCode.create:Please update the docs/code appropriately (if correct note it with comment just after the sync case statement.","ZOOKEEPER-118","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-09-10 01:56:48.848","Closed","findbugs flagged switch statement in followerrequestprocessor.run()","Bug","2008-10-26 09:10:42.055",0,0,7750,7818
16995,"2008-08-12 02:04:32.229","Leader.lead() creates a new thread that can never terminate (short of restarting vm)naked notifyall in lead() method - what is the condition variable? Best if set inside the sync block","ZOOKEEPER-117","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-09-25 05:28:09.943","Closed","threading issues in Leader election","Bug","2008-10-26 09:10:41.999",0,1,7750,7818
16997,"2008-08-08 05:10:41.986","Findbugs flagged this, notice that we are checking for null after using the reference.               if (senderWorkerMap.get(s.socket().getInetAddress()) != null) {                    senderWorkerMap.get(s.socket().getInetAddress()).finish();                }                /*                 * Start new worker thread with a clean state.                 */                if (s != null) {","ZOOKEEPER-115","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-13 22:11:02.979","Closed","Potential NPE in QuorumCnxManager","Bug","2008-10-26 09:10:41.957",0,1,7750,7818
17000,"2008-08-07 05:01:27.792","src/java/main ZooKeeper.java has a method ""public void disconnect()"" that is not part of the public api but put there for testing purposes (to test disconnection of the client from the server w/o actually shutting down the session)This method needs to be moved out of the public api. preferably we should have a subclass in the test code itself that provides this method.","ZOOKEEPER-112","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-09-06 04:31:16.242","Closed","src/java/main ZooKeeper.java has test code embedded into it.","Bug","2008-10-26 09:10:41.876",0,0,7818,7818
17002,"2008-08-06 02:18:25.111","The current build.xml ant script uses svnant to obtain the latest revision number from the repo, however svnant is not compatible with subversion 1.5 (http://subversion.tigris.org/svn_1.5_releasenotes.html), and so the build fails with working copies checked out by this version.  The build fails with ""this version of subversion is too old, please get a newer version...""  This will become more apparent as svn 1.5 trickles out; I'm using a brand new dev environment with both subclipse 1.4 and svn 1.5 client, so I got bit rather quickly.Those with svn 1.5 can get the code from the trunk, but cannot do an ant build.svnant hasn't been updated in more than a year and appears to be dead, so it may no longer be a viable tool for the ant build.","ZOOKEEPER-110","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-08-16 02:01:42.308","Closed","Build script relies on svnant, which is not compatible with subversion 1.5 working copies","Bug","2008-10-26 09:10:41.801",0,2,8066,8066
17007,"2008-07-26 11:27:58.323","The ZooKeeper java client main loop crashes on KeeperExceptions.  They should be handled when possible.","ZOOKEEPER-105","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-07-29 02:21:13.14","Closed","ZooKeeper java client main loop crashes on KeeperExceptions","Bug","2008-10-26 09:10:41.624",0,0,8224,8224
17026,"2008-07-24 00:59:46.555","Will attach the test output in an attachment...","ZOOKEEPER-86","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-16 06:02:45.764","Closed","intermittent test failure of org.apache.zookeeper.test.AsyncTest","Bug","2010-03-27 01:24:54.191",0,0,8385,8385
17036,"2008-07-16 10:30:29.367","Cobertura library removed due to licensing issues, but the code-coverage ant targets were left in.  Have deleted them and created patch.","ZOOKEEPER-76","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-07-17 02:39:17.977","Closed","Commit 677109 removed the cobertura library, but not the build targets.","Bug","2008-10-26 09:10:40.999",0,0,7818,8066
17037,"2008-07-16 06:50:25.445","The cobertura library is GPL and must be removed from the code base. All of the X.jar files should have matching X.license files that contain their license. There is no license for junit. ","ZOOKEEPER-75","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-07-16 07:51:47.877","Closed","cleanup the library directory","Bug","2008-10-26 09:10:40.96",0,0,7818,8381
17108,"2008-06-10 00:42:38.071"," FastLeaderElection.java line 224: The part of the condition after && is not needed: This is the else branch of an if statement, where the condition is exactly the first part. Hence, the part after && *must* be true.","ZOOKEEPER-4","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-09-10 05:09:01.113","Closed","Unnecessary condition check in FastLeaderElection","Bug","2008-10-15 04:31:43.232",0,1,7750,7829
17044,"2008-07-08 06:11:14.375","the parseACLs(String aclString) method attempts to prase ACLs from the form of scheme:id:perm into the three components, delineated by the colons.  The current version calls indexOf for both the first and second colon, receiving the same value for both and failing as if there were only one colon in the string.  I created a one-line patch to call lastIndexOf for the second colon.","ZOOKEEPER-68","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-07-08 07:03:13.164","Closed","parseACLs in ZooKeeper.java fails to parse elements of ACL, should be lastIndexOf rather than IndexOf","Bug","2008-10-26 09:10:40.729",0,0,7827,8066
17049,"2008-07-01 06:08:36.288","There is a race condition in the java close operation on ZooKeeper.java.Client is sending a disconnect request to the server. Server will close any open connections with the client when it receives this. If the client has not yet shutdown it's subthreads (event/send threads for example) these threads may consider the condition an error. We see this alot in the tests where the clients output error logs because they are unaware that a disconnection has been requested by the client.Ben mentioned: perhaps we just have to change state to closed (on client) before sending disconnect request.","ZOOKEEPER-63","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-08-28 07:39:25.343","Closed","Race condition in client close() operation","Bug","2008-10-26 09:10:40.661",0,1,7818,7818
17053,"2008-06-30 19:20:15.005","There are two synchronized blocks locking on different objects, and to me they should be guarded by the same object. Here are the parts of the code I'm talking about:{noformat}NIOServerCnxn.readRequest@444...          synchronized (this) {                outstandingRequests++;                // check throttling                if (zk.getInProcess() > factory.outstandingLimit) {                    disableRecv();                    // following lines should not be needed since we are already                    // reading                    // } else {                    // enableRecv();                }            } {noformat}{noformat}NIOServerCnxn.sendResponse@740...         synchronized (this.factory) {                outstandingRequests--;                // check throttling                if (zk.getInProcess() < factory.outstandingLimit                        || outstandingRequests < 1) {                    sk.selector().wakeup();                    enableRecv();                }            }{noformat}I think the second one is correct, and the first synchronized block should be guarded by ""this.factory"". This could be related to issue ZOOKEEPER-57, but I have no concrete indication that this is the case so far.","ZOOKEEPER-59","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-03-09 10:01:14.567","Closed","Synchronized block in NIOServerCnxn","Bug","2010-03-27 01:24:53.875",0,2,7750,7750
17054,"2008-06-27 23:38:34.954","There is a race condition involving the ByteByffer incomingBuffer, a field of ClientCnxn.SendThread. SendThread reads a packet in two steps: first it reads the length of the packet, followed by a read of the packet itself. Each of these steps corresponds to a call to doIO() from the main loop of run(). If there is an exception or the session times out, then it may leave incomingBuffer in an inconsistent state. The attached patch adds code to reset incomingBuffer upon a call to SendThread.cleanup(). This method is called upon an exception on run().","ZOOKEEPER-58","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-07-27 16:07:11.487","Closed","Race condition on ClientCnxn.java","Bug","2008-10-26 09:10:40.584",0,0,7829,7750
17057,"2008-06-27 05:13:32.594","Due to the recent directory layout change,1) the ant target ""svn-revision"" fails to retrieve the release number,2) the ant target ""dist"" fails to create the distribution package.","ZOOKEEPER-55","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-06-27 07:50:28.933","Closed","build.xml failes to retrieve a release number from SVN and the ant target ""dist"" fails","Bug","2008-10-26 09:10:40.49",0,0,8380,8380
17058,"2008-06-26 07:30:36.411","remove the sleeps in the tests. ","ZOOKEEPER-54","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-08-26 07:53:48.774","Closed","remove sleeps in the tests.","Bug","2008-10-26 09:10:40.432",0,0,7818,7827
17059,"2008-06-26 06:43:46.265","tests are failing on solaris and randomly on my machine as well.","ZOOKEEPER-53","Blocker","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-06-26 07:06:20.501","Closed","tests failing on solaris.","Bug","2008-10-26 09:10:40.362",0,0,7827,7827
17062,"2008-06-21 04:36:38.538","We need to make clear in the documentation and enforce in the code the following watch event rules:# A watch event will be delivered once to each watcher, even if it is registered multiple times. For example, if the same watch object is used for getChildren(""/foo"", watchObj) and getData(""/foo"", watchObj, stat) and foo is deleted, watchObj will be called once to processed the NodeDeleted event.# Session events will be delivered to all watchers.*Note: a watcher is a Watcher object in Java or a (watch function, context) pair in C.*There is currently a bug in the Java client that causes the session disconnected event to be delivered twice to the default watcher if the default watcher is also used to watch a path. This violates rule 1.","ZOOKEEPER-50","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2010-01-16 05:58:54.191","Closed","Watch event delivery rules","Bug","2013-05-02 10:29:16.514",0,3,7829,7829
17063,"2008-06-20 15:23:28.753","As reported by Shane:Still exploring the ACL stuff in Zookeeper.  Tried using setACL for a  path but get InvalidACL error thrown .... looking at pRequest in  PrepRequestProcessor ... and in particular these lines ...                 SetACLRequest setAclRequest = new SetACLRequest();                 if (!fixupACL(request.authInfo,  setAclRequest.getAcl())) {                     throw new KeeperException(Code.InvalidACL);                 }a new SetACLRequest will return a null when called in fixupACL  returning false and throwing the exception .... as far as I can see.","ZOOKEEPER-49","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-08-05 21:46:59.175","Closed","SetACL does not work","Bug","2013-05-02 10:29:14.426",0,0,7829,7829
17064,"2008-06-20 14:02:47.685","AUTH_ID is used (usually done using Ids.CREATOR_ALL_ACL ) to represent the id that was used to authenticate with ZooKeeper. Thus, an exception should be raised if there are no authenticated ids present. Currently, the exception is not being raised.","ZOOKEEPER-48","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-08-01 04:11:03.882","Closed","AUTH_ID not handled correctly when no auth ids are present","Bug","2013-05-02 10:29:14.431",0,0,7829,7829
17091,"2008-06-11 05:25:05.836","Moved from SourceForge to Apache.http://sourceforge.net/tracker/index.php?func=detail&aid=1985723&group_id=209147&atid=1008544","ZOOKEEPER-21","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-09-27 02:09:56.567","Closed","Improve zk ctor/watcher (state transition) docs","Bug","2008-10-26 09:10:37.731",0,0,7818,7818
17092,"2008-06-11 05:23:59.634","Moved from SourceForge to Apache.http://sourceforge.net/tracker/index.php?func=detail&aid=1981340&group_id=209147&atid=1008544","ZOOKEEPER-20","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-08-28 02:36:36.523","Closed","Child watches are not triggered when the node is deleted","Bug","2008-10-26 09:10:37.697",0,0,7818,7818
17093,"2008-06-11 05:22:54.301","Moved from SourceForge to Apache.http://sourceforge.net/tracker/index.php?func=detail&aid=1981288&group_id=209147&atid=1008544","ZOOKEEPER-19","Major","ZOOKEEPER","ZooKeeper","ASF","Later","2008-09-10 05:06:55.746","Closed","Vector of Integers with Jute","Bug","2008-09-10 05:10:28.314",0,0,7750,7818
17094,"2008-06-11 05:20:06.936","Moved from SourceForge to Apache.http://sourceforge.net/tracker/index.php?func=detail&aid=1979772&group_id=209147&atid=1008544","ZOOKEEPER-18","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-09-26 06:33:39.586","Closed","keeper state inconsistency","Bug","2008-10-26 09:10:37.662",1,1,7818,7818
17095,"2008-06-11 05:18:37.792","Moved from SourceForge to Apache.http://sourceforge.net/tracker/index.php?func=detail&aid=1967467&group_id=209147&atid=1008544","ZOOKEEPER-17","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-11 06:56:54.163","Closed","zookeeper_init doc needs clarification","Bug","2008-10-26 09:10:37.628",0,0,7818,7818
17096,"2008-06-11 05:17:18.644","Moved from SourceForge to Apache.http://sourceforge.net/tracker/index.php?func=detail&aid=1963141&group_id=209147&atid=1008544","ZOOKEEPER-16","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-01-31 09:27:39.029","Closed","Need to do path validation","Bug","2009-02-14 05:18:48.723",0,1,7818,7818
17097,"2008-06-11 05:15:39.188","Moved from SourceForge to Apache.http://sourceforge.net/tracker/index.php?func=detail&aid=1959060&group_id=209147&atid=1008544","ZOOKEEPER-15","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-07-23 12:31:40.308","Closed","handle failure better in build.xml:test","Bug","2008-10-26 09:10:37.578",0,0,7818,7818
17101,"2008-06-11 05:10:23.482","Moved from SourceForge to Apache.http://sourceforge.net/tracker/index.php?func=detail&aid=1941109&group_id=209147&atid=1008544","ZOOKEEPER-11","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-07-23 12:30:53.368","Closed","ArrayList is used instead of List","Bug","2008-10-26 09:10:37.556",0,0,7818,7818
17104,"2008-06-11 05:05:10.988","Moved from SourceForge to Apache.http://sourceforge.net/tracker/index.php?func=detail&aid=1886743&group_id=209147&atid=1008544","ZOOKEEPER-8","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-10-08 02:52:33.317","Closed","Stat enchaned to include num of children and size","Bug","2008-10-26 09:10:37.505",0,0,7818,7818
17105,"2008-06-11 05:01:06.652","Moved from SourceForge to Apache.http://sourceforge.net/tracker/index.php?func=detail&aid=1831408&group_id=209147&atid=1008544Would be nice to fix in 3.0 as it's a non-bw compat change, also user feedback show's confusion on state/event/missingjavadoc.","ZOOKEEPER-7","Minor","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-09-27 05:27:47.316","Closed","Use enums rather than ints for types and state","Bug","2008-10-26 09:10:37.457",0,0,8066,7818
17106,"2008-06-11 04:42:02.577","Submitted on behalf of Jacob. Is is possible for us to address this in the next release? Will cause b/w compatibility issues for c client users but sounds like a good idea to fix now.--------I've attached a file with the problem identifiers. All of theseidentifiers are unprotected by a ZOO_ (or something similar) prefix.I've also looked at zookeeper.jute.h and the names of some these structsare really unfortunate -- theyre sure to collide with some headers e.g.Stat. There's also some exceptions to the consistent naming scheme --op_result_t, String_vector, Id_vector are the ones I noticed.-------- file -------These enum constants are unprotected:typedef enum {LOG_LEVEL_ERROR=1,LOG_LEVEL_WARN=2,LOG_LEVEL_INFO=3,LOG_LEVEL_DEBUG=4} ZooLogLevel;extern ZOOAPI const int PERM_READ;extern ZOOAPI const int PERM_WRITE;extern ZOOAPI const int PERM_CREATE;extern ZOOAPI const int PERM_DELETE;extern ZOOAPI const int PERM_ADMIN;extern ZOOAPI const int PERM_ALL;extern ZOOAPI struct Id ANYONE_ID_UNSAFE;extern ZOOAPI struct Id AUTH_IDS;extern ZOOAPI struct ACL_vector OPEN_ACL_UNSAFE;extern ZOOAPI struct ACL_vector READ_ACL_UNSAFE;extern ZOOAPI struct ACL_vector CREATOR_ALL_ACL;extern ZOOAPI const int EPHEMERAL;extern ZOOAPI const int SEQUENCE;extern ZOOAPI const int EXPIRED_SESSION_STATE;extern ZOOAPI const int AUTH_FAILED_STATE;extern ZOOAPI const int CONNECTING_STATE;extern ZOOAPI const int ASSOCIATING_STATE;extern ZOOAPI const int CONNECTED_STATE;extern ZOOAPI const int CREATED_EVENT;extern ZOOAPI const int DELETED_EVENT;extern ZOOAPI const int CHANGED_EVENT;extern ZOOAPI const int CHILD_EVENT;extern ZOOAPI const int SESSION_EVENT;extern ZOOAPI const int NOTWATCHING_EVENT;","ZOOKEEPER-6","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-09-30 19:15:40.154","Closed","List of problem identifiers in zookeeper.h","Bug","2008-10-26 09:10:37.405",0,0,7818,7818
17109,"2008-06-10 00:39:34.9","syncLimit as documented in QuorumPeer is documented twice with two different aspects of in each instance. It should be better documented and unified. (Probably remove the second instance.)","ZOOKEEPER-3","Trivial","ZOOKEEPER","ZooKeeper","ASF","Fixed","2009-11-19 01:48:01.793","Closed"," syncLimit has slightly different comments in the class header, and > inline with the variable.","Bug","2010-04-09 21:43:11.531",0,1,7827,7829
17110,"2008-06-10 00:34:31.959","There are a couple of cases of member variables that need to be marked volatile or surrounded in a synchronization block. A couple of examples are:* QuorumPeer state should be synchronous* currentVote in QuorumPeer is marked volatile, but when it's members are often accessed individually as if they were in an atomic unit. Such code should be changed to get a reference to the currentVote and they access members through that reference.* It looks like logicalClock in FastLeaderElection should be volatile. It should either be fixed or commented to explain why it doesn't need to be.","ZOOKEEPER-2","Major","ZOOKEEPER","ZooKeeper","ASF","Fixed","2008-08-26 05:13:14.578","Closed","Synchronization issues in QuorumPeer and FastLeader election","Bug","2008-09-10 05:11:22.601",0,1,7750,7829
